@inproceedings{Swearngin:CHI'19,
  title        = {{Modeling Mobile Interface Tappability Using Crowdsourcing and Deep Learning}},
  author       = {Swearngin, Amanda and Li, Yang},
  year         = 2019,
  booktitle    = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  location     = {Glasgow, Scotland Uk},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {CHI '19},
  pages        = {1–11},
  doi          = {10.1145/3290605.3300305},
  isbn         = 9781450359702,
  url          = {https://doi.org/10.1145/3290605.3300305},
  abstract     = {Tapping is an immensely important gesture in mobile touchscreen interfaces, yet people still frequently are required to learn which elements are tappable through trial and error. Predicting human behavior for this everyday gesture can help mobile app designers understand an important aspect of the usability of their apps without having to run a user study. In this paper, we present an approach for modeling tappability of mobile interfaces at scale. We conducted large-scale data collection of interface tappability over a rich set of mobile apps using crowdsourcing and computationally investigated a variety of signifiers that people use to distinguish tappable versus not-tappable elements. Based on the dataset, we developed and trained a deep neural network that predicts how likely a user will perceive an interface element as tappable versus not tappable. Using the trained tappability model, we developed TapShoe, a tool that automatically diagnoses mismatches between the tappability of each element as perceived by a human user---predicted by our model, and the intended or actual tappable state of the element specified by the developer or designer. Our model achieved reasonable accuracy: mean precision 90.2\% and recall 87.0\%, in matching human perception on identifying tappable UI elements. The tappability model and TapShoe were well received by designers via an informal evaluation with 7 professional interaction designers.},
  numpages     = 11,
  keywords     = {tappability, mobile interfaces, deep learning, crowdsourcing}
}
@inproceedings{Zhao:FSE22,
  title        = {{Avgust: automating usage-based test generation from videos of app executions}},
  author       = {Zhao, Yixue and Talebipour, Saghar and Baral, Kesina and Park, Hyojae and Yee, Leon and Khan, Safwat Ali and Brun, Yuriy and Medvidovi\'{c}, Nenad and Moran, Kevin},
  year         = 2022,
  booktitle    = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  location     = {, Singapore, Singapore,},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ESEC/FSE 2022},
  pages        = {421–433},
  doi          = {10.1145/3540250.3549134},
  isbn         = 9781450394130,
  url          = {https://doi.org/10.1145/3540250.3549134},
  abstract     = {Writing and maintaining UI tests for mobile apps is a time-consuming and tedious task. While decades of research have produced auto- mated approaches for UI test generation, these approaches typically focus on testing for crashes or maximizing code coverage. By contrast, recent research has shown that developers prefer usage-based tests, which center around specific uses of app features, to help support activities such as regression testing. Very few existing techniques support the generation of such tests, as doing so requires automating the difficult task of understanding the semantics of UI screens and user inputs. In this paper, we introduce Avgust, which automates key steps of generating usage-based tests. Avgust uses neural models for image understanding to process video recordings of app uses to synthesize an app-agnostic state-machine encoding of those uses. Then, Avgust uses this encoding to synthesize test cases for a new target app. We evaluate Avgust on 374 videos of common uses of 18 popular apps and show that 69\% of the tests Avgust generates successfully execute the desired usage, and that Avgust’s classifiers outperform the state of the art.},
  numpages     = 13,
  keywords     = {UI Understanding, Test Generation, Mobile Application, AI/ML}
}
@inproceedings{Havranek:ICSE'21,
  title        = {{V2S: A Tool for Translating Video Recordings of Mobile App Usages into Replayable Scenarios}},
  author       = {Havranek, Madeleine and Bernal-Cárdenas, Carlos and Cooper, Nathan and Chaparro, Oscar and Poshyvanyk, Denys and Moran, Kevin},
  year         = 2021,
  booktitle    = {2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)},
  volume       = {},
  number       = {},
  pages        = {65--68},
  doi          = {10.1109/ICSE-Companion52605.2021.00037},
  keywords     = {Uniform resource locators;Performance evaluation;Object detection;Tools;Software;Video recording;Software engineering;Bug Reporting;Screen Recordings;Object Detection;Mobile Apps}
}
@inproceedings{Moran:ASE'18,
  title        = {{Detecting and summarizing GUI changes in evolving mobile apps}},
  author       = {Moran, Kevin and Watson, Cody and Hoskins, John and Purnell, George and Poshyvanyk, Denys},
  year         = 2018,
  booktitle    = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
  location     = {Montpellier, France},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASE '18},
  pages        = {543–553},
  doi          = {10.1145/3238147.3238203},
  isbn         = 9781450359375,
  url          = {https://doi.org/10.1145/3238147.3238203},
  abstract     = {Mobile applications have become a popular software development domain in recent years due in part to a large user base, capable hardware, and accessible platforms. However, mobile developers also face unique challenges, including pressure for frequent releases to keep pace with rapid platform evolution, hardware iteration, and user feedback. Due to this rapid pace of evolution, developers need automated support for documenting the changes made to their apps in order to aid in program comprehension. One of the more challenging types of changes to document in mobile apps are those made to the graphical user interface (GUI) due to its abstract, pixel-based representation. In this paper, we present a fully automated approach, called GCAT, for detecting and summarizing GUI changes during the evolution of mobile apps. GCAT leverages computer vision techniques and natural language generation to accurately and concisely summarize changes made to the GUI of a mobile app between successive commits or releases. We evaluate the performance of our approach in terms of its precision and recall in detecting GUI changes compared to developer specified changes, and investigate the utility of the generated change reports in a controlled user study. Our results indicate that GCAT is capable of accurately detecting and classifying GUI changes - outperforming developers - while providing useful documentation.},
  numpages     = 11,
  keywords     = {Android, GUI changes, Mobile Apps, Software Evolution}
}
@inproceedings{Moran:ICSE'18,
  title        = {{Automated reporting of GUI design violations for mobile apps}},
  author       = {Moran, Kevin and Li, Boyang and Bernal-C\'{a}rdenas, Carlos and Jelf, Dan and Poshyvanyk, Denys},
  year         = 2018,
  booktitle    = {Proceedings of the 40th International Conference on Software Engineering},
  location     = {Gothenburg, Sweden},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ICSE '18},
  pages        = {165–175},
  doi          = {10.1145/3180155.3180246},
  isbn         = 9781450356381,
  url          = {https://doi.org/10.1145/3180155.3180246},
  abstract     = {The inception of a mobile app often takes form of a mock-up of the Graphical User Interface (GUI), represented as a static image delineating the proper layout and style of GUI widgets that satisfy requirements. Following this initial mock-up, the design artifacts are then handed off to developers whose goal is to accurately implement these GUIs and the desired functionality in code. Given the sizable abstraction gap between mock-ups and code, developers often introduce mistakes related to the GUI that can negatively impact an app's success in highly competitive marketplaces. Moreover, such mistakes are common in the evolutionary context of rapidly changing apps. This leads to the time-consuming and laborious task of design teams verifying that each screen of an app was implemented according to intended design specifications.This paper introduces a novel, automated approach for verifying whether the GUI of a mobile app was implemented according to its intended design. Our approach resolves GUI-related information from both implemented apps and mock-ups and uses computer vision techniques to identify common errors in the implementations of mobile GUIs. We implemented this approach for Android in a tool called Gvt and carried out both a controlled empirical evaluation with open-source apps as well as an industrial evaluation with designers and developers from Huawei. The results show that Gvt solves an important, difficult, and highly practical problem with remarkable efficiency and accuracy and is both useful and scalable from the point of view of industrial designers and developers. The tool is currently used by over one-thousand industrial designers \& developers at Huawei to improve the quality of their mobile apps.},
  numpages     = 11
}
@inproceedings{Linares:ICSME'17-2,
  title        = {{How do Developers Test Android Applications?}},
  author       = {Linares-Vásquez, Mario and Bernal-Cardenas, Cárlos and Moran, Kevin and Poshyvanyk, Denys},
  year         = 2017,
  booktitle    = {2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  volume       = {},
  number       = {},
  pages        = {613--622},
  doi          = {10.1109/ICSME.2017.47},
  keywords     = {Testing;Tools;Mobile communication;Androids;Humanoid robots;Manuals;Programming}
}
@inproceedings{Linares:ICSME'17,
  title        = {{Continuous, Evolutionary and Large-Scale: A New Perspective for Automated Mobile App Testing}},
  author       = {Linares-Vásquez, Mario and Moran, Kevin and Poshyvanyk, Denys},
  year         = 2017,
  booktitle    = {2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  volume       = {},
  number       = {},
  pages        = {399--410},
  doi          = {10.1109/ICSME.2017.27},
  keywords     = {Testing;Mobile communication;Graphical user interfaces;Tools;Systematics;Automation;Manuals}
}
@inproceedings{Linares:MSR15,
  title        = {{Mining Android app usages for generating actionable GUI-based execution scenarios}},
  author       = {Linares-V\'{a}squez, Mario and White, Martin and Bernal-C\'{a}rdenas, Carlos and Moran, Kevin and Poshyvanyk, Denys},
  year         = 2015,
  booktitle    = {Proceedings of the 12th Working Conference on Mining Software Repositories},
  location     = {Florence, Italy},
  publisher    = {IEEE Press},
  series       = {MSR '15},
  pages        = {111–122},
  isbn         = 9780769555942,
  abstract     = {GUI-based models extracted from Android app execution traces, events, or source code can be extremely useful for challenging tasks such as the generation of scenarios or test cases. However, extracting effective models can be an expensive process. Moreover, existing approaches for automatically deriving GUI-based models are not able to generate scenarios that include events which were not observed in execution (nor event) traces. In this paper, we address these and other major challenges in our novel hybrid approach, coined as MonkeyLab. Our approach is based on the Record→Mine→Generate→Validate framework, which relies on recording app usages that yield execution (event) traces, mining those event traces and generating execution scenarios using statistical language modeling, static and dynamic analyses, and validating the resulting scenarios using an interactive execution of the app on a real device. The framework aims at mining models capable of generating feasible and fully replayable (i.e., actionable) scenarios reflecting either natural user behavior or uncommon usages (e.g., corner cases) for a given app. We evaluated MonkeyLab in a case study involving several medium-to-large open-source Android apps. Our results demonstrate that MonkeyLab is able to mine GUI-based models that can be used to generate actionable execution scenarios for both natural and unnatural sequences of events on Google Nexus 7 tablets.},
  numpages     = 12,
  keywords     = {statistical language models, mobile apps, mining execution traces and event logs, GUI models}
}
@article{Cardenas:TSE23,
  title        = {{Translating Video Recordings of Complex Mobile App UI Gestures into Replayable Scenarios}},
  author       = {C. Bernal-Cardenas and N. Cooper and M. Havranek and K. Moran and O. Chaparro and D. Poshyvanyk and A. Marcus},
  year         = 2023,
  month        = {apr},
  journal      = {IEEE Transactions on Software Engineering},
  publisher    = {IEEE Computer Society},
  address      = {Los Alamitos, CA, USA},
  volume       = 49,
  number       = {04},
  pages        = {1782--1803},
  doi          = {10.1109/TSE.2022.3192279},
  issn         = {1939-3520},
  abstract     = {Screen recordings of mobile applications are easy to obtain and capture a wealth of information pertinent to software developers (e.g., bugs or feature requests), making them a popular mechanism for crowdsourced app feedback. Thus, these videos are becoming a common artifact that developers must manage. In light of unique mobile development constraints, including swift release cycles and rapidly evolving platforms, automated techniques for analyzing all types of rich software artifacts provide benefit to mobile developers. Unfortunately, automatically analyzing screen recordings presents serious challenges, due to their graphical nature, compared to other types of (textual) artifacts. To address these challenges, this paper introduces V2S+, an automated approach for translating video recordings of Android app usages into replayable scenarios. V2S+ is based primarily on computer vision techniques and adapts recent solutions for object detection and image classification to detect and classify user gestures captured in a video, and convert these into a replayable test scenario. Given that V2S+ takes a computer vision-based approach, it is applicable to both hybrid and native Android applications. We performed an extensive evaluation of V2S+ involving 243 videos depicting 4,028 GUI-based actions collected from users exercising features and reproducing bugs from a collection of over 90 popular native and hybrid Android apps. Our results illustrate that V2S+ can accurately replay scenarios from screen recordings, and is capable of reproducing $\approx$≈ 90.2% of sequential actions recorded in native application scenarios on physical devices, and $\approx$≈ 83% of sequential actions recorded in hybrid application scenarios on emulators, both with low overhead. A case study with three industrial partners illustrates the potential usefulness of V2S+ from the viewpoint of developers.},
  keywords     = {recording;computer architecture;training;testing;proposals;pipelines;performance evaluation}
}
@inproceedings{Chiou:CHI23,
  title        = {{BAGEL: An Approach to Automatically Detect Navigation-Based Web Accessibility Barriers for Keyboard Users}},
  author       = {Chiou, Paul T. and Alotaibi, Ali S. and Halfond, William G.J.},
  year         = 2023,
  booktitle    = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  location     = {Hamburg, Germany},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {CHI '23},
  doi          = {10.1145/3544548.3580749},
  isbn         = 9781450394215,
  url          = {https://doi.org/10.1145/3544548.3580749},
  abstract     = {The Web has become an essential part of many people’s daily lives, enabling them to complete everyday and essential tasks online and access important information resources. The ability to navigate the Web via the keyboard interface is critical to people with various types of disabilities. However, modern websites often violate web accessibility guidelines for keyboard navigability. In this paper, we present a novel approach for automatically detecting web accessibility barriers that prevent or hinder keyboard users’ ability to navigate web pages. An extensive evaluation of our technique on real-world subjects showed that our technique was able to detect navigation-based keyboard accessibility barriers in web applications with high precision and recall.},
  articleno    = 45,
  numpages     = 17,
  keywords     = {Web Accessibility, WCAG, Keyboard Navigation, Software Testing}
}
@inproceedings{wang_vet_2021,
  title        = {{Vet: identifying and avoiding UI exploration tarpits}},
  shorttitle   = {Vet},
  author       = {Wang, Wenyu and Yang, Wei and Xu, Tianyin and Xie, Tao},
  year         = 2021,
  month        = aug,
  booktitle    = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
  publisher    = {ACM},
  address      = {Athens Greece},
  pages        = {83--94},
  doi          = {10.1145/3468264.3468554},
  isbn         = {978-1-4503-8562-6},
  url          = {https://dl.acm.org/doi/10.1145/3468264.3468554},
  urldate      = {2023-02-12},
  abstract     = {Despite over a decade of research, it is still challenging for mobile UI testing tools to achieve satisfactory effectiveness, especially on industrial apps with rich features and large code bases. Our experiences suggest that existing mobile UI testing tools are prone to exploration tarpits, where the tools get stuck with a small fraction of app functionalities for an extensive amount of time. For example, a tool logs out an app at early stages without being able to log back in, and since then the tool gets stuck with exploring the app’s pre-login functionalities (i.e., exploration tarpits) instead of its main functionalities. While tool vendors/users can manually hardcode rules for the tools to avoid specific exploration tarpits, these rules can hardly generalize, being fragile in face of diverted testing environments, fast app iterations, and the demand of batch testing product lines. To identify and resolve exploration tarpits, we propose Vet, a general approach including a supporting system for the given specific Android UI testing tool on the given specific app under test (AUT). Vet runs the tool on the AUT for some time and records UI traces, based on which Vet identifies exploration tarpits by recognizing their patterns in the UI traces. Vet then pinpoints the actions (e.g., clicking logout) or the screens that lead to or exhibit exploration tarpits. In subsequent test runs, Vet guides the testing tool to prevent or recover from exploration tarpits. From our evaluation with state-of-the-art Android UI testing tools on popular industrial apps, Vet identifies exploration tarpits that cost up to 98.6\% testing time budget. These exploration tarpits reveal not only limitations in UI exploration strategies but also defects in tool implementations. Vet automatically addresses the identified exploration tarpits, enabling each evaluated tool to achieve higher code coverage and improve crash-triggering capabilities.},
  language     = {en}
}
@inproceedings{mansur2023aidui,
  title        = {{Aidui: Toward automated recognition of dark patterns in user interfaces}},
  author       = {Mansur, SM Hasan and Salma, Sabiha and Awofisayo, Damilola and Moran, Kevin},
  year         = 2023,
  booktitle    = {2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  pages        = {1958--1970},
  organization = {IEEE}
}
@article{ren2015faster,
  title        = {{Faster r-cnn: Towards real-time object detection with region proposal networks}},
  author       = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year         = 2015,
  journal      = {Advances in neural information processing systems},
  volume       = 28
}
@article{Chen22,
  title        = {{Accessible or Not? An Empirical Investigation of Android App Accessibility}},
  author       = {Chen, Sen and Chen, Chunyang and Fan, Lingling and Fan, Mingming and Zhan, Xian and Liu, Yang},
  year         = 2022,
  journal      = {IEEE Transactions on Software Engineering},
  volume       = 48,
  number       = 10,
  pages        = {3954--3968},
  doi          = {10.1109/TSE.2021.3108162}
}
@inproceedings{Deka:2017:Rico,
  title        = {{Rico: A Mobile App Dataset for Building Data-Driven Design Applications}},
  author       = {Deka, Biplab and Huang, Zifeng and Franzen, Chad and Hibschman, Joshua and Afergan, Daniel and Li, Yang and Nichols, Jeffrey and Kumar, Ranjitha},
  year         = 2017,
  booktitle    = {Proceedings of the 30th Annual Symposium on User Interface Software and Technology},
  series       = {UIST '17},
  keywords     = {Mobile app design; design mining; design search; app datasets}
}
https://support.google.com/accessibility/android/answer/6122836?hl=en
@techreport{kitchenham2007guidelines,
  title        = {{Guidelines for performing Systematic Literature Reviews in Software Engineering}},
  author       = {Kitchenham, Barbara Ann and Charters, Stuart},
  year         = 2007,
  month        = {07},
  day          = 09,
  number       = {EBSE 2007-001},
  url          = {https://www.elsevier.com/__data/promis_misc/525444systematicreviewsguide.pdf},
  abstract     = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
  added-at     = {2019-11-16T00:31:45.000+0100},
  biburl       = {https://www.bibsonomy.org/bibtex/23f4b30c0fe1435b642467af4cca120ef/jpmor},
  citeulike-article-id = 3955888,
  institution  = {Keele University and Durham University Joint Report},
  interhash    = {aed0229656ada843d3e3f24e5e5c9eb9},
  intrahash    = {3f4b30c0fe1435b642467af4cca120ef},
  keywords     = {engineering evidence evidence-based literature real review software systematic},
  language     = {English},
  posted-at    = {2009-01-28 11:17:05},
  priority     = 2,
  school       = {Keele University},
  timestamp    = {2020-10-07T13:36:50.000+0200}
}
@url{switch-access,
  title        = {{Android Switch Access Service}},
  howpublished = {\url{https://support.google.com/accessibility/android/answer/6122836?hl=en}}
}
}
@url{univ-design,
  title        = {{Universal Design Definition and Guidelines}},
  howpublished = {\url{https://www.section508.gov/blog/Universal-Design-What-is-it/}}
}
}
@url{Fdroid,
  title        = {{\url{https://f-droid.org/}}},
  year         = {F-droid}
}
@url{GooglePlayStore,
  title        = {{\url{https://play.google.com/store}}},
  year         = {Google Play Store}
}
@url{LabelStudio,
  title        = {{\url{https://labelstud.io}}},
  year         = {LabelStudio}
}
@inproceedings{Cardenas:ICSE'20,
  title        = {{Translating Video Recordings of Mobile App Usages into Replayable Scenarios}},
  author       = {Bernal-C\'{a}rdenas, Carlos and Cooper, Nathan and Moran, Kevin and Chaparro, Oscar and Marcus, Andrian and Poshyvanyk, Denys},
  year         = 2020,
  booktitle    = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
  location     = {Seoul, South Korea},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ICSE '20},
  pages        = {309–321},
  doi          = {10.1145/3377811.3380328},
  isbn         = 9781450371216,
  url          = {https://doi.org/10.1145/3377811.3380328},
  abstract     = {Screen recordings of mobile applications are easy to obtain and capture a wealth of information pertinent to software developers (e.g., bugs or feature requests), making them a popular mechanism for crowdsourced app feedback. Thus, these videos are becoming a common artifact that developers must manage. In light of unique mobile development constraints, including swift release cycles and rapidly evolving platforms, automated techniques for analyzing all types of rich software artifacts provide benefit to mobile developers. Unfortunately, automatically analyzing screen recordings presents serious challenges, due to their graphical nature, compared to other types of (textual) artifacts. To address these challenges, this paper introduces V2S, a lightweight, automated approach for translating video recordings of Android app usages into replayable scenarios. V2S is based primarily on computer vision techniques and adapts recent solutions for object detection and image classification to detect and classify user actions captured in a video, and convert these into a replayable test scenario. We performed an extensive evaluation of V2S involving 175 videos depicting 3,534 GUI-based actions collected from users exercising features and reproducing bugs from over 80 popular Android apps. Our results illustrate that V2S can accurately replay scenarios from screen recordings, and is capable of reproducing ≈89% of our collected videos with minimal overhead. A case study with three industrial partners illustrates the potential usefulness of V2S from the viewpoint of developers.},
  numpages     = 13,
  keywords     = {screen recordings, bug reporting, object detection}
}
@inproceedings{Deka:UIST'17,
  title        = {{Rico: A Mobile App Dataset for Building Data-Driven Design Applications}},
  author       = {Deka, Biplab and Huang, Zifeng and Franzen, Chad and Hibschman, Joshua and Afergan, Daniel and Li, Yang and Nichols, Jeffrey and Kumar, Ranjitha},
  year         = 2017,
  booktitle    = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
  location     = {Qu\'{e}bec City, QC, Canada},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {UIST '17},
  pages        = {845–854},
  doi          = {10.1145/3126594.3126651},
  isbn         = 9781450349819,
  url          = {https://doi.org/10.1145/3126594.3126651},
  abstract     = {Data-driven models help mobile app designers understand best practices and trends, and can be used to make predictions about design performance and support the creation of adaptive UIs. This paper presents Rico, the largest repository of mobile app designs to date, created to support five classes of data-driven applications: design search, UI layout generation, UI code generation, user interaction modeling, and user perception prediction. To create Rico, we built a system that combines crowdsourcing and automation to scalably mine design and interaction data from Android apps at runtime. The Rico dataset contains design data from more than 9.7k Android apps spanning 27 categories. It exposes visual, textual, structural, and interactive design properties of more than 72k unique UI screens. To demonstrate the kinds of applications that Rico enables, we present results from training an autoencoder for UI layout similarity, which supports query- by-example search over UIs.},
  numpages     = 10,
  keywords     = {app datasets, design mining, design search, mobile app design}
}
@inproceedings{zhou2017east,
  title        = {{East: an efficient and accurate scene text detector}},
  author       = {Zhou, Xinyu and Yao, Cong and Wen, He and Wang, Yuzhi and Zhou, Shuchang and He, Weiran and Liang, Jiajun},
  year         = 2017,
  booktitle    = {Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages        = {5551--5560}
}
@inproceedings{Mehralian:FSE'21,
  title        = {{Data-Driven Accessibility Repair Revisited: On the Effectiveness of Generating Labels for Icons in Android Apps}},
  author       = {Mehralian, Forough and Salehnamadi, Navid and Malek, Sam},
  year         = 2021,
  booktitle    = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  location     = {Athens, Greece},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ESEC/FSE 2021},
  pages        = {107–118},
  doi          = {10.1145/3468264.3468604},
  isbn         = 9781450385626,
  url          = {https://doi.org/10.1145/3468264.3468604},
  abstract     = {Mobile apps are playing an increasingly important role in our daily lives, including the lives of approximately 304 million users worldwide that are either completely blind or suffer from some form of visual impairment. These users rely on screen readers to interact with apps. Screen readers, however, cannot describe the image icons that appear on the screen, unless those icons are accompanied with developer-provided textual labels. A prior study of over 5,000 Android apps found that in around 50% of the apps, less than 10% of the icons are labeled. To address this problem, a recent award-winning approach, called LabelDroid, employed deep-learning techniques to train a model on a dataset of existing icons with labels to automatically generate labels for visually similar, unlabeled icons. In this work, we empirically study the nature of icon labels in terms of distribution and their dependency on different sources of information. We then assess the effectiveness of LabelDroid in predicting labels for unlabeled icons. We find that icon images are insufficient in representing icon labels, while other sources of information from the icon usage context can enrich images in determining proper tokens for labels. We propose the first context-aware label generation approach, called COALA, that incorporates several sources of information from the icon in generating accurate labels. Our experiments show that although COALA significantly outperforms LabelDroid in both user study and automatic evaluation, further research is needed. We suggest that future studies should be more cautious when basing their approach on automatically extracted labeled data.},
  numpages     = 12,
  keywords     = {Alternative Text, Screen Reader, Accessibility, Deep Learning, Android}
}
@inproceedings{chen2020unblind,
  title        = {{Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning}},
  author       = {Chen, Jieshan and Chen, Chunyang and Xing, Zhenchang and Xu, Xiwei and Zhu, Liming and Li, Guoqiang and Wang, Jinshui},
  year         = 2020,
  booktitle    = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
  location     = {Seoul, South Korea},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ICSE '20},
  pages        = {322–334},
  doi          = {10.1145/3377811.3380327},
  isbn         = 9781450371216,
  url          = {https://doi.org/10.1145/3377811.3380327},
  numpages     = 13,
  keywords     = {accessibility, neural networks, content description, user interface, image-based buttons}
}
@inproceedings{Salehnamadi:ASE'22,
  title        = {{Groundhog: An Automated Accessibility Crawler for Mobile Apps}},
  author       = {Salehnamadi, Navid and Mehralian, Forough and Malek, Sam},
  year         = 2022,
  booktitle    = {37th IEEE/ACM International Conference on Automated Software Engineering (ASE 2022)},
  volume       = {},
  number       = {},
  pages        = {},
  doi          = {}
}
@inproceedings{Eler18,
  title        = {{Automated Accessibility Testing of Mobile Apps}},
  author       = {Eler, Marcelo Medeiros and Rojas, Jose Miguel and Ge, Yan and Fraser, Gordon},
  year         = 2018,
  booktitle    = {2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)},
  volume       = {},
  number       = {},
  pages        = {116--126},
  doi          = {10.1109/ICST.2018.00021}
}
@inproceedings{Lin:ASE'20,
  title        = {{Test Automation in Open-Source Android Apps: A Large-Scale Empirical Study}},
  author       = {Lin, Jun-Wei and Salehnamadi, Navid and Malek, Sam},
  year         = 2021,
  booktitle    = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
  location     = {Virtual Event, Australia},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASE '20},
  pages        = {1078–1089},
  doi          = {10.1145/3324884.3416623},
  isbn         = 9781450367684,
  url          = {https://doi.org/10.1145/3324884.3416623},
  abstract     = {Automated testing of mobile apps has received significant attention in recent years from researchers and practitioners alike. In this paper, we report on the largest empirical study to date, aimed at understanding the test automation culture prevalent among mobile app developers. We systematically examined more than 3.5 million repositories on GitHub and identified more than 12, 000 non-trivial and real-world Android apps. We then analyzed these non-trivial apps to investigate (1) the prevalence of adoption of test automation; (2) working habits of mobile app developers in regards to automated testing; and (3) the correlation between the adoption of test automation and the popularity of projects. Among others, we found that (1) only 8% of the mobile app development projects leverage automated testing practices; (2) developers tend to follow the same test automation practices across projects; and (3) popular projects, measured in terms of the number of contributors, stars, and forks on GitHub, are more likely to adopt test automation practices. To understand the rationale behind our observations, we further conducted a survey with 148 professional and experienced developers contributing to the subject apps. Our findings shed light on the current practices and future research directions pertaining to test automation for mobile app development.},
  numpages     = 12,
  keywords     = {automated testing, mobile apps, empirical study, Android}
}
@inproceedings{Su:FSE'17,
  title        = {{Guided, Stochastic Model-Based GUI Testing of Android Apps}},
  author       = {Su, Ting and Meng, Guozhu and Chen, Yuting and Wu, Ke and Yang, Weiming and Yao, Yao and Pu, Geguang and Liu, Yang and Su, Zhendong},
  year         = 2017,
  booktitle    = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
  location     = {Paderborn, Germany},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ESEC/FSE 2017},
  pages        = {245–256},
  doi          = {10.1145/3106237.3106298},
  isbn         = 9781450351058,
  url          = {https://doi.org/10.1145/3106237.3106298},
  abstract     = {Mobile apps are ubiquitous, operate in complex environments and are developed under the time-to-market pressure. Ensuring their correctness and reliability thus becomes an important challenge. This paper introduces Stoat, a novel guided approach to perform stochastic model-based testing on Android apps. Stoat operates in two phases: (1) Given an app as input, it uses dynamic analysis enhanced by a weighted UI exploration strategy and static analysis to reverse engineer a stochastic model of the app's GUI interactions; and (2) it adapts Gibbs sampling to iteratively mutate/refine the stochastic model and guides test generation from the mutated models toward achieving high code and model coverage and exhibiting diverse sequences. During testing, system-level events are randomly injected to further enhance the testing effectiveness. Stoat was evaluated on 93 open-source apps. The results show (1) the models produced by Stoat cover 17~31% more code than those by existing modeling tools; (2) Stoat detects 3X more unique crashes than two state-of-the-art testing tools, Monkey and Sapienz. Furthermore, Stoat tested 1661 most popular Google Play apps, and detected 2110 previously unknown and unique crashes. So far, 43 developers have responded that they are investigating our reports. 20 of reported crashes have been confirmed, and 8 already fixed.},
  numpages     = 12,
  keywords     = {Mobile Apps, GUI Testing, Model-based Testing}
}
@inproceedings{Gu:ICSE'19,
  title        = {{Practical GUI Testing of Android Applications via Model Abstraction and Refinement}},
  author       = {Gu, Tianxiao and Sun, Chengnian and Ma, Xiaoxing and Cao, Chun and Xu, Chang and Yao, Yuan and Zhang, Qirun and Lu, Jian and Su, Zhendong},
  year         = 2019,
  booktitle    = {Proceedings of the 41st International Conference on Software Engineering},
  location     = {Montreal, Quebec, Canada},
  publisher    = {IEEE Press},
  series       = {ICSE '19},
  pages        = {269–280},
  doi          = {10.1109/ICSE.2019.00042},
  url          = {https://doi.org/10.1109/ICSE.2019.00042},
  abstract     = {This paper introduces a new, fully automated model-based approach for effective testing of Android apps. Different from existing model-based approaches that guide testing with a static GUI model (i.e., the model does not evolve its abstraction during testing, and is thus often imprecise), our approach dynamically optimizes the model by leveraging the runtime information during testing. This capability of model evolution significantly improves model precision, and thus dramatically enhances the testing effectiveness compared to existing approaches, which our evaluation confirms. We have realized our technique in a practical tool, Ape. On 15 large, widely-used apps from the Google Play Store, Ape outperforms the state-of-the-art Android GUI testing tools in terms of both testing coverage and the number of detected unique crashes. To further demonstrate Ape's effectiveness and usability, we conduct another evaluation of Ape on 1,316 popular apps, where it found 537 unique crashes. Out of the 38 reported crashes, 13 have been fixed and 5 have been confirmed.},
  numpages     = 12,
  keywords     = {GUI testing, CEGAR, mobile app testing}
}
@misc{appendix,
  title        = {{MotorEase GitHub Repository}},
  howpublished = {\url{https://github.com/SageSELab/MotorEase}}
}
@misc{zenodo,
  title        = {{MotorEase Zenodo Archive}},
  howpublished = {\url{https://zenodo.org/doi/10.5281/zenodo.10460700}}
}
@misc{site,
  title        = {{MotorEase Website}},
  howpublished = {\url{https://sagelab.io/MotorEase}}
}
@url{instaStats,
  title        = {{Instagram stats: Ranking in google play, downloads; users}},
  journal      = {Similarweb},
  url          = {https://www.similarweb.com/app/google-play/com.instagram.android/statistics/}
}
@misc{AppleAccess,
  title        = {{Accessibility: Apple Human Interface Guidelines}},
  howpublished = {\url{https://developer.apple.com/design/human-interface-guidelines/foundations/accessibility/}}
}
@misc{GoogleScanner,
  title        = {{Accessibility Scanner}},
  howpublished = {\url{https://play.google.com/store/apps/details?id=com.google.android.apps.accessibility.auditor}}
}
@inproceedings{Montague14,
  title        = {{Motor-Impaired Touchscreen Interactions in the Wild}},
  author       = {Montague, Kyle and Nicolau, Hugo and Hanson, Vicki L.},
  year         = 2014,
  booktitle    = {Proceedings of the 16th International ACM SIGACCESS Conference on Computers ; Accessibility},
  location     = {Rochester, New York, USA},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASSETS '14},
  pages        = {123–130},
  doi          = {10.1145/2661334.2661362},
  isbn         = 9781450327206,
  url          = {https://doi.org/10.1145/2661334.2661362},
  abstract     = {Touchscreens are pervasive in mainstream technologies; they offer novel user interfaces and exciting gestural interactions. However, to interpret and distinguish between the vast ranges of gestural inputs, the devices require users to consistently perform interactions inline with the predefined location, movement and timing parameters of the gesture recognizers. For people with variable motor abilities, particularly hand tremors, performing these input gestures can be extremely challenging and impose limitations on the possible interactions the user can make with the device. In this paper, we examine touchscreen performance and interaction behaviors of motor-impaired users on mobile devices. The primary goal of this work is to measure and understand the variance of touchscreen interaction performances by people with motor-impairments. We conducted a four-week in-the-wild user study with nine participants using a mobile touchscreen device. A Sudoku stimulus application measured their interaction performance abilities during this time. Our results show that not only does interaction performance vary significantly between users, but also that an individual's interaction abilities are significantly different between device sessions. Finally, we propose and evaluate the effect of novel tap gesture recognizers to accommodate for individual variances in touchscreen interactions.},
  numpages     = 8,
  keywords     = {touchscreen, in-the-wild, user models, motor-impaired}
}
@inproceedings{Zhang13,
  title        = {{Optimization of Switch Keyboards}},
  author       = {Zhang, Xiao (Cosmo) and Fang, Kan and Francis, Gregory},
  year         = 2013,
  booktitle    = {Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility},
  location     = {Bellevue, Washington},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASSETS '13},
  doi          = {10.1145/2513383.2513394},
  isbn         = 9781450324052,
  url          = {https://doi.org/10.1145/2513383.2513394},
  abstract     = {Patients with motor control difficulties often "type" on a computer using a switch keyboard to guide a scanning cursor to text elements. We show how to optimize some parts of the design of switch keyboards by casting the design problem as mixed integer programming. A new algorithm to find an optimized design solution is approximately 3600 times faster than a previous algorithm, which was also susceptible to finding a non-optimal solution. The optimization requires a model of the probability of an entry error, and we show how to build such a model from experimental data. Example optimized keyboards are demonstrated.},
  articleno    = 60,
  numpages     = 2,
  keywords     = {"locked-in" patients, mixed integer programming, switch keyboard}
}
@inproceedings{Oh13,
  title        = {{Follow That Sound: Using Sonification and Corrective Verbal Feedback to Teach Touchscreen Gestures}},
  author       = {Oh, Uran and Kane, Shaun K. and Findlater, Leah},
  year         = 2013,
  booktitle    = {Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility},
  location     = {Bellevue, Washington},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASSETS '13},
  doi          = {10.1145/2513383.2513455},
  isbn         = 9781450324052,
  url          = {https://doi.org/10.1145/2513383.2513455},
  abstract     = {While sighted users may learn to perform touchscreen gestures through observation (e.g., of other users or video tutorials), such mechanisms are inaccessible for users with visual impairments. As a result, learning to perform gestures can be challenging. We propose and evaluate two techniques to teach touchscreen gestures to users with visual impairments: (1) corrective verbal feedback using text-to-speech and automatic analysis of the user's drawn gesture; (2) gesture sonification to generate sound based on finger touches, creating an audio representation of a gesture. To refine and evaluate the techniques, we conducted two controlled lab studies. The first study, with 12 sighted participants, compared parameters for sonifying gestures in an eyes-free scenario and identified pitch + stereo panning as the best combination. In the second study, 6 blind and low-vision participants completed gesture replication tasks with the two feedback techniques. Subjective data and preliminary performance findings indicate that the techniques offer complementary advantages.},
  articleno    = 13,
  numpages     = 8,
  keywords     = {sonification, blindness, touchscreen, visual impairments, gestures}
}
@inproceedings{Montague12,
  title        = {{Designing for Individuals: Usable Touch-Screen Interaction through Shared User Models}},
  author       = {Montague, Kyle and Hanson, Vicki L. and Cobley, Andy},
  year         = 2012,
  booktitle    = {Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility},
  location     = {Boulder, Colorado, USA},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASSETS '12},
  pages        = {151–158},
  doi          = {10.1145/2384916.2384943},
  isbn         = 9781450313216,
  url          = {https://doi.org/10.1145/2384916.2384943},
  abstract     = {Mobile touch-screen devices are becoming increasingly popular across a diverse range of users. Whilst there is a wealth of information and utilities available via downloadable apps, there is still a large proportion of users with visual and motor impairments who are unable to use the technology fully due to their interaction needs. In this paper we present an evaluation of the use of shared user modelling and adaptive interfaces to improve the accessibility of mobile touch-screen technologies. By using abilities based information collected through application use and continually updating the user model and interface adaptations, it is easy for users to make applications aware of their needs and preferences. Three smart phone apps were created for this study and tested with 12 adults who had diverse visual and motor impairments. Results indicated significant benefits from the shared user models that can automatically adapt interfaces, across applications, to address usability needs.},
  numpages     = 8,
  keywords     = {mobile touch screens, adaptive interfaces, shared user modelling}
}
@inproceedings{Szpiro16,
  title        = {{How People with Low Vision Access Computing Devices: Understanding Challenges and Opportunities}},
  author       = {Szpiro, Sarit Felicia Anais and Hashash, Shafeka and Zhao, Yuhang and Azenkot, Shiri},
  year         = 2016,
  booktitle    = {Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility},
  location     = {Reno, Nevada, USA},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASSETS '16},
  pages        = {171–180},
  doi          = {10.1145/2982142.2982168},
  isbn         = 9781450341240,
  url          = {https://doi.org/10.1145/2982142.2982168},
  abstract     = {Low vision is a pervasive condition in which people have difficulty seeing even with corrective lenses. People with low vision frequently use mainstream computing devices, however how they use their devices to access information and whether digital low vision accessibility tools provide adequate support remains understudied. We addressed these questions with a contextual inquiry study. We observed 11 low vision participants using their smartphones, tablets, and computers when performing simple tasks such as reading email. We found that participants preferred accessing information visually than aurally (e.g., screen readers), and juggled a variety of accessibility tools. However, accessibility tools did not provide them with appropriate support. Moreover, participants had to constantly perform multiple gestures in order to see content comfortably. These challenges made participants inefficient-they were slow and often made mistakes; even tech savvy participants felt frustrated and not in control. Our findings reveal the unique needs of low vision people, which differ from those of people with no vision and design opportunities for improving low vision accessibility tools.},
  numpages     = 10,
  keywords     = {computing devices, contextual inquiry, low vision, accessibility}
}
@inproceedings{Norman13,
  title        = {{How Accessible is the Process of Web Interface Design?}},
  author       = {Norman, Kirk and Arber, Yevgeniy and Kuber, Ravi},
  year         = 2013,
  booktitle    = {Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility},
  location     = {Bellevue, Washington},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASSETS '13},
  doi          = {10.1145/2513383.2513385},
  isbn         = 9781450324052,
  url          = {https://doi.org/10.1145/2513383.2513385},
  abstract     = {This paper describes a data gathering study, examining the experiences and day-to-day challenges faced by blind web interface developers when designing sites and online applications. Findings have revealed that considerable amounts of time and cognitive effort can be spent checking code in text editing software and examining the content presented via the web browser. Participants highlighted the burden experienced from committing large sections of code to memory, and the restrictions associated with assistive technologies when performing collaborative tasks with sighted developers and clients. Our future work aims to focus on the development of a multimodal web editing and browsing solution, designed to support both blind and sighted parties during the design process.},
  articleno    = 51,
  numpages     = 2,
  keywords     = {HTML, accessibility, blind, web development, screen reader}
}
@inproceedings{Salehnamadi21,
  title        = {{Latte: Use-Case and Assistive-Service Driven Automated Accessibility Testing Framework for Android}},
  author       = {Salehnamadi, Navid and Alshayban, Abdulaziz and Lin, Jun-Wei and Ahmed, Iftekhar and Branham, Stacy and Malek, Sam},
  year         = 2021,
  booktitle    = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  location     = {Yokohama, Japan},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {CHI '21},
  doi          = {10.1145/3411764.3445455},
  isbn         = 9781450380966,
  url          = {https://doi.org/10.1145/3411764.3445455},
  abstract     = {For 15% of the world population with disabilities, accessibility is arguably the most critical software quality attribute. The ever-growing reliance of users with disability on mobile apps further underscores the need for accessible software in this domain. Existing automated accessibility assessment techniques primarily aim to detect violations of predefined guidelines, thereby produce a massive amount of accessibility warnings that often overlook the way software is actually used by users with disability. This paper presents a novel, high-fidelity form of accessibility testing for Android apps, called Latte, that automatically reuses tests written to evaluate an app’s functional correctness to assess its accessibility as well. Latte first extracts the use case corresponding to each test, and then executes each use case in the way disabled users would, i.e., using assistive services. Our empirical evaluation on real-world Android apps demonstrates Latte’s effectiveness in detecting substantially more useful defects than prior techniques.},
  articleno    = 274,
  numpages     = 11,
  keywords     = {Accessibility, Automated Testing, Mobile Application}
}
@url{TalkBack,
  title        = {{Google TalkBack source code \url{https://github.com/google/talkback}}},
  year         = 2019
}
@inproceedings{Wu21,
  title        = {{Screen Parsing: Towards Reverse Engineering of UI Models from Screenshots}},
  author       = {Wu, Jason and Zhang, Xiaoyi and Nichols, Jeff and Bigham, Jeffrey P},
  year         = 2021,
  booktitle    = {The 34th Annual ACM Symposium on User Interface Software and Technology},
  location     = {Virtual Event, USA},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {UIST '21},
  pages        = {470–483},
  doi          = {10.1145/3472749.3474763},
  isbn         = 9781450386357,
  url          = {https://doi.org/10.1145/3472749.3474763},
  abstract     = {Automated understanding of user interfaces (UIs) from their pixels can improve accessibility, enable task automation, and facilitate interface design without relying on developers to comprehensively provide metadata. A first step is to infer what UI elements exist on a screen, but current approaches are limited in how they infer how those elements are semantically grouped into structured interface definitions. In this paper, we motivate the problem of screen parsing, the task of predicting UI elements and their relationships from a screenshot. We describe our implementation of screen parsing and provide an effective training procedure that optimizes its performance. In an evaluation comparing the accuracy of the generated output, we find that our implementation significantly outperforms current systems (up to 23%). Finally, we show three example applications that are facilitated by screen parsing: (i) UI similarity search, (ii) accessibility enhancement, and (iii) code generation from UI screenshots.},
  numpages     = 14,
  keywords     = {user interface modeling, hierarchy prediction, ui semantics}
}
@inproceedings{Zhang21,
  title        = {{Screen Recognition: Creating Accessibility Metadata for Mobile Applications from Pixels}},
  author       = {Zhang, Xiaoyi and de Greef, Lilian and Swearngin, Amanda and White, Samuel and Murray, Kyle and Yu, Lisa and Shan, Qi and Nichols, Jeffrey and Wu, Jason and Fleizach, Chris and Everitt, Aaron and Bigham, Jeffrey P},
  year         = 2021,
  booktitle    = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  location     = {Yokohama, Japan},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {CHI '21},
  doi          = {10.1145/3411764.3445186},
  isbn         = 9781450380966,
  url          = {https://doi.org/10.1145/3411764.3445186},
  abstract     = {Many accessibility features available on mobile platforms require applications (apps) to provide complete and accurate metadata describing user interface (UI) components. Unfortunately, many apps do not provide sufficient metadata for accessibility features to work as expected. In this paper, we explore inferring accessibility metadata for mobile apps from their pixels, as the visual interfaces often best reflect an app’s full functionality. We trained a robust, fast, memory-efficient, on-device model to detect UI elements using a dataset of 77,637 screens (from 4,068 iPhone apps) that we collected and annotated. To further improve UI detections and add semantic information, we introduced heuristics (e.g., UI grouping and ordering) and additional models (e.g., recognize UI content, state, interactivity). We built Screen Recognition to generate accessibility metadata to augment iOS VoiceOver. In a study with 9 screen reader users, we validated that our approach improves the accessibility of existing mobile apps, enabling even previously inaccessible apps to be used.},
  articleno    = 275,
  numpages     = 15,
  keywords     = {accessibility enhancement, ui detection, mobile accessibility}
}
@inproceedings{Park14,
  title        = {{Toward Accessible Mobile Application Design: Developing Mobile Application Accessibility Guidelines for People with Visual Impairment}},
  author       = {Park, Kyudong and Goh, Taedong and So, Hyo-Jeong},
  year         = 2014,
  booktitle    = {Proceedings of HCI Korea},
  location     = {Seoul, Republic of Korea},
  publisher    = {Hanbit Media, Inc.},
  address      = {Seoul, KOR},
  series       = {HCIK '15},
  pages        = {31–38},
  isbn         = 9788968487521,
  abstract     = {While the use of Smartphones has improved the life of people with disabilities, several mobile content and applications remain inaccessible to people with visual impairment. Toward the overarching goal of accessible mobile application design, this two-phased study attempts to develop mobile application accessibility guidelines for people with visual impairment. First, we investigated how people with visual impairment use mobile phones. Four participants with visual impairment performed specified tasks. Their usage patterns and follow-up interviews were analyzed. Serious accessibility problems were found in both typing and VoiceOver functions. Second, we evaluated and developed systematic guidelines and standards of designing accessible mobile applications through a heuristic walkthrough method. Four experts with extensive experiences and knowledge about mobile application development/design used the VoiceOver function with iPhone for 5 days, and then walked through thought-provoking tasks. In conclusion, we propose a set of 10 heuristics for developing accessible mobile applications and suggest a critical need for internationally-agreed guidelines and standards to improve the current mobile environment for people with disabilities.},
  numpages     = 8,
  keywords     = {evaluation, heuristic walkthrough, design guideline, standardization, VoiceOver}
}
@inproceedings{Chen20,
  title        = {{Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning}},
  author       = {Chen, Jieshan and Chen, Chunyang and Xing, Zhenchang and Xu, Xiwei and Zhu, Liming and Li, Guoqiang and Wang, Jinshui},
  year         = 2020,
  booktitle    = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
  location     = {Seoul, South Korea},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ICSE '20},
  pages        = {322–334},
  doi          = {10.1145/3377811.3380327},
  isbn         = 9781450371216,
  url          = {https://doi.org/10.1145/3377811.3380327},
  abstract     = {According to the World Health Organization(WHO), it is estimated that approximately 1.3 billion people live with some forms of vision impairment globally, of whom 36 million are blind. Due to their disability, engaging these minority into the society is a challenging problem. The recent rise of smart mobile phones provides a new solution by enabling blind users' convenient access to the information and service for understanding the world. Users with vision impairment can adopt the screen reader embedded in the mobile operating systems to read the content of each screen within the app, and use gestures to interact with the phone. However, the prerequisite of using screen readers is that developers have to add natural-language labels to the image-based components when they are developing the app. Unfortunately, more than 77% apps have issues of missing labels, according to our analysis of 10,408 Android apps. Most of these issues are caused by developers' lack of awareness and knowledge in considering the minority. And even if developers want to add the labels to UI components, they may not come up with concise and clear description as most of them are of no visual issues. To overcome these challenges, we develop a deep-learning based model, called LabelDroid, to automatically predict the labels of image-based buttons by learning from large-scale commercial apps in Google Play. The experimental results show that our model can make accurate predictions and the generated labels are of higher quality than that from real Android developers.},
  numpages     = 13,
  keywords     = {accessibility, image-based buttons, content description, user interface, neural networks}
}
@inproceedings{Peng19,
  title        = {{PersonalTouch: Improving Touchscreen Usability by Personalizing Accessibility Settings Based on Individual User's Touchscreen Interaction}},
  author       = {Peng, Yi-Hao and Lin, Muh-Tarng and Chen, Yi and Chen, TzuChuan and Ku, Pin Sung and Taele, Paul and Lim, Chin Guan and Chen, Mike Y.},
  year         = 2019,
  booktitle    = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  location     = {Glasgow, Scotland Uk},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {CHI '19},
  pages        = {1–11},
  doi          = {10.1145/3290605.3300913},
  isbn         = 9781450359702,
  url          = {https://doi.org/10.1145/3290605.3300913},
  abstract     = {Modern touchscreen devices have recently introduced customizable touchscreen settings to improve accessibility for users with motor impairments. For example, iOS 10 introduced the following four Touch Accommodation settings: 1) Hold Duration, 2) Ignore Repeat, 3) Tap Assistance, and 4) Tap Assistance Gesture Delay. These four independent settings lead to a total of more than 1 million possible configurations, making it impractical to manually determine the optimal settings. We present PersonalTouch, which collects and analyzes touchscreen gestures performed by individual users, and recommends personalized, optimal touchscreen accessibility settings. Results from our user study show that PersonalTouch significantly improves touch input success rate for users with motor impairments (20.2%, N=12, p=.00054) and for users without motor impairments (1.28%, N=12, p=.032).},
  numpages     = 11,
  keywords     = {motor impairment, accessibility, touch-screen interaction, personalization}
}
@inproceedings{Zhang18,
  title        = {{Robust Annotation of Mobile Application Interfaces in Methods for Accessibility Repair and Enhancement}},
  author       = {Zhang, Xiaoyi and Ross, Anne Spencer and Fogarty, James},
  year         = 2018,
  booktitle    = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
  location     = {Berlin, Germany},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {UIST '18},
  pages        = {609–621},
  doi          = {10.1145/3242587.3242616},
  isbn         = 9781450359481,
  url          = {https://doi.org/10.1145/3242587.3242616},
  abstract     = {Accessibility issues in mobile apps make those apps difficult or impossible to access for many people. Examples include elements that fail to provide alternative text for a screen reader, navigation orders that are difficult, or custom widgets that leave key functionality inaccessible. Social annotation techniques have demonstrated compelling approaches to such accessibility concerns in the web, but have been difficult to apply in mobile apps because of the challenges of robustly annotating interfaces. This research develops methods for robust annotation of mobile app interface elements. Designed for use in runtime interface modification, our methods are based in screen identifiers, element identifiers, and screen equivalence heuristics. We implement initial developer tools for annotating mobile app accessibility metadata, evaluate our current screen equivalence heuristics in a dataset of 2038 screens collected from 50 mobile apps, present three case studies implementing runtime repair of common accessibility issues, and examine repair of real-world accessibility issues in 26 apps. These contributions overall demonstrate strong opportunities for social annotation in mobile accessibility.},
  numpages     = 13,
  keywords     = {runtime modification, accessibility, robust annotation}
}
@inproceedings{Alshayban20,
  title        = {{Accessibility Issues in Android Apps: State of Affairs, Sentiments, and Ways Forward}},
  author       = {Alshayban, Abdulaziz and Ahmed, Iftekhar and Malek, Sam},
  year         = 2020,
  booktitle    = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
  location     = {Seoul, South Korea},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ICSE '20},
  pages        = {1323–1334},
  doi          = {10.1145/3377811.3380392},
  isbn         = 9781450371216,
  url          = {https://doi.org/10.1145/3377811.3380392},
  abstract     = {Mobile apps are an integral component of our daily life. Ability to use mobile apps is important for everyone, but arguably even more so for approximately 15% of the world population with disabilities. This paper presents the results of a large-scale empirical study aimed at understanding accessibility of Android apps from three complementary perspectives. First, we analyze the prevalence of accessibility issues in over 1, 000 Android apps. We find that almost all apps are riddled with accessibility issues, hindering their use by disabled people. We then investigate the developer sentiments through a survey aimed at understanding the root causes of so many accessibility issues. We find that in large part developers are unaware of accessibility design principles and analysis tools, and the organizations in which they are employed do not place a premium on accessibility. We finally investigate user ratings and comments on app stores. We find that due to the disproportionately small number of users with disabilities, user ratings and app popularity are not indicative of the extent of accessibility issues in apps. We conclude the paper with several observations that form the foundation for future research and development.},
  numpages     = 12
}
@inproceedings{Debora21,
  title        = {{Accessibility and Software Engineering Processes: A Systematic Literature Review}},
  author       = {D{\'e}bora Maria Barroso Paiva and Andr{\'e} Pimenta Freire and Renata Pontin {de Mattos Fortes}},
  year         = 2021,
  journal      = {Journal of Systems and Software},
  volume       = 171,
  pages        = 110819,
  doi          = {https://doi.org/10.1016/j.jss.2020.110819},
  issn         = {0164-1212},
  url          = {https://www.sciencedirect.com/science/article/pii/S0164121220302168},
  abstract     = {Guidelines, techniques, and methods have been presented in the literature in recent years to contribute to the development of accessible software and to promote digital inclusion. Considering that software product quality depends on the quality of the development process, researchers have investigated how to include accessibility during the software development process in order to obtain accessible software. Two Systematic Literature Reviews (SLR) have been conducted in the past to identify such research initiatives. This paper presents a new SLR, considering the period from 2011 to 2019. The review of 94 primary studies showed the distribution of publications on different phases of the software life cycle, mainly the design and testing phases. The study also identified, for the first time, papers about accessibility and software process establishment. This result reinforces that, in fact, accessibility is not characterized as a property of the final software only. Instead, it evolves over the software life cycle. Besides, this study aims to provide designers and developers with an updated view of methods, tools, and other assets that contribute to process enrichment, valuing accessibility, as well as shows the gaps and challenges which deserve to be investigated.},
  keywords     = {Accessibility, Software Engineering, Systematic Literature Review, Design for disabilities, Methods for accessibility},
  bdsk-url-2   = {https://doi.org/10.1016/j.jss.2020.110819}
}
@url{ADALaws,
  title        = {{ADAlaws \url{https://www.ada.gov/cguide.htm}}},
  year         = 2019
}
@url{CurrentState,
  title        = {{The Current State of Cell Phone Accessibility \url{https://www.afb.org/aw/12/6/15926}}},
  year         = 2011
}
@url{HarvardAccess,
  title        = {{Motor Impairment \url{https://accessibility.huit.harvard.edu/disabilities/motor-impairment?page=1}}},
  year         = 2022
}
@url{GoogleAccess,
  title        = {{Accessibility \url{https://developer.android.com/guide/topics/ui/accessibility}}},
  year         = 2022
}
@url{voiceover,
  title        = {{VoiceOver \url{https://cloud.google.com/translate/docs/}}},
  year         = 2019
}
@inproceedings{Ross18,
  title        = {{Examining Image-Based Button Labeling for Accessibility in Android Apps through Large-Scale Analysis}},
  author       = {Anne Spencer Ross and Xiaoyi Zhang and James Fogarty and Jacob O. Wobbrock},
  year         = 2018,
  month        = oct,
  booktitle    = {Proceedings of the 20th International {ACM} {SIGACCESS} Conference on Computers and Accessibility},
  publisher    = {{ACM}},
  doi          = {10.1145/3234695.3236364},
  url          = {https://doi.org/10.1145/3234695.3236364}
}
@url{ADAWeb,
  title        = {{ADA Web Accessibility Lawsuit Recap Report \url{https://blog.usablenet.com/2018- ada- web- accessibility- lawsuit- recap- report}}},
  year         = 2018
}
@url{MsftAccess,
  title        = {{Accessibility Techonology and Tools | Microsoft Accessibility. Promoting disability inclusion \url{<https://www.microsoft.com/en-us/accessibility}}},
  year         = 2022
}
@url{AccessGov,
  title        = {{Accessibility Guide \url{https://accessibility.18f.gov/checklist/accessibility}}},
  year         = 2022
}
@url{WHO,
  title        = {{World Report on Disability \url{http://www.who.int/disabilities/world_report/2011/en/}}},
  year         = 2011
}
@url{IOSDesign,
  title        = {{Adaptivity and layout - visual design - IOS - human interface guidelines - apple developer \url{https://developer.apple.com/design/human-interface-guidelines/ios/visual-design/adaptivity-and-layout/}}},
  year         = 2019
}
@url{ANDRDesign,
  title        = {{“Design for Android: android developers,” \url{https://developer.android.com/design}}}
}
@inproceedings{MacKenzie11,
  title        = {{1 Thumb, 4 Buttons, 20 Words per Minute: Design and Evaluation of H4-Writer}},
  author       = {MacKenzie, I. Scott and Soukoreff, R. William and Helga, Joanna},
  year         = 2011,
  booktitle    = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
  location     = {Santa Barbara, California, USA},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {UIST '11},
  pages        = {471–480},
  doi          = {10.1145/2047196.2047258},
  isbn         = 9781450307161,
  url          = {https://doi.org/10.1145/2047196.2047258},
  abstract     = {We present what we believe is the most efficient and quickest four-key text entry method available. H4-Writer uses Huffman coding to assign minimized key sequences to letters, with full access to error correction, punctuation, digits, modes, etc. The key sequences are learned quickly, and support eyes-free entry. With KSPC = 2.321, the effort to enter text is comparable to multitap on a mobile phone keypad; yet multitap requires nine keys. In a longitudinal study with six participants, an average text entry speed of 20.4 wpm was observed in the 10th session. Error rates were under 1%. To improve external validity, an extended session was included that required input of punctuation and other symbols. Entry speed dropped only by about 3 wpm, suggesting participants quickly leveraged their acquired skill with H4-Writer to access advanced features.},
  numpages     = 10,
  keywords     = {mobile text entry, small devices, text entry, Huffman coding}
}
}
@inproceedings{Moran18,
  title        = {{Automated Reporting of GUI Design Violations for Mobile Apps}},
  author       = {Moran, Kevin and Li, Boyang and Bernal-C\'{a}rdenas, Carlos and Jelf, Dan and Poshyvanyk, Denys},
  year         = 2018,
  booktitle    = {Proceedings of the 40th International Conference on Software Engineering},
  location     = {Gothenburg, Sweden},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ICSE '18},
  pages        = {165–175},
  doi          = {10.1145/3180155.3180246},
  isbn         = 9781450356381,
  url          = {https://doi.org/10.1145/3180155.3180246},
  abstract     = {The inception of a mobile app often takes form of a mock-up of the Graphical User Interface (GUI), represented as a static image delineating the proper layout and style of GUI widgets that satisfy requirements. Following this initial mock-up, the design artifacts are then handed off to developers whose goal is to accurately implement these GUIs and the desired functionality in code. Given the sizable abstraction gap between mock-ups and code, developers often introduce mistakes related to the GUI that can negatively impact an app's success in highly competitive marketplaces. Moreover, such mistakes are common in the evolutionary context of rapidly changing apps. This leads to the time-consuming and laborious task of design teams verifying that each screen of an app was implemented according to intended design specifications.This paper introduces a novel, automated approach for verifying whether the GUI of a mobile app was implemented according to its intended design. Our approach resolves GUI-related information from both implemented apps and mock-ups and uses computer vision techniques to identify common errors in the implementations of mobile GUIs. We implemented this approach for Android in a tool called Gvt and carried out both a controlled empirical evaluation with open-source apps as well as an industrial evaluation with designers and developers from Huawei. The results show that Gvt solves an important, difficult, and highly practical problem with remarkable efficiency and accuracy and is both useful and scalable from the point of view of industrial designers and developers. The tool is currently used by over one-thousand industrial designers amp; developers at Huawei to improve the quality of their mobile apps.},
  numpages     = 11
}
@article{Nunes15,
  title        = {{User interface design guidelines for smartphone applications for people with Parkinson's disease}},
  author       = {Francisco Nunes and Paula Alexandra Silva and Jo{\~{a}}o Cevada and Ana Correia Barros and Lu{\'{\i}}s Teixeira},
  year         = 2015,
  month        = oct,
  journal      = {Universal Access in the Information Society},
  publisher    = {Springer Science and Business Media {LLC}},
  volume       = 15,
  number       = 4,
  pages        = {659--679},
  doi          = {10.1007/s10209-015-0440-1},
  url          = {https://doi.org/10.1007/s10209-015-0440-1}
}
@inproceedings{Eler18,
  title        = {{Automated Accessibility Testing of Mobile Apps}},
  author       = {Marcelo Medeiros Eler and Jose Miguel Rojas and Yan Ge and Gordon Fraser},
  year         = 2018,
  month        = apr,
  booktitle    = {2018 {IEEE} 11th International Conference on Software Testing,  Verification and Validation ({ICST})},
  publisher    = {{IEEE}},
  doi          = {10.1109/icst.2018.00021},
  url          = {https://doi.org/10.1109/icst.2018.00021}
}
@inproceedings{glove,
  title        = {{Glove: Global Vectors for Word Representation}},
  author       = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  year         = 2014,
  booktitle    = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2014, October 25-29, 2014, Doha, Qatar, {A} meeting of SIGDAT, a Special Interest Group of the {ACL}},
  publisher    = {{ACL}},
  pages        = {1532--1543},
  doi          = {10.3115/v1/d14-1162},
  url          = {https://doi.org/10.3115/v1/d14-1162},
  editor       = {Alessandro Moschitti and Bo Pang and Walter Daelemans},
  timestamp    = {Fri, 06 Aug 2021 00:40:40 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/PenningtonSM14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Kane11,
  title        = {{Access overlays}},
  author       = {Shaun K. Kane and Meredith Ringel Morris and Annuska Z. Perkins and Daniel Wigdor and Richard E. Ladner and Jacob O. Wobbrock},
  year         = 2011,
  month        = oct,
  booktitle    = {Proceedings of the 24th annual {ACM} symposium on User interface software and technology},
  publisher    = {{ACM}},
  doi          = {10.1145/2047196.2047232},
  url          = {https://doi.org/10.1145/2047196.2047232}
}
@inproceedings{Abascal11,
  title        = {{Automatically Generating Tailored Accessible User Interfaces for Ubiquitous Services}},
  author       = {Abascal, Julio and Aizpurua, Amaia and Cearreta, Idoia and Gamecho, Borja and Garay-Vitoria, Nestor and Mi\~{n}\'{o}n, Ra\'{u}l},
  year         = 2011,
  booktitle    = {The Proceedings of the 13th International ACM SIGACCESS Conference on Computers and Accessibility},
  location     = {Dundee, Scotland, UK},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASSETS '11},
  pages        = {187–194},
  doi          = {10.1145/2049536.2049570},
  isbn         = 9781450309202,
  url          = {https://doi.org/10.1145/2049536.2049570},
  abstract     = {Ambient Assisted Living environments provide support to people with disabilities and elderly people, usually at home. This concept can be extended to public spaces, where ubiquitous accessible services allow people with disabilities to access intelligent machines such as information kiosks. One of the key issues in achieving full accessibility is the instantaneous generation of an adapted accessible interface suited to the specific user that requests the service. In this paper we present the method used by the EGOKI interface generator to select the most suitable interaction resources and modalities for each user in the automatic creation of the interface. The validation of the interfaces generated for four different types of users is presented and discussed.},
  numpages     = 8,
  keywords     = {ubiquitous computing, automatic user interface generation, adaptive systems, accessible user interfaces}
}
@inproceedings{Gajos07,
  title        = {{Automatically Generating User Interfaces Adapted to Users' Motor and Vision Capabilities}},
  author       = {Gajos, Krzysztof Z. and Wobbrock, Jacob O. and Weld, Daniel S.},
  year         = 2007,
  booktitle    = {Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology},
  location     = {Newport, Rhode Island, USA},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {UIST '07},
  pages        = {231–240},
  doi          = {10.1145/1294211.1294253},
  isbn         = 9781595936790,
  url          = {https://doi.org/10.1145/1294211.1294253},
  abstract     = {Most of today's GUIs are designed for the typical, able-bodied user; atypical users are, for the most part, left to adapt as best they can, perhaps using specialized assistive technologies as an aid. In this paper, we present an alternative approach: SUPPLE++ automatically generates interfaces which are tailored to an individual's motor capabilities and can be easily adjusted to accommodate varying vision capabilities. SUPPLE++ models users. motor capabilities based on a onetime motor performance test and uses this model in an optimization process, generating a personalized interface. A preliminary study indicates that while there is still room for improvement, SUPPLE++ allowed one user to complete tasks that she could not perform using a standard interface, while for the remaining users it resulted in an average time savings of 20%, ranging from an slowdown of 3% to a speedup of 43%.},
  numpages     = 10,
  keywords     = {supple, motor impairments, optimization, multiple impairments, vision impairments, decision theory}
}
@url{WebGuide,
  title        = {{Web content accessibility guidelines (WCAG)}},
  year         = 2022,
  howpublished = {\url{https://www.w3.org/TR/WCAG21/}, journal={W3C}}
}
@inproceedings{Moran:ICST'16,
  title        = {{Automatically Discovering, Reporting and Reproducing Android Application Crashes}},
  author       = {Moran, Kevin and Linares-Vásquez, Mario and Bernal-Cárdenas, Carlos and Vendome, Christopher and Poshyvanyk, Denys},
  year         = 2016,
  booktitle    = {2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)},
  volume       = {},
  number       = {},
  pages        = {33--44},
  doi          = {10.1109/ICST.2016.34}
}
@article{crashscope,
  title        = {{CrashScope: A Practical Tool for Automated Testing of Android Applications}},
  author       = {Moran,  Kevin and Linares-Vasquez,  Mario and Bernal-Cardenas,  Carlos and Vendome,  Christopher and Poshyvanyk,  Denys},
  publisher    = {arXiv},
  doi          = {10.48550/ARXIV.1801.06428},
  url          = {https://arxiv.org/abs/1801.06428},
  copyright    = {arXiv.org perpetual,  non-exclusive license},
  keywords     = {Software Engineering (cs.SE),  FOS: Computer and information sciences,  FOS: Computer and information sciences}
}
@inproceedings{Parhi06,
  title        = {{Target Size Study for One-Handed Thumb Use on Small Touchscreen Devices}},
  author       = {Parhi, Pekka and Karlson, Amy K. and Bederson, Benjamin B.},
  year         = 2006,
  booktitle    = {Proceedings of the 8th Conference on Human-Computer Interaction with Mobile Devices and Services},
  location     = {Helsinki, Finland},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {MobileHCI '06},
  pages        = {203–210},
  doi          = {10.1145/1152215.1152260},
  isbn         = 1595933905,
  url          = {https://doi.org/10.1145/1152215.1152260},
  abstract     = {This paper describes a two-phase study conducted to determine optimal target sizes for one-handed thumb use of mobile handheld devices equipped with a touch-sensitive screen. Similar studies have provided recommendations for target sizes when using a mobile device with two hands plus a stylus, and interacting with a desktop-sized display with an index finger, but never for thumbs when holding a small device in a single hand. The first phase explored the required target size for single-target (discrete) pointing tasks, such as activating buttons, radio buttons or checkboxes. The second phase investigated optimal sizes for widgets used for tasks that involve a sequence of taps (serial), such as text entry. Since holding a device in one hand constrains thumb movement, we varied target positions to determine if performance depended on screen location. The results showed that while speed generally improved as targets grew, there were no significant differences in error rate between target sizes =9.6 mm in discrete tasks and targets =7.7 mm in serial tasks. Along with subjective ratings and the findings on hit response variability, we found that target size of 9.2 mm for discrete tasks and targets of 9.6 mm for serial tasks should be sufficiently large for one-handed thumb use on touchscreen-based handhelds without degrading performance and preference.},
  numpages     = 8,
  keywords     = {key size, keypads, mobile devices, one-handed, touch screens}
}
@inproceedings{Kong21,
  title        = {{New Metrics for Understanding Touch by People with and without Limited Fine Motor Function}},
  author       = {Kong, Junhan and Zhong, Mingyuan and Fogarty, James and Wobbrock, Jacob O.},
  year         = 2021,
  booktitle    = {Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility},
  location     = {Virtual Event, USA},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASSETS '21},
  doi          = {10.1145/3441852.3476559},
  isbn         = 9781450383066,
  url          = {https://doi.org/10.1145/3441852.3476559},
  abstract     = {Current performance measures with touch-based systems usually focus on overall performance, such as touch accuracy and target acquisition speed. But a touch is not an atomic event; it is a process that unfolds over time, and this process can be characterized to gain insight into users’ touch behaviors. To this end, our work proposes 13 target-agnostic touch performance metrics to characterize what happens during a touch. These metrics are: touch direction, variability, drift, duration, extent, absolute/signed area change, area variability, area deviation, absolute/signed angle change, angle variability, and angle deviation. Unlike traditional touch performance measures that treat a touch as a single (x, y) coordinate, we regard a touch as a time series of ovals that occur from finger-down to finger-up. We provide a mathematical formula and intuitive description for each metric we propose. To evaluate our metrics, we run an analysis on a publicly available dataset containing touch inputs by people with and without limited fine motor function, finding our metrics helpful in characterizing different fine motor control challenges. Our metrics can be useful to designers and evaluators of touch-based systems, particularly when making touch screens accessible to all forms of touch.},
  articleno    = 80,
  numpages     = 4,
  keywords     = {limited fine motor function, Understanding touch}
}
@misc{UIED,
  title        = {{UIED - UI element detection, detecting UI elements from UI screenshots or drawnings}},
  author       = {MulongXie},
  year         = 2021,
  url          = {https://github.com/MulongXie/UIED}
}
@misc{ocr,
  title        = {{Google Cloud Vision API}},
  journal      = {Google},
  publisher    = {Google},
  howpublished = {\url{https://cloud.google.com/vision/docs/ocr}}
}
@inproceedings{FlrezAristizbal19,
  title        = {{DesignABILITY}},
  author       = {Leandro Fl{\'{o}}rez-Aristiz{\'{a}}bal and Sandra Cano and C{\'{e}}sar A. Collazos and Andr{\'{e}}s F. Solano and Stephen Brewster},
  year         = 2019,
  month        = may,
  booktitle    = {Proceedings of the 2019 {CHI} Conference on Human Factors in Computing Systems},
  publisher    = {{ACM}},
  doi          = {10.1145/3290605.3300240},
  url          = {https://doi.org/10.1145/3290605.3300240}
}
@inproceedings{Milne18,
  title        = {{Blocks4All}},
  author       = {Lauren R. Milne and Richard E. Ladner},
  year         = 2018,
  month        = apr,
  booktitle    = {CHI'18},
  publisher    = {{ACM}},
  doi          = {10.1145/3173574.3173643},
  url          = {https://doi.org/10.1145/3173574.3173643}
}
@inproceedings{Li21,
  title        = {{Accessibility of High-Fidelity Prototyping Tools}},
  author       = {Junchen Li and Garreth W. Tigwell and Kristen Shinohara},
  year         = 2021,
  month        = may,
  booktitle    = {Proceedings of the 2021 {CHI} Conference on Human Factors in Computing Systems},
  publisher    = {{ACM}},
  doi          = {10.1145/3411764.3445520},
  url          = {https://doi.org/10.1145/3411764.3445520}
}
@inproceedings{Pavel20,
  title        = {{Rescribe}},
  author       = {Amy Pavel and Gabriel Reyes and Jeffrey P. Bigham},
  year         = 2020,
  month        = oct,
  booktitle    = {Proceedings of the 33rd Annual {ACM} Symposium on User Interface Software and Technology},
  publisher    = {{ACM}},
  doi          = {10.1145/3379337.3415864},
  url          = {https://doi.org/10.1145/3379337.3415864}
}
@inproceedings{Chiou21,
  title        = {{Detecting and Localizing Keyboard Accessibility Failures in Web Applications}},
  author       = {Chiou, Paul T. and Alotaibi, Ali S. and Halfond, William G. J.},
  year         = 2021,
  booktitle    = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  location     = {Athens, Greece},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ESEC/FSE 2021},
  pages        = {855–867},
  doi          = {10.1145/3468264.3468581},
  isbn         = 9781450385626,
  url          = {https://doi.org/10.1145/3468264.3468581},
  abstract     = {The keyboard is the most universally supported input method operable by people with disabilities. Yet, many popular websites lack keyboard accessible mechanism, which could cause failures that make the website unusable. In this paper, we present a novel approach for automatically detecting and localizing keyboard accessibility failures in web applications. Our extensive evaluation of our technique on real world web pages showed that our technique was able to detect keyboard failures in web applications with high precision and recall and was able to accurately identify the underlying elements in the web pages that led to the observed problems.},
  numpages     = 13,
  keywords     = {Web Accessibility, Software Testing, WCAG, Keyboard Navigation}
}
@inproceedings{Astler11,
  title        = {{Increased Accessibility to Nonverbal Communication through Facial and Expression Recognition Technologies for Blind/Visually Impaired Subjects}},
  author       = {Astler, Douglas and Chau, Harrison and Hsu, Kailin and Hua, Alvin and Kannan, Andrew and Lei, Lydia and Nathanson, Melissa and Paryavi, Esmaeel and Rosen, Michelle and Unno, Hayato and Wang, Carol and Zaidi, Khadija and Zhang, Xuemin and Tang, Cha-Min},
  year         = 2011,
  booktitle    = {The Proceedings of the 13th International ACM SIGACCESS Conference on Computers and Accessibility},
  location     = {Dundee, Scotland, UK},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASSETS '11},
  pages        = {259–260},
  doi          = {10.1145/2049536.2049596},
  isbn         = 9781450309202,
  url          = {https://doi.org/10.1145/2049536.2049596},
  abstract     = {Conversation between two individuals requires verbal dialogue; the majority of human communication however consists of non-verbal cues such as gestures and facial expressions. Blind individuals are thus hindered in their interaction capabilities. To address this, we are building a computer vision system with facial recognition and expression algorithms to relay nonverbal messages to a blind user. The device will communicate the identities and facial expressions of communication partners in realtime. In order to ensure that this device will be useful to the blind community, we conducted surveys and interviews and we are working with subjects to test prototypes of the device. This paper describes the algorithms and design concepts incorporated in this device, and it provides a commentary on early survey and interview results. A corresponding poster with demonstration stills is exhibited at this conference.},
  numpages     = 2,
  keywords     = {blindness, face recognition, computer vision, assistive technologies, expression recognition}
}
@url{torchvision,
  title        = {{Torchvision- torchvision main documentation}},
  year         = 2022,
  journal      = {torchvision - Torchvision main documentation},
  howpublished = {\url{https://pytorch.org/vision/stable/index.html}}
}
@inproceedings{Calvo16,
  title        = {{Beyond Web Content Accessibility Guidelines: Expert Accessibility Reviews}},
  author       = {Calvo, Rocio and Seyedarabi, Faezeh and Savva, Andreas},
  year         = 2016,
  booktitle    = {Proceedings of the 7th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
  location     = {Vila Real, Portugal},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {DSAI 2016},
  pages        = {77–84},
  doi          = {10.1145/3019943.3019955},
  isbn         = 9781450347488,
  url          = {https://doi.org/10.1145/3019943.3019955},
  abstract     = {Despite the existence of web accessibility guidelines - e.g. Web Content Accessibility Guidelines - to help developers and designers to create more accessible websites, many websites are still not accessible. Studies showed that guidelines do not cover all the problems encounter by users with disabilities. This study investigates the problems found by seven accessibility experts in 62 accessibility evaluation reviews of mobile and desktop websites as well as mobile applications. Each evaluation conducted an accessibility review using Web Content Accessibility Guidelines 2.0 AA. In addition, experts highlighted potential issues which were not covered by these guidelines but could be potential issues encountered by people with disabilities. The study depicts those issues that were raised by experts during those audits but were not covered by the WCAG 2.0 guidelines. Finally, the study provides additional knowledge into potential problems as identified by experts and recommends techniques that developers and designers can use to create more accessible websites.},
  numpages     = 8,
  keywords     = {Expert Evaluation, Heuristic Evaluation, Web Content Accessibility Guidelines, Web Accessibility}
}
@article{Sarsenbayeva22,
  title        = {{Methodological Standards in Accessibility Research on Motor Impairments: A Survey}},
  author       = {Sarsenbayeva, Zhanna and van Berkel, Niels and Velloso, Eduardo and Goncalves, Jorge and Kostakos, Vassilis},
  year         = 2022,
  month        = {may},
  journal      = {ACM Comput. Surv.},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  doi          = {10.1145/3543509},
  issn         = {0360-0300},
  url          = {https://doi.org/10.1145/3543509},
  note         = {Just Accepted},
  abstract     = {The design and evaluation of accessibility technology is a core component of the Computer Science landscape, aiming to ensure that digital innovations are accessible to all. One of the most prominent and long-lasting areas of accessibility research focuses on motor impairments, deficiencies that affect the ability to move, manipulate objects, and interact with the physical world. In this survey paper, we present an extensive overview of the last two decades of research into accessibility for people with motor impairments. Following a structured selection process, we analysed the study details as reported in 177 relevant papers. Based on this analysis, we critically assess user representation, measurement instruments, and existing barriers that exist in accessibility research. Finally, we discuss future directions for accessibility research within the Computer Science domain.},
  keywords     = {Accessibility, methodology, motor impairments, motor-impaired users}
}
@article{Steriadis03,
  title        = {{Designing Human-Computer Interfaces for Quadriplegic People}},
  author       = {Steriadis, Constantine E. and Constantinou, Philip},
  year         = 2003,
  month        = {jun},
  journal      = {ACM Trans. Comput.-Hum. Interact.},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  volume       = 10,
  number       = 2,
  pages        = {87–118},
  doi          = {10.1145/772047.772049},
  issn         = {1073-0516},
  url          = {https://doi.org/10.1145/772047.772049},
  issue_date   = {June 2003},
  abstract     = {The need for participation in an emerging Information Society has led to several research efforts for designing accessibility solutions for disabled people. In this paper we present a method for developing Human-Computer Interfaces (HCIs) for quadriplegic people in modern programming environments. The presented method accommodates the design of scanning interfaces with modern programming tools, leading to flexible interfaces with improved appearance and it is based on the use of specially designed software objects called "wifsids" (Widgets For Single-switch Input Devices). The wifsid structure is demonstrated and 4 types of wifsids are analyzed. Developed software applications are to be operated by single-switch activations that are captured through the wifsids, with the employment of several modes of the scanning technique. We also demonstrate the "Autonomia" software application, that has been developed according to the specific methodology. The basic snapshots of this application are analyzed, in order to demonstrate how the wifsids cooperate with the scanning process in a user-friendly environment that enables a quadriplegic person to access an ordinary computer system.},
  numpages     = 32,
  keywords     = {augmentative communications, wifsid, scanning selection, quadriplegic people, disability, word-prediction, single-switch input, Accessibility, assistive technology, graphical keyboard, mouse simulation, motor-impaired users}
}
@article{Song18,
  title        = {{Crowdsourcing-based web accessibility evaluation with golden maximum likelihood inference}},
  author       = {Song, Shuyi and Bu, Jiajun and Artmeier, Andreas and Shi, Keyue and Wang, Ye and Yu, Zhi and Wang, Can},
  year         = 2018,
  month        = nov,
  journal      = {Proc. ACM Hum. Comput. Interact.},
  publisher    = {ACM (ACM)},
  volume       = 2,
  number       = {CSCW},
  pages        = {1--21},
  copyright    = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
  abstract     = {Web accessibility evaluation examines how well websites comply with accessibility guidelines which help people with disabilities to perceive, navigate and contribute to the Web. This demanding task usually requires manual assessment by experts with many years of training and experience. However, not enough experts are available to carry out the increasing number of evaluation projects while non-experts often have different opinions about the presence of accessibility barriers. Addressing these issues, we introduce a crowdsourcing system with a novel truth inference algorithm to derive reliable and accurate assessments from conflicting opinions of evaluators. Extensive evaluation on 23,901 complex tasks assessed by 50 people with and without disabilities shows that our approach outperforms state of the art approaches. In addition, we conducted surveys to identify frequent barriers that people with disabilities are facing in their daily lives and the difficulty to access Web pages when they encounter these barriers. The frequencies and severities of barriers correlate with their derived importance in our evaluation project.},
  language     = {en}
}
@incollection{Correia20,
  title        = {{Virtual assistants: An accessibility assessment in virtual assistants for people with motor disability on mobile devices}},
  author       = {Correia, Walter and Macedo, Jefte and Penha, Marcelo and Quintino, Jonysberg and Pellegrini, Fernanda and Anjos, Marcelo and Florentin, Fabiana and Santos, Andre and Da Silva, Fabio Q B},
  year         = 2020,
  booktitle    = {Advances in Intelligent Systems and Computing},
  publisher    = {Springer International Publishing},
  address      = {Cham},
  series       = {Advances in intelligent systems and computing},
  pages        = {239--249}
}
@inproceedings{Perez16,
  title        = {{A usability evaluation of two virtual aids to enhance cursor accessibility for people with motor impairments}},
  author       = {P{\'e}rez, J Eduardo and Valencia, Xabier and Arrue, Myriam and Abascal, Julio},
  year         = 2016,
  month        = apr,
  booktitle    = {Proceedings of the 13th International Web for All Conference},
  location     = {Montreal Canada},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  copyright    = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
  abstract     = {Basic actions in the context of Web browsing, such as pointing at and clicking on links, can be seriously hindered by dexterity impairments affecting the use of hands and arms. In this paper, we present two different virtual aids for assisting motor-impaired users when pointing at and clicking on links. One of them, the ``circular cursor'', aims at reducing the level of accuracy required for clicking on links, whereas the other one, the ``cross cursor'', aims at reducing target distance for pointing at them. We conducted a web-based usability testing for both cursors with 9 motor-impaired and 6 able-bodied users applying their usual pointing device (4 keyboard, 4 joystick, 1 trackball and 6 mouse). The results show that motor-impaired participants mainly preferred one of either of the two variants proposed to the traditional cursor without any virtual aid for Web browsing.},
  conference   = {W4A'16: International Web for All Conference}
}
@inproceedings{Kim13,
  title        = {{Surveying the accessibility of touchscreen games for persons with motor impairments}},
  author       = {Kim, Yoojin and Sutreja, Nita and Froehlich, Jon and Findlater, Leah},
  year         = 2013,
  month        = oct,
  booktitle    = {Proceedings of the 15th International {ACM} {SIGACCESS} Conference on Computers and Accessibility},
  location     = {Bellevue Washington},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  conference   = {ASSETS '13: The 15th International ACM SIGACCESS Conference on Computers and Accessibility}
}
@inproceedings{Creed14,
  title        = {{Enhancing multi-touch table accessibility for wheelchair users}},
  author       = {Creed, Chris and Beale, Russell},
  year         = 2014,
  booktitle    = {Proceedings of the 16th international {ACM} {SIGACCESS} conference on Computers \& accessibility - {ASSETS} '14},
  location     = {Rochester, New York, USA},
  publisher    = {ACM Press},
  address      = {New York, New York, USA},
  abstract     = {Wheelchair users can find accessing digital content on large multi-touch tables particularly difficult and frustrating due to their limited reach. We present work in progress that is exploring the potential of enhancing touch table accessibility through the use of mid-air gesturing technology. An overview of an experimental prototype is provided along with the key findings from an evaluation conducted with fifteen wheelchair users at a public library and heritage centre.},
  conference   = {the 16th international ACM SIGACCESS conference}
}
@inproceedings{Brewster96,
  title        = {{Enhancing scanning input with non-speech sounds}},
  author       = {Brewster, Stephen A and Raty, Veli-Pekka and Kortekangas, Atte},
  year         = 1996,
  booktitle    = {Proceedings of the second annual {ACM} conference on Assistive technologies - Assets '96},
  location     = {Vancouver, British Columbia, Canada},
  publisher    = {ACM Press},
  address      = {New York, New York, USA},
  abstract     = {This paper proposes the addition of non-speech sounds to aid people who use scanning as their method of input. Scanning input is a temporal task; users have to press a switch when a cursor is over the required target. However, it is usually presented as a spatial task with the items to be scanned laid-out in a grid. Research has shown that for temporal tasks the auditory modality is often better than the visual. This paper investigates this by adding non-speech sound to a visual scanning system. It also shows how our natural abilities to perceive rhythms can be supported so that they can be used to aid the scanning process. Structured audio messages called Earcons were used for the sound output. The results from a preliminary investigation were favourable, indicating that the idea is feasible and further research should be undertaken.},
  conference   = {the second annual ACM conference}
}
@inproceedings{Folmer11,
  title        = {{Navigating a 3D avatar using a single switch}},
  author       = {Folmer, Eelke and Liu, Fangzhou and Ellis, Barrie},
  year         = 2011,
  booktitle    = {Proceedings of the 6th International Conference on Foundations of Digital Games - {FDG} '11},
  location     = {Bordeaux, France},
  publisher    = {ACM Press},
  address      = {New York, New York, USA},
  conference   = {the 6th International Conference}
}
@article{Lopez17,
  title        = {{Design and development of one-switch video games for children with severe motor disabilities}},
  author       = {L{\'o}pez, Sebasti{\'a}n Aced and Corno, Fulvio and De Russis, Luigi},
  year         = 2017,
  month        = oct,
  journal      = {ACM Trans. Access. Comput.},
  publisher    = {ACM (ACM)},
  volume       = 10,
  number       = 4,
  pages        = {1--42},
  copyright    = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
  abstract     = {Video games are not just played for fun; they have become a handy instrument for the cognitive, emotional, and social development of children. However, several barriers prevent many children with disabilities from playing action-oriented video games, alone or with their peers. In particular, children with severe motor disabilities, who rely on one-switch interaction for accessing electronic devices, find fast-paced games that require rapid decision-making and timely responses, completely unplayable. This article contributes to lowering such barriers by presenting GNomon ( G aming NOMON ), a software framework based on the NOMON mode of interaction that allows the creation of action-oriented single-switch video games. The article reports the results of two studies that evaluate the playability and rehabilitation suitability of GNomon-based video games. The playability of GNomon-based games is evaluated by assessing their learnability, effectiveness, errors, satisfaction, memorability, and enjoyability with a group of eight children with severe motor disabilities. The suitability for pediatric rehabilitation is determined by means of a focus group with a team of speech therapists, physiotherapists, and psychologists from a Local Health Agency in Turin, Italy. The results of the playability study are positive: All children had fun playing GNomon-based video games, and seven of eight were able to interact and play autonomously. The results of the rehabilitation-suitability study also entail that GNomon-based games can be exploited in training hand-eye coordination and maintenance of selective attention over time. The article finally offers critical hindsight and reflections and shows possible new future game concepts.},
  language     = {en}
}
@inproceedings{axeray,
  title        = {{Semantic Web Accessibility Testing via Hierarchical Visual Analysis}},
  author       = {Bajammal, Mohammad and Mesbah, Ali},
  year         = 2021,
  booktitle    = {2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
  volume       = {},
  number       = {},
  pages        = {1610--1621},
  doi          = {10.1109/ICSE43902.2021.00143}
}
@inproceedings{Vatvu17,
  title        = {{Improving Gesture Recognition Accuracy on Touch Screens for Users with Low Vision}},
  author       = {Vatavu, Radu-Daniel},
  year         = 2017,
  booktitle    = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
  location     = {Denver, Colorado, USA},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {CHI '17},
  pages        = {4667–4679},
  doi          = {10.1145/3025453.3025941},
  isbn         = 9781450346559,
  url          = {https://doi.org/10.1145/3025453.3025941},
  abstract     = {We contribute in this work on gesture recognition to improve the accessibility of touch screens for people with low vision. We examine the accuracy of popular recognizers for gestures produced by people with and without visual impairments, and we show that the user-independent accuracy of $P, the best recognizer among those evaluated, is small for people with low vision (83.8%), despite $P being very effective for gestures produced by people without visual impairments (95.9%). By carefully analyzing the gesture articulations produced by people with low vision, we inform key algorithmic revisions for the P recognizer, which we call P+. We show significant accuracy improvements of $P+ for gestures produced by people with low vision, from 83.8% to 94.7% on average and up to 98.2%, and 3x faster execution times compared to P.},
  numpages     = 13,
  keywords     = {P+, recognition, visual impairments, P, gesture recognition, algorithms, evaluation, point clouds, touch screens, recognition accuracy, low vision, 1, touch gestures}
}
@inproceedings{Park14,
  title        = {{Toward Accessible Mobile Application Design: Developing Mobile Application Accessibility Guidelines for People with Visual Impairment}},
  author       = {Park, Kyudong and Goh, Taedong and So, Hyo-Jeong},
  year         = 2014,
  booktitle    = {Proceedings of HCI Korea},
  location     = {Seoul, Republic of Korea},
  publisher    = {Hanbit Media, Inc.},
  address      = {Seoul, KOR},
  series       = {HCIK '15},
  pages        = {31–38},
  isbn         = 9788968487521,
  abstract     = {While the use of Smartphones has improved the life of people with disabilities, several mobile content and applications remain inaccessible to people with visual impairment. Toward the overarching goal of accessible mobile application design, this two-phased study attempts to develop mobile application accessibility guidelines for people with visual impairment. First, we investigated how people with visual impairment use mobile phones. Four participants with visual impairment performed specified tasks. Their usage patterns and follow-up interviews were analyzed. Serious accessibility problems were found in both typing and VoiceOver functions. Second, we evaluated and developed systematic guidelines and standards of designing accessible mobile applications through a heuristic walkthrough method. Four experts with extensive experiences and knowledge about mobile application development/design used the VoiceOver function with iPhone for 5 days, and then walked through thought-provoking tasks. In conclusion, we propose a set of 10 heuristics for developing accessible mobile applications and suggest a critical need for internationally-agreed guidelines and standards to improve the current mobile environment for people with disabilities.},
  numpages     = 8,
  keywords     = {evaluation, heuristic walkthrough, design guideline, standardization, VoiceOver}
}
@inproceedings{Yeh11,
  title        = {{Creating Contextual Help for GUIs Using Screenshots}},
  author       = {Yeh, Tom and Chang, Tsung-Hsiang and Xie, Bo and Walsh, Greg and Watkins, Ivan and Wongsuphasawat, Krist and Huang, Man and Davis, Larry S. and Bederson, Benjamin B.},
  year         = 2011,
  booktitle    = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
  location     = {Santa Barbara, California, USA},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {UIST '11},
  pages        = {145–154},
  doi          = {10.1145/2047196.2047214},
  isbn         = 9781450307161,
  url          = {https://doi.org/10.1145/2047196.2047214},
  abstract     = {Contextual help is effective for learning how to use GUIs by showing instructions and highlights on the actual interface rather than in a separate viewer. However, end-users and third-party tech support typically cannot create contextual help to assist other users because it requires programming skill and source code access. We present a creation tool for contextual help that allows users to apply common computer skills-taking screenshots and writing simple scripts. We perform pixel analysis on screenshots to make this tool applicable to a wide range of applications and platforms without source code access. We evaluated the tool's usability with three groups of participants: developers, in-structors, and tech support. We further validated the applicability of our tool with 60 real tasks supported by the tech support of a university campus.},
  numpages     = 10,
  keywords     = {pixel analysis, help, contextual help}
}
@inproceedings{Stuerzlinger06,
  title        = {{User Interface Fa\c{c}ades: Towards Fully Adaptable User Interfaces}},
  author       = {Stuerzlinger, Wolfgang and Chapuis, Olivier and Phillips, Dusty and Roussel, Nicolas},
  year         = 2006,
  booktitle    = {Proceedings of the 19th Annual ACM Symposium on User Interface Software and Technology},
  location     = {Montreux, Switzerland},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {UIST '06},
  pages        = {309–318},
  doi          = {10.1145/1166253.1166301},
  isbn         = 1595933131,
  url          = {https://doi.org/10.1145/1166253.1166301},
  abstract     = {User interfaces are becoming more and more complex. Adaptable and adaptive interfaces have been proposed to address this issue and previous studies have shown that users prefer interfaces that they can adapt to self-adjusting ones. However, most existing systems provide users with little support for adapting their interfaces. Interface customization techniques are still very primitive and usually constricted to particular applications. In this paper, we present User Interface Fa\c{c}ades, a system that provides users with simple ways to adapt, reconfigure, and re-combine existing graphical interfaces, through the use of direct manipulation techniques. The paper describes the user's view of the system, provides some technical details, and presents several examples to illustrate its potential.},
  numpages     = 10,
  keywords     = {adaptable user interfaces}
}
STARTING POINT============================================
@inproceedings{Mchugh20,
  title        = {{Assistive Technology Design as a Computer Science Learning Experience}},
  author       = {McHugh, Thomas B. and Barth, Cooper},
  year         = 2020,
  booktitle    = {Proceedings of the 22nd International ACM SIGACCESS Conference on Computers and Accessibility},
  location     = {Virtual Event, Greece},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASSETS '20},
  doi          = {10.1145/3373625.3417081},
  isbn         = 9781450371032,
  url          = {https://doi.org/10.1145/3373625.3417081},
  abstract     = {As awareness surrounding the importance of developing accessible applications has grown, work to integrate inclusive design into computer science (CS) curriculum has gained traction. However, there remain obstacles to integrating accessibility into introductory CS coursework. In this paper, we discuss current challenges to building assistive technology and the findings of a formative study exploring the role of accessibility in an undergraduate CS curriculum. We respond to the observed obstacles by presenting V11, a cross-platform programming interface to empower novice CS students to build assistive technology. To evaluate the effectiveness of V11 as a CS and accessibility learning tool, we conducted design workshops with ten undergraduate CS students, who brainstormed solutions to a real accessibility problem and then used V11 to prototype their solution. Post-workshop evaluations showed a 28\% average increase in student interest in building accessible technology, and V11 was rated easier to use than other accessibility programming tools. Student reflections indicate that V11 can be an accessibility learning tool, while also teaching fundamental Computer Science concepts.},
  articleno    = 100,
  numpages     = 4,
  keywords     = {accessibility, assistive technology, computer science education, inclusive design, allyship}
}
@inproceedings{Almeida10,
  title        = {{Universal Design Principles Combined with Web Accessibility Guidelines: A Case Study}},
  author       = {Almeida, Leonelo Dell Anhol and Baranauskas, Maria Cec\'{\i}lia Calani},
  year         = 2010,
  booktitle    = {Proceedings of the IX Symposium on Human Factors in Computing Systems},
  location     = {Belo Horizonte, Minas Gerais, Brazil},
  publisher    = {Brazilian Computer Society},
  address      = {Porto Alegre, BRA},
  series       = {IHC '10},
  pages        = {169–178},
  abstract     = {Accessibility evaluation processes currently face important issues. One is the difficulty to understand the issues pointed out in accessibility guidelines and the procedure to correct them. Other regards the coverage of the accessibility guidelines in relation to the whole technical and social aspects, and contexts of use. Those deficiencies contribute to the low adherence to recommendations and naive solutions since designers usually are not experts in accessibility. Aiming at contributing to this scenario in clarifying understanding and action towards the development of accessible code we proposed a merging of accessibility guidelines for web content from WCAG 2.0 and accessibility physical recommendations from ISO 9241 to Universal Design principles and guidelines. To evaluate our approach we conducted a case study involving students in computer science and specialists in web accessibility. As results we verified relevant contributions regarding the number and quality of the identified accessibility issues even for the specialists in accessibility.},
  numpages     = 10,
  keywords     = {universal design, web accessibility guidelines, evaluation}
}
@article{Yan19,
  title        = {{The Current Status of Accessibility in Mobile Apps}},
  author       = {Yan, Shunguo and Ramachandran, P. G.},
  year         = 2019,
  month        = {feb},
  journal      = {ACM Trans. Access. Comput.},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  volume       = 12,
  number       = 1,
  doi          = {10.1145/3300176},
  issn         = {1936-7228},
  url          = {https://doi.org/10.1145/3300176},
  issue_date   = {March 2019},
  abstract     = {This study evaluated the status of accessibility in mobile apps by investigating the graphical user interface (GUI) structures and conformance to accessibility guidelines of 479 Android apps in 23 business categories from Google Play. An automated tool, IBM Mobile Accessibility Checker (MAC), was used to identify the accessibility issues, which were categorized as a violation (V), potential violation (PV), or warning (W). The results showed 94.8\%, 97.5\%, and 66.4\% of apps studied contained issues related to V, PV, or W, respectively. Five widget categories (TextView, ImageView, View, Button, and ImageButton) were used to create 92\% of the total number of the GUI elements and caused 89\%, 78\%, and 86\% of V, PV, and W, respectively. These accessibility issues were mainly caused by lack of element focus, missing element description, low text color contrast, lack of sufficient spacing between elements, and less than minimum sizes of text fonts and elements. Together, these accessibility issues accounted for 97.0\%, 77.8\%, and 94.5\% of V, PV, and W, respectively.This study proposed coverage measures to estimate the percentage of accessibility issues identified by an automated tool. The result showed that MAC, on average, identified about 67\% of accessibility issues in mobile apps.Two new accessibility conformance measures were proposed in this study: inaccessible element rate (IAER) and accessibility issue rate (AIR). IAER estimates the percentage of GUI elements that are inaccessible. AIR calculates the percentage of the actual number of accessibility issues relative to the maximum number of accessibility issues. Average IAER and AIR scores were 27.3\%, 19.9\%, 6.3\% and 20.7\%, 15.0\%, 5.4\% for V, PV, and W, respectively, for the studied apps. The IAER score showed approximately 30\% of the GUI elements had accessibility issues, and the AIR score indicated that 15\% of the accessibility issues remained and need to be fixed to make the apps accessible.},
  articleno    = 3,
  numpages     = 31,
  keywords     = {accessibility evaluation, disability, accessibility, Mobile, mobile apps, graphical user interface (GUI), usability}
}
@inproceedings{Silva20,
  title        = {{On the Relation between Code Elements and Accessibility Issues in Android Apps}},
  author       = {da Silva, Henrique Neves and Endo, Andre Takeshi and Eler, Marcelo Medeiros and Vergilio, Silvia Regina and Durelli, Vinicius H. S.},
  year         = 2020,
  booktitle    = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
  location     = {Natal, Brazil},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {SAST 20},
  pages        = {40–49},
  doi          = {10.1145/3425174.3425209},
  isbn         = 9781450387552,
  url          = {https://doi.org/10.1145/3425174.3425209},
  abstract     = {Mobile apps have gone mainstream and become part of our daily lives. Currently, many efforts have been made to make apps more accessible to people with disabilities. However, little is still known on how to implement more accessible apps. In the Android API, there are (code) elements that may be employed to (in)directly improve the app's accessibility. This paper aims to investigate the prevalence of accessibility code elements and their relation to potential accessibility issues. First, we identified code elements of the native Android API that may be related to accessibility features, and mapped them to principles and success criteria of the Web Content Accessibility Guidelines (WCAG) 2.1. Using a sample of 111 open source mobile apps available in Google Play, we conducted a characterization study to examine the prevalence of accessibility code elements. We also analyzed how these code elements are related to issues detected by the static analyzer Android Lint and the accessibility testing tool MATE. Our results indicate that code elements are not widely used; the ones directly related to accessibility are present in only a few apps. Additionally, our results would seem to suggest that apps that adopt accessibility code elements, tend to have less accessibility issues. By analyzing our results from the standpoint of the WCAG principles, we conclude that there is room for improvement in terms of how both the Android API and automated testing tools deal with accessibility-related issues.},
  numpages     = 10,
  keywords     = {Mobile apps. Accessibility}
}
@inproceedings{Kim16,
  title        = {{How to Develop Accessibility UX Design Guideline in Samsung}},
  author       = {Kim, Hyun K. and Kim, Changwon and Lim, Eunyoung and Kim, Hyunjin},
  year         = 2016,
  booktitle    = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
  location     = {Florence, Italy},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {MobileHCI '16},
  pages        = {551–556},
  doi          = {10.1145/2957265.2957271},
  isbn         = 9781450344135,
  url          = {https://doi.org/10.1145/2957265.2957271},
  abstract     = {Accessibility is the major social responsibility of the Information Technology (IT) companies. New accessibility technology can make IT devices more accessible to diverse users, thus it can reduce barriers to the use of IT devices. The objective of this study is to inform the procedures to develop accessibility UX guidelines in Samsung. In 2014, the comprehensive literature survey was conducted including academic research papers, accessibility laws, and international standard documents. In 2015, a user test was conducted to clarify and specify the guidelines. In 2016, lawyers reviewed the guidelines to increase the reliability of them. The proposed procedure is helpful to develop new accessibility UX guidelines.},
  numpages     = 6,
  keywords     = {user interface, accessibility guideline, IT devices}
}
@inproceedings{Santiago22,
  title        = {{Are User Reviews Useful for Identifying Accessibility Issues That Autistic Users Face? An Exploratory Study}},
  author       = {Santiago, Mara Taynar and Marques, Anna Beatriz},
  year         = 2022,
  booktitle    = {Proceedings of the 21st Brazilian Symposium on Human Factors in Computing Systems},
  location     = {Diamantina, Brazil},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {IHC '22},
  doi          = {10.1145/3554364.3559114},
  isbn         = 9781450395069,
  url          = {https://doi.org/10.1145/3554364.3559114},
  abstract     = {Google Play Store offers several user reviews that can provide data on aspects of user experience, usability, and accessibility. Several studies address the importance of user reviews and their contributions to the evolution of interactive systems. However, accessibility has been little discussed in this type of study, especially accessibility for users with Autism Spectrum Disorder (ASD). Considering the potential of user reviews, this paper presents a textual analysis of reviews extracted from eight educational applications for autistic children: ABC autismo, Aprendendo com biel e seus amigos, AutApp autismo, Autismo projeto integrar, Jade autism, Matraquinha, OTO (Olhar Tocar Ouvir) and Teacch.me. The analysis adopted the Guidelines for Accessible Interfaces for people with Autism (GAIA) to classify user reviews. An accessibility diagnostic was obtained for apps evaluated and demonstrated that they have recurring accessibility problems regarding responses to actions, visibility of the state of the system, and customization. When compared to studies that have adopted other methods to evaluate the accessibility of the same applications, we observed that the user reviews provided more evidence of accessibility problems.},
  articleno    = 6,
  numpages     = 11,
  keywords     = {user review, accessibility, educational mobile apps, autism spectrum disorder}
}
@inproceedings{Mateus20,
  title        = {{Accessibility of Mobile Applications: Evaluation by Users with Visual Impairment and by Automated Tools}},
  author       = {Mateus, Delvani Ant\^{o}nio and Silva, Carlos Alberto and Eler, Marcelo Medeiros and Freire, Andr\'{e} Pimenta},
  year         = 2020,
  booktitle    = {Proceedings of the 19th Brazilian Symposium on Human Factors in Computing Systems},
  location     = {Diamantina, Brazil},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {IHC '20},
  doi          = {10.1145/3424953.3426633},
  isbn         = 9781450381727,
  url          = {https://doi.org/10.1145/3424953.3426633},
  abstract     = {Providing accessible mobile applications to people with visual disabilities demands appropriate evaluation techniques and tools to identify problems during the design of such systems. Automated accessibility evaluation tools are important to support evaluation tasks and to make evaluators more productive to perform repetitive analyses. However, automated tools cannot find alone all problems that users encounter in accessibility evaluations of mobile applications. Despite previous investigations on the coverage of accessibility problems encountered by automated tools on websites, there is little knowledge about the relationship between the problems encountered by those tools and problems faced by users with visual impairments in mobile applications. This paper presents a study comparing issues encountered by the automated tools MATE (Mobile Accessibility Testing) and Accessibility Scanner with a set of 415 instances of accessibility problems encountered in a previous user study involving six blind and five partially-sighted users on four mobile applications. The results showed that 36 types of problems were encountered only by users, tree types of problems were encountered both by users and by the tools, and 11 types of problems were encountered only by the automated tools. The results show the kinds of relevant problems that automated tools can identify, aiding in the early identification of such problems. The study also contributes to determining the types of problems that are only encountered by evaluations with users, reinforcing the importance of involving users in accessibility evaluation and characterizing the problems in mobile applications that can go unnoticed if automated tools are used alone.},
  articleno    = 4,
  numpages     = 10,
  keywords     = {mobile accessibility, user evaluation, automated tests}
}
@inproceedings{Shiver15,
  title        = {{Evaluating Alternatives for Better Deaf Accessibility to Selected Web-Based Multimedia}},
  author       = {Shiver, Brent N. and Wolfe, Rosalee J.},
  year         = 2015,
  booktitle    = {Proceedings of the 17th International ACM SIGACCESS Conference on Computers and Accessibility},
  location     = {Lisbon, Portugal},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASSETS '15},
  pages        = {231–238},
  doi          = {10.1145/2700648.2809857},
  isbn         = 9781450334006,
  url          = {https://doi.org/10.1145/2700648.2809857},
  abstract     = {The proliferation of video and audio media on the Internet has created a distinct disadvantage for deaf Internet users. Despite technological and legislative milestones in recent decades in making television and movies more accessible, there has been less progress with online access. A major obstacle to providing captions for Internet media is the high cost of captioning and transcribing services. This paper reports on two studies that focused on multimedia accessibility for Internet users who were born deaf or became deaf at an early age. An initial study attempted to identify priorities for deaf accessibility improvement. A total of 20 deaf and hard-of-hearing participants were interviewed via videophone about their Internet usage and the issues that were the most frustrating. The most common theme was concern over a lack of accessibility for online news. In the second study, a total of 95 deaf and hard-of-hearing participants evaluated different caption styles, some of which were generated through automatic speech recognition.Results from the second study confirm that captioning online videos makes the Internet more accessible to the deaf users, even when the captions are automatically generated. However color-coded captions used to highlight confidence levels were found neither to be beneficial nor detrimental; yet when asked directly about the benefit of color-coding, participants strongly favored the concept.},
  numpages     = 8,
  keywords     = {captioning, speech-to-text, multimedia accessibility, automatic speech recognition, deaf, web accessibility}
}
@inproceedings{Aizpurua14,
  title        = {{Are Users the Gold Standard for Accessibility Evaluation?}},
  author       = {Aizpurua, Amaia and Arrue, Myriam and Harper, Simon and Vigo, Markel},
  year         = 2014,
  booktitle    = {Proceedings of the 11th Web for All Conference},
  location     = {Seoul, Korea},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {W4A '14},
  doi          = {10.1145/2596695.2596705},
  isbn         = 9781450326513,
  url          = {https://doi.org/10.1145/2596695.2596705},
  abstract     = {User testing is considered a key part of web accessibility evaluation. However, little is known about how effective is for identifying accessibility problems. Our experience, informed by a series of studies with blind users, corroborates that a website with a significant number of guideline violations can be perceived as accessible, and on the contrary, some participants may not perceive a highly accessible website as accessible. Accessibility guidelines are often criticised by their partial coverage and questionable validity. However, we should be very careful about making categorical statements in this regard as there are a number of variables that may introduce biases in user tests. We identify sources of bias related to user expertise, the experimental setting, employed language and reporting that, if not adequately controlled, may influence on the validity and reliability of the evaluation results. We discuss the limitations and practical implications of user testing with blind users for web accessibility evaluation.},
  articleno    = 13,
  numpages     = 4,
  keywords     = {web accessibility, screen readers, user testing, blind users}
}
@inproceedings{Silva19,
  title        = {{Design and Evaluation of Mobile Applications for People with Visual Impairments: A Compilation of Usable Accessibility Guidelines}},
  author       = {Silva, Giovanna Magda S. and de C. Andrade, Rossana M. and de Gois R. Darin, Ticianne},
  year         = 2019,
  booktitle    = {Proceedings of the 18th Brazilian Symposium on Human Factors in Computing Systems},
  location     = {Vit\'{o}ria, Esp\'{\i}rito Santo, Brazil},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {IHC '19},
  doi          = {10.1145/3357155.3358450},
  isbn         = 9781450369718,
  url          = {https://doi.org/10.1145/3357155.3358450},
  abstract     = {In a context where the individual's daily needs and activities are increasingly focused on mobile devices, the use of sound design practices focused on technically accessible and cognitively usable interfaces becomes critical. This issue is even more sensitive when considered under the perspective of people with visual impairments, who depend on operating systems (OS) accessibility features. However, native OS accessibility support is not mature enough to ensure usability and accessibility simultaneously in different kinds of applications. To overcome these limitations, Human-Computer Interaction literature has been proposing guidelines, best practices, and heuristics for designing and evaluating usable and accessible designs. Still, this information is diffuse and can be challenging to implement because they were proposed considering different definitions and specifications. To support practitioners, students and researchers in finding and implementing usability and accessibility guidelines in mobile applications, the present work presents a compilation of 369 recommendations identified in the literature on the design and evaluation of usable and accessible interfaces for Sil people on mobile devices.},
  articleno    = 21,
  numpages     = 10,
  keywords     = {mobile devices, usable accessibility, recommendations compilation}
}
@inproceedings{Li22,
  title        = {{TangibleGrid: Tangible Web Layout Design for Blind Users}},
  author       = {Li, Jiasheng and Yan, Zeyu and Jarjue, Ebrima Haddy and Shetty, Ashrith and Peng, Huaishu},
  year         = 2022,
  booktitle    = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
  location     = {Bend, OR, USA},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {UIST '22},
  doi          = {10.1145/3526113.3545627},
  isbn         = 9781450393201,
  url          = {https://doi.org/10.1145/3526113.3545627},
  abstract     = {We present TangibleGrid, a novel device that allows blind users to understand and design the layout of a web page with real-time tangible feedback. We conducted semi-structured interviews and a series of co-design sessions with blind users to elicit insights that guided the design of TangibleGrid. Our final prototype contains shape-changing brackets representing the web elements and a baseboard representing the web page canvas. Blind users can design a web page layout through creating and editing web elements by snapping or adjusting tangible brackets on top of the baseboard. The baseboard senses the brackets’ type, size, and location, verbalizes the information, and renders the web page on the client browser. Through a formative user study, we found that blind users could understand a web page layout through TangibleGrid. They were also able to design a new web layout from scratch without the help of sighted people.},
  articleno    = 47,
  numpages     = 12,
  keywords     = {Accessible web design, tangible user interface, accessibility, tactile feedback, visual impairment}
}
@inproceedings{Brajnik15,
  title        = {{Model-Based Automated Accessibility Testing}},
  author       = {Brajnik, Giorgio and Pighin, Chiara and Fabbro, Sara},
  year         = 2015,
  booktitle    = {Proceedings of the 17th International ACM SIGACCESS Conference on Computers and Accessibility},
  location     = {Lisbon, Portugal},
  publisher    = {ACM},
  series       = {ASSETS '15},
  doi          = {10.1145/2700648.2811357},
  isbn         = 9781450334006,
  url          = {https://doi.org/10.1145/2700648.2811357},
  keywords     = {human factors}
}
@inproceedings{Bajammal21,
  title        = {{Semantic Web Accessibility Testing via Hierarchical Visual Analysis}},
  author       = {Bajammal, Mohammad and Mesbah, Ali},
  year         = 2021,
  booktitle    = {Proceedings of the 43rd International Conference on Software Engineering},
  location     = {Madrid, Spain},
  publisher    = {IEEE Press},
  series       = {ICSE '21},
  pages        = {1610–1621},
  doi          = {10.1109/ICSE43902.2021.00143},
  isbn         = 9781450390859,
  url          = {https://doi.org/10.1109/ICSE43902.2021.00143},
  abstract     = {Web accessibility, the design of web apps to be usable by users with disabilities, impacts millions of people around the globe. Although accessibility has traditionally been a marginal afterthought that is often ignored in many software products, it is increasingly becoming a legal requirement that must be satisfied. While some web accessibility testing tools exist, most only perform rudimentary syntactical checks that do not assess the more important high-level semantic aspects that users with disabilities rely on. Accordingly, assessing web accessibility has largely remained a laborious manual process requiring human input. In this paper, we propose an approach, called AxeRay, that infers semantic groupings of various regions of a web page and their semantic roles. We evaluate our approach on 30 real-world websites and assess the accuracy of semantic inference as well as the ability to detect accessibility failures. The results show that AxeRay achieves, on average, an F-measure of 87\% for inferring semantic groupings, and is able to detect accessibility failures with 85\% accuracy.},
  numpages     = 12,
  keywords     = {visual analysis, web testing, web accessibility, accessibility testing}
}
@inproceedings{Ramachandra18,
  title        = {{Testing Web-Based Applications with the <u>v</u>oice <u>c</u>ontrolled <u>a</u>ccessibility and <u>t</u>esting Tool (VCAT)}},
  author       = {Ramachandra, Nagendra Prasad Kasaghatta and Csallner, Christoph},
  year         = 2018,
  booktitle    = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
  location     = {Gothenburg, Sweden},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ICSE '18},
  pages        = {208–209},
  doi          = {10.1145/3183440.3195099},
  isbn         = 9781450356633,
  url          = {https://doi.org/10.1145/3183440.3195099},
  abstract     = {Many web applications and software engineering tools such as test generators are not accessible for users who do not use traditional input devices such as mouse and keyboard. To address this shortcoming of current applications, this work leverages recent speech recognition advances to create a browser plugin that interprets voice inputs as web browser commands and as steps in a corresponding test case. In an initial experiment, the resulting Voice Controlled Accessibility and Testing tool (VCAT) prototype for Chrome and Selenium yielded a lower overall runtime than a traditional test creation approach.},
  numpages     = 2,
  keywords     = {voice control, accessible software engineering tool, selenium}
}
NEW=============================================
@inproceedings{Liu21,
  title        = {{Discovering UI Display Issues with Visual Understanding}},
  author       = {Liu, Zhe},
  year         = 2021,
  booktitle    = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
  location     = {Virtual Event, Australia},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASE '20},
  pages        = {1373–1375},
  doi          = {10.1145/3324884.3418917},
  isbn         = 9781450367684,
  url          = {https://doi.org/10.1145/3324884.3418917},
  abstract     = {GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, blurred screen, missing image always occur during GUI rendering on difference devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a novel approach, OwlEye, based on deep learning for modelling visual information of the GUI screenshot. Therefore, OwlEye can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. We manually construct a large-scale labelled dataset with 4,470 GUI screenshots with UI display issues. We develop a heuristics-based data augmentation method and a GAN-based data augmentation method for boosting the performance of our OwlEye. At present, the evaluation demonstrates that our OwlEye can achieve 85\% precision and 84\% recall in detecting UI display issues, and 90\% accuracy in localizing these issues.},
  numpages     = 3
}
@inproceedings{Liu2021,
  title        = {{Owl Eyes: Spotting UI Display Issues via Visual Understanding}},
  author       = {Liu, Zhe and Chen, Chunyang and Wang, Junjie and Huang, Yuekai and Hu, Jun and Wang, Qing},
  year         = 2021,
  booktitle    = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
  location     = {Virtual Event, Australia},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASE '20},
  pages        = {398–409},
  doi          = {10.1145/3324884.3416547},
  isbn         = 9781450367684,
  url          = {https://doi.org/10.1145/3324884.3416547},
  abstract     = {Graphical User Interface (GUI) provides a visual bridge between a software application and end users, through which they can interact with each other. With the development of technology and aesthetics, the visual effects of the GUI are more and more attracting. However, such GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, blurred screen, missing image always occur during GUI rendering on different devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a novel approach, OwlEye, based on deep learning for modelling visual information of the GUI screenshot. Therefore, OwlEye can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. We manually construct a large-scale labelled dataset with 4,470 GUI screenshots with UI display issues and develop a heuristics-based data augmentation method for boosting the performance of our OwlEye. The evaluation demonstrates that our OwlEye can achieve 85\% precision and 84\% recall in detecting UI display issues, and 90\% accuracy in localizing these issues. We also evaluate OwlEye with popular Android apps on Google Play and F-droid, and successfully uncover 57 previously-undetected UI display issues with 26 of them being confirmed or fixed so far.},
  numpages     = 12,
  keywords     = {deep learning, mobile app, UI display, UI testing}
}
@inproceedings{Nguyen:ASE'15,
  title        = {{Reverse Engineering Mobile Application User Interfaces with REMAUI (T)}},
  author       = {Nguyen, Tuan Anh and Csallner, Christoph},
  year         = 2015,
  booktitle    = {2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  volume       = {},
  number       = {},
  pages        = {248--259},
  doi          = {10.1109/ASE.2015.32}
}
@article{Moran:TSE'18,
  title        = {{Machine Learning-Based Prototyping of Graphical User Interfaces for Mobile Apps}},
  author       = {Moran, Kevin and Bernal-Cárdenas, Carlos and Curcio, Michael and Bonett, Richard and Poshyvanyk, Denys},
  year         = 2020,
  journal      = {IEEE Transactions on Software Engineering},
  volume       = 46,
  number       = 2,
  pages        = {196--221},
  doi          = {10.1109/TSE.2018.2844788}
}
@inproceedings{Chen18,
  title        = {{From UI Design Image to GUI Skeleton: A Neural Machine Translator to Bootstrap Mobile GUI Implementation}},
  author       = {Chen, Chunyang and Su, Ting and Meng, Guozhu and Xing, Zhenchang and Liu, Yang},
  year         = 2018,
  booktitle    = {Proceedings of the 40th International Conference on Software Engineering},
  location     = {Gothenburg, Sweden},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ICSE '18},
  pages        = {665–676},
  doi          = {10.1145/3180155.3180240},
  isbn         = 9781450356381,
  url          = {https://doi.org/10.1145/3180155.3180240},
  abstract     = {A GUI skeleton is the starting point for implementing a UI design image. To obtain a GUI skeleton from a UI design image, developers have to visually understand UI elements and their spatial layout in the image, and then translate this understanding into proper GUI components and their compositions. Automating this visual understanding and translation would be beneficial for bootstraping mobile GUI implementation, but it is a challenging task due to the diversity of UI designs and the complexity of GUI skeletons to generate. Existing tools are rigid as they depend on heuristically-designed visual understanding and GUI generation rules. In this paper, we present a neural machine translator that combines recent advances in computer vision and machine translation for translating a UI design image into a GUI skeleton. Our translator learns to extract visual features in UI images, encode these features' spatial layouts, and generate GUI skeletons in a unified neural network framework, without requiring manual rule development. For training our translator, we develop an automated GUI exploration method to automatically collect large-scale UI data from real-world applications. We carry out extensive experiments to evaluate the accuracy, generality and usefulness of our approach.},
  numpages     = 12,
  keywords     = {user interface, reverse engineering, deep learning}
}
@inproceedings{Vendome19,
  title        = {{Can Everyone use my app? An Empirical Study on Accessibility in Android Apps}},
  author       = {Vendome, Christopher and Solano, Diana and Liñán, Santiago and Linares-Vásquez, Mario},
  year         = 2019,
  booktitle    = {2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  volume       = {},
  number       = {},
  pages        = {41--52},
  doi          = {10.1109/ICSME.2019.00014}
}
@article{Shaoqing15,
  title        = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
  author       = {Shaoqing Ren and Kaiming He and Ross B. Girshick and Jian Sun},
  year         = 2015,
  journal      = {CoRR},
  volume       = {abs/1506.01497},
  url          = {http://arxiv.org/abs/1506.01497},
  eprinttype   = {arXiv},
  eprint       = {1506.01497},
  timestamp    = {Mon, 13 Aug 2018 16:46:02 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/RenHG015.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
a service of  Schloss Dagstuhl - Leibniz Center for Informatics homebrowsesearchabout
@inproceedings{Kong21,
  title        = {{New Metrics for Understanding Touch by People with and without Limited Fine Motor Function}},
  author       = {Kong, Junhan and Zhong, Mingyuan and Fogarty, James and Wobbrock, Jacob O.},
  year         = 2021,
  booktitle    = {Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility},
  location     = {Virtual Event, USA},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ASSETS '21},
  doi          = {10.1145/3441852.3476559},
  isbn         = 9781450383066,
  url          = {https://doi.org/10.1145/3441852.3476559},
  abstract     = {Current performance measures with touch-based systems usually focus on overall performance, such as touch accuracy and target acquisition speed. But a touch is not an atomic event; it is a process that unfolds over time, and this process can be characterized to gain insight into users’ touch behaviors. To this end, our work proposes 13 target-agnostic touch performance metrics to characterize what happens during a touch. These metrics are: touch direction, variability, drift, duration, extent, absolute/signed area change, area variability, area deviation, absolute/signed angle change, angle variability, and angle deviation. Unlike traditional touch performance measures that treat a touch as a single (x, y) coordinate, we regard a touch as a time series of ovals that occur from finger-down to finger-up. We provide a mathematical formula and intuitive description for each metric we propose. To evaluate our metrics, we run an analysis on a publicly available dataset containing touch inputs by people with and without limited fine motor function, finding our metrics helpful in characterizing different fine motor control challenges. Our metrics can be useful to designers and evaluators of touch-based systems, particularly when making touch screens accessible to all forms of touch.},
  articleno    = 80,
  numpages     = 4,
  keywords     = {limited fine motor function, Understanding touch}
}
@inproceedings{combodroid,
  title        = {{ComboDroid: Generating High-Quality Test Inputs for Android Apps via Use Case Combinations}},
  author       = {Wang, Jue and Jiang, Yanyan and Xu, Chang and Cao, Chun and Ma, Xiaoxing and Lu, Jian},
  year         = 2020,
  booktitle    = {ICSE 2020},
  location     = {Seoul, South Korea},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {ICSE '20},
  pages        = {469–480},
  doi          = {10.1145/3377811.3380382},
  isbn         = 9781450371216,
  url          = {https://doi.org/10.1145/3377811.3380382},
  numpages     = 12,
  keywords     = {mobile apps, software testing}
}
@inproceedings{ape,
  title        = {{Practical GUI Testing of Android Applications via Model Abstraction and Refinement}},
  author       = {Gu, Tianxiao and Sun, Chengnian and Ma, Xiaoxing and Cao, Chun and Xu, Chang and Yao, Yuan and Zhang, Qirun and Lu, Jian and Su, Zhendong},
  year         = 2019,
  booktitle    = {Proceedings of the 41st International Conference on Software Engineering},
  location     = {Montreal, Quebec, Canada},
  publisher    = {IEEE Press},
  series       = {ICSE '19},
  pages        = {269–280},
  doi          = {10.1109/ICSE.2019.00042},
  url          = {https://doi.org/10.1109/ICSE.2019.00042},
  numpages     = 12,
  keywords     = {mobile app testing, GUI testing, CEGAR}
}
@inproceedings{Avgust,
  title        = {{Avgust: automating usage-based test generation from videos of app executions}},
  author       = {Zhao, Yixue and Talebipour, Saghar and Baral, Kesina and Park, Hyojae and Yee, Leon and Khan, Safwat Ali and Brun, Yuriy and Medvidovi\'{c}, Nenad and Moran, Kevin},
  year         = 2022,
  booktitle    = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  location     = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {ESEC/FSE 2022},
  pages        = {421–433},
  doi          = {10.1145/3540250.3549134},
  isbn         = 9781450394130,
  url          = {https://doi.org/10.1145/3540250.3549134},
  abstract     = {Writing and maintaining UI tests for mobile apps is a time-consuming and tedious task. While decades of research have produced auto- mated approaches for UI test generation, these approaches typically focus on testing for crashes or maximizing code coverage. By contrast, recent research has shown that developers prefer usage-based tests, which center around specific uses of app features, to help support activities such as regression testing. Very few existing techniques support the generation of such tests, as doing so requires automating the difficult task of understanding the semantics of UI screens and user inputs. In this paper, we introduce Avgust, which automates key steps of generating usage-based tests. Avgust uses neural models for image understanding to process video recordings of app uses to synthesize an app-agnostic state-machine encoding of those uses. Then, Avgust uses this encoding to synthesize test cases for a new target app. We evaluate Avgust on 374 videos of common uses of 18 popular apps and show that 69\% of the tests Avgust generates successfully execute the desired usage, and that Avgust’s classifiers outperform the state of the art.},
  numpages     = 13,
  keywords     = {UI Understanding, Test Generation, Mobile Application, AI/ML}
}
@inproceedings{Deka16,
  title        = {{ERICA: Interaction Mining Mobile Apps}},
  author       = {Deka, Biplab and Huang, Zifeng and Kumar, Ranjitha},
  year         = 2016,
  booktitle    = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
  location     = {Tokyo, Japan},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {UIST '16},
  pages        = {767–776},
  doi          = {10.1145/2984511.2984581},
  isbn         = 9781450341899,
  url          = {https://doi.org/10.1145/2984511.2984581},
  abstract     = {Design plays an important role in adoption of apps. App design, however, is a complex process with multiple design activities. To enable data-driven app design applications, we present interaction mining -- capturing both static (UI layouts, visual details) and dynamic (user flows, motion details) components of an app's design. We present ERICA, a system that takes a scalable, human-computer approach to interaction mining existing Android apps without the need to modify them in any way. As users interact with apps through ERICA, it detects UI changes, seamlessly records multiple data-streams in the background, and unifies them into a user interaction trace. Using ERICA we collected interaction traces from over a thousand popular Android apps. Leveraging this trace data, we built machine learning classifiers to detect elements and layouts indicative of 23 common user flows. User flows are an important component of UX design and consists of a sequence of UI states that represent semantically meaningful tasks such as searching or composing. With these classifiers, we identified and indexed more than 3000 flow examples, and released the largest online search engine of user flows in Android apps.},
  numpages     = 10,
  keywords     = {app design, design mining, interaction mining, user flows}
}
@misc{bao22,
  title        = {{BEiT: BERT Pre-Training of Image Transformers}},
  author       = {Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
  year         = 2022,
  eprint       = {2106.08254},
  archiveprefix = {arXiv},
  primaryclass = {cs.CV}
}
@inproceedings{Liu18,
  title        = {{Learning Design Semantics for Mobile Apps}},
  author       = {Liu, Thomas F. and Craft, Mark and Situ, Jason and Yumer, Ersin and Mech, Radomir and Kumar, Ranjitha},
  year         = 2018,
  booktitle    = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
  location     = {<conf-loc>, <city>Berlin</city>, <country>Germany</country>, </conf-loc>},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {UIST '18},
  pages        = {569–579},
  doi          = {10.1145/3242587.3242650},
  isbn         = 9781450359481,
  url          = {https://doi.org/10.1145/3242587.3242650},
  abstract     = {Recently, researchers have developed black-box approaches to mine design and interaction data from mobile apps. Although the data captured during this interaction mining is descriptive, it does not expose the design semantics of UIs: what elements on the screen mean and how they are used. This paper introduces an automatic approach for generating semantic annotations for mobile app UIs. Through an iterative open coding of 73k UI elements and 720 screens, we contribute a lexical database of 25 types of UI components, 197 text button concepts, and 135 icon classes shared across apps. We use this labeled data to learn code-based patterns to detect UI components and to train a convolutional neural network that distinguishes between icon classes with 94\% accuracy. To demonstrate the efficacy of our approach at scale, we compute semantic annotations for the 72k unique UIs in the Rico dataset, assigning labels for 78\% of the total visible, non-redundant elements.},
  numpages     = 11,
  keywords     = {design semantics, machine learning, mobile app design}
}
@inproceedings{machiry2013dynodroid,
  title        = {{Dynodroid: an input generation system for Android apps}},
  author       = {Aravind Machiry and Rohan Tahiliani and Mayur Naik},
  year         = 2013,
  booktitle    = {ESEC/FSE'13},
  publisher    = {{ACM}},
  pages        = {224--234},
  editor       = {Bertrand Meyer and Luciano Baresi and Mira Mezini}
}
@inproceedings{machiry2013dynodroid,
  title        = {{Dynodroid: an input generation system for Android apps}},
  author       = {Aravind Machiry and Rohan Tahiliani and Mayur Naik},
  year         = 2013,
  booktitle    = {ESEC/FSE'13},
  publisher    = {{ACM}},
  pages        = {224--234},
  editor       = {Bertrand Meyer and Luciano Baresi and Mira Mezini}
}
@inproceedings{yanfu2024athena,
  title        = {{Enhancing Code Understanding for Impact Analysis by Combining Transformers and Program Dependence Graphs}},
  author       = {Yan, Yanfu and Cooper, Nathan and Moran, Kevin and Bavota, Gabriele and Poshyvanyk, Denys and Rich, Steve},
  year         = 2024,
  month        = {July},
  booktitle    = {The ACM International Conference on the Foundations of Software Engineering (FSE)},
  volume       = {}
}
@inproceedings{kipf2016semi,
  title        = {{Semi-Supervised Classification with Graph Convolutional Networks}},
  author       = {Thomas N. Kipf and Max Welling},
  year         = 2017,
  booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  url          = {https://openreview.net/forum?id=SJU4ayYgl},
  timestamp    = {Thu, 25 Jul 2019 14:25:55 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/KipfW17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{defferrard2016convolutional,
  title        = {{Convolutional neural networks on graphs with fast localized spectral filtering}},
  author       = {Defferrard, Micha\"{e}l and Bresson, Xavier and Vandergheynst, Pierre},
  year         = 2016,
  booktitle    = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  location     = {Barcelona, Spain},
  publisher    = {Curran Associates Inc.},
  address      = {Red Hook, NY, USA},
  series       = {NIPS'16},
  pages        = {3844–3852},
  doi          = {10.5555/3157382.3157527},
  isbn         = 9781510838819,
  abstract     = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
  numpages     = 9
}
@inproceedings{sasnauskas2014intent,
  title        = {{Intent fuzzer: crafting intents of death}},
  author       = {Sasnauskas, Raimondas and Regehr, John},
  year         = 2014,
  booktitle    = {WODA \& PERTEA},
  organization = {ACM}
}
@inproceedings{feng2016understanding,
  title        = {{Understanding and defending the Binder attack surface in Android}},
  author       = {Feng, Huan and Shin, Kang G},
  year         = 2016,
  booktitle    = {ACSAC},
  organization = {ACM}
}
@inproceedings{liang2014caiipa,
  title        = {{Caiipa: Automated large-scale mobile app testing through contextual fuzzing}},
  author       = {Liang, Chieh-Jan Mike and Lane, Nicholas D and Brouwers, Niels and Zhang, Li and Karlsson, B{\"o}rje F and Liu, Hao and Liu, Yan and Tang, Jun and Shan, Xiang and Chandra, Ranveer and others},
  year         = 2014,
  booktitle    = {MobiCom},
  organization = {ACM}
}
@misc{khan24,
  title        = {{AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding}},
  author       = {Safwat Ali Khan and Wenyu Wang and Yiran Ren and Bin Zhu and Jiangfan Shi and Alyssa McGowan and Wing Lam and Kevin Moran},
  year         = 2024,
  eprint       = {2404.01240},
  archiveprefix = {arXiv},
  primaryclass = {cs.SE}
}
@article{vajjala2024vietoris,
  title        = {{Vietoris-rips complex: A new direction for cross-domain cold-start recommendation}},
  author       = {Krishna Vajjala, Ajay and Meher, D and Pothagoni, Shrunal and Zhu, Ziwei and Rosenblum, D},
  year         = 2024,
  booktitle    = {Proceedings of the SIAM International Conference on Data Mining (SDM)}
}
@article{weisstein2003heron,
  title        = {{Heron's formula}},
  author       = {Weisstein, E.},
  year         = 2003,
  journal      = {MathWorld}
}
@inproceedings{ye2013droidfuzzer,
  title        = {{Droidfuzzer: Fuzzing the android apps with intent-filter tag}},
  author       = {Ye, Hui and Cheng, Shaoyin and Zhang, Lanbo and Jiang, Fan},
  year         = 2013,
  booktitle    = {MoMM},
  organization = {ACM}
}
@article{mulliner2009fuzzing,
  title        = {{Fuzzing the Phone in your Phone}},
  author       = {Mulliner, Collin and Miller, Charlie},
  year         = 2009,
  journal      = {Black Hat USA}
}
@inproceedings{choudhary2015automated,
  title        = {{Automated test input generation for android: Are we there yet?}},
  author       = {Choudhary, Shauvik Roy and Gorla, Alessandra and Orso, Alessandro},
  year         = 2015,
  booktitle    = {ASE},
  organization = {IEEE}
}
@inproceedings{rastogi2013appsplayground,
  title        = {{AppsPlayground: automatic security analysis of smartphone applications}},
  author       = {Rastogi, Vaibhav and Chen, Yan and Enck, William},
  year         = 2013,
  booktitle    = {CODASPY},
  organization = {ACM}
}
@inproceedings{supor,
  title        = {{SUPOR: precise and scalable sensitive user input detection for android apps}},
  author       = {Huang, Jianjun and Li, Zhichun and Xiao, Xusheng and Wu, Zhenyu and Lu, Kangjie and Zhang, Xiangyu and Jiang, Guofei},
  year         = 2015,
  booktitle    = {Proceedings of the USENIX Security Symposium (USENIX Security)},
  pages        = {977--992}
}
@inproceedings{CuriousDroid,
  title        = {{CuriousDroid: automated user interface interaction for android application analysis sandboxes}},
  author       = {Carter, Patrick and Mulliner, Collin and Lindorfer, Martina and Robertson, William and Kirda, Engin},
  year         = 2016,
  booktitle    = {FC}
}
@inproceedings{uiref,
  title        = {{UiRef: Analysis of Sensitive User Inputs in Android Applications}},
  author       = {Andow, Benjamin and Acharya, Akhil and Li, Dengfeng and Enck, William and Singh, Kapil and Xie, Tao},
  year         = 2017,
  booktitle    = {Proceedings of the 10th ACM Conference on Security and Privacy in Wireless and Mobile Networks (WiSec)},
  pages        = {23--34}
}
@inproceedings{uipicker,
  title        = {{Uipicker: User-input privacy identification in mobile applications}},
  author       = {Nan, Yuhong and Yang, Min and Yang, Zhemin and Zhou, Shunfan and Gu, Guofei and Wang, Xiaofeng},
  year         = 2015,
  booktitle    = {Proceedings of the USENIX Security Symposium (USENIX Security)},
  pages        = {993--1008}
}
@inproceedings{liu2017automatic,
  title        = {{Automatic text input generation for mobile testing}},
  author       = {Liu, Peng and Zhang, Xiangyu and Pistoia, Marco and Zheng, Yunhui and Marques, Manoel and Zeng, Lingfei},
  year         = 2017,
  booktitle    = {ICSE},
  organization = {IEEE}
}
@article{Alpay23,
  title        = {{Multimodal video retrieval with CLIP: a user study}},
  author       = {Alpay,  Tayfun and Magg,  Sven and Broze,  Philipp and Speck,  Daniel},
  year         = 2023,
  month        = sep,
  journal      = {Information Retrieval Journal},
  publisher    = {Springer Science and Business Media LLC},
  volume       = 26,
  number       = {1–2},
  doi          = {10.1007/s10791-023-09425-2},
  issn         = {1573-7659},
  url          = {http://dx.doi.org/10.1007/s10791-023-09425-2}
}
@inproceedings{Wei23,
  title        = {{Improving CLIP Fine-tuning Performance}},
  author       = {Wei,  Yixuan and Hu,  Han and Xie,  Zhenda and Liu,  Ze and Zhang,  Zheng and Cao,  Yue and Bao,  Jianmin and Chen,  Dong and Guo,  Baining},
  year         = 2023,
  month        = oct,
  booktitle    = {2023 IEEE/CVF International Conference on Computer Vision (ICCV)},
  publisher    = {IEEE},
  doi          = {10.1109/iccv51070.2023.00501},
  url          = {http://dx.doi.org/10.1109/ICCV51070.2023.00501}
}
@inproceedings{Che23,
  title        = {{Enhancing Multimodal Understanding with CLIP-Based Image-to-Text Transformation}},
  author       = {Che, Chang and Lin, Qunwei and Zhao, Xinyu and Huang, Jiaxin and Yu, Liqiang},
  year         = 2023,
  booktitle    = {Proceedings of the 2023 6th International Conference on Big Data Technologies},
  location     = {, Qingdao, China,},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {ICBDT '23},
  pages        = {414–418},
  doi          = {10.1145/3627377.3627442},
  isbn         = 9798400707667,
  url          = {https://doi.org/10.1145/3627377.3627442},
  abstract     = {The process of transforming input images into corresponding textual explanations stands as a crucial and complex endeavor within the domains of computer vision and natural language processing. In this paper, we propose an innovative ensemble approach that harnesses the capabilities of Contrastive Language-Image Pretraining (CLIP) models. Our ensemble framework encompasses two significant variations of the CLIP model, each meticulously designed to cater to specific nuances within the image-to-text transformation landscape. The first model introduces an elaborated architecture, featuring multiple layers with distinct learning rates, thereby amplifying its adeptness in capturing intricate relationships between images and text. The second model strategically exploits CLIP’s inherent zero-shot learning potential to generate image-text embeddings, subsequently harnessed by a K-Nearest Neighbors (KNN) model. Through this KNN-based paradigm, the model facilitates image-to-text transformation by identifying closely related embeddings within the embedding space. Notably, our ensemble approach is rigorously evaluated, employing the cosine similarity metric to gauge the alignment between model-generated embeddings and ground truth representations. Comparative experiments vividly highlight the superiority of our ensemble strategy over standalone CLIP models. This study not only advances the state-of-the-art in image-to-text transformation but also accentuates the promising trajectory of ensemble learning in effectively addressing intricate multimodal tasks.},
  numpages     = 5,
  keywords     = {CLIP, Elaborated Architecture, Ensemble Learning, Image-to-Text Transformation, K-Nearest Neighbors, Multimodal Alignment, Zero-Shot Learning}
}
@inproceedings{arnatovich2016achieving,
  title        = {{Achieving high code coverage in Android UI testing via automated widget exercising}},
  author       = {Arnatovich, Yauhen Leanidavich and Ngo, Minh Ngoc and Kuan, Tan Hee Beng and Soh, Charlie},
  year         = 2016,
  booktitle    = {APSEC},
  organization = {IEEE}
}
@inproceedings{li2017droidbot,
  title        = {{DroidBot: a lightweight UI-guided test input generator for Android}},
  author       = {Li, Yuanchun and Yang, Ziyue and Guo, Yao and Chen, Xiangqun},
  year         = 2017,
  booktitle    = {ICSE-C},
  organization = {IEEE}
}
@inproceedings{mao2016sapienz,
  title        = {{Sapienz: Multi-objective automated testing for Android applications}},
  author       = {Mao, Ke and Harman, Mark and Jia, Yue},
  year         = 2016,
  booktitle    = {ISSTA},
  organization = {ACM}
}
@inproceedings{reran,
  title        = {{RERAN: timing- and touch-sensitive record and replay for Android}},
  author       = {Lorenzo Gomez and Iulian Neamtiu and Tanzirul Azim and Todd D. Millstein},
  year         = 2013,
  booktitle    = {35th International Conference on Software Engineering, {ICSE} '13, San Francisco, CA, USA, May 18-26, 2013},
  publisher    = {{IEEE} Computer Society},
  pages        = {72--81},
  editor       = {David Notkin and Betty H. C. Cheng and Klaus Pohl}
}
@misc{mosaic,
  title        = {{Mosaic: Cross-Platform User-Interaction Record and Replay for the Fragmented Android Ecosystem}},
  year         = 2022,
  url          = {https://github.com/Matthalp-zz/mosaic},
  key          = {{mosaic}}
}
@misc{replaykit,
  title        = {{replayKit}},
  year         = 2022,
  url          = {https://github.com/appetizerio/replaykit},
  key          = {{replaykit}}
}
@misc{appium,
  title        = {{appium}},
  year         = 2022,
  url          = {https://appium.io/docs/en/commands/device/recording-screen/start-recording-screen/},
  key          = {{replaykit}}
}
@misc{monkeyrunner,
  title        = {{monkeyrunner}},
  year         = 2022,
  url          = {https://developer.android.com/studio/test/monkeyrunner},
  key          = {{monkeyrunner}}
}
@misc{culebra,
  title        = {{culebra}},
  year         = 2022,
  url          = {https://github.com/dtmilano/AndroidViewClient/wiki/culebra},
  key          = {{culebra}}
}
@misc{androR2dataset,
  title        = {{AndroR2 Dataset}},
  year         = 2022,
  url          = {https://github.com/se-umn/2022\_saner\_bug\_report\_reproduction\_study/tree/master/apks},
  key          = {{androR2}}
}
@inproceedings{RecDroid,
  title        = {{ReCDroid: Automatically Reproducing Android Application Crashes from Bug Reports}},
  author       = {Zhao, Yu and Yu, Tingting and Su, Ting and Liu, Yang and Zheng, Wei and Zhang, Jingzhi and G.J. Halfond, William},
  year         = 2019,
  booktitle    = {2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)},
  url          = {https://github.com/AndroidTestBugReport/ReCDroid}
}
@inproceedings{V2S,
  title        = {{V2S: A Tool for Translating Video Recordings of Mobile App Usages into Replayable Scenarios}},
  author       = {Madeleine Havranek and Carlos Bernal{-}C{\'{a}}rdenas and Nathan Cooper and Oscar Chaparro and Denys Poshyvanyk and Kevin Moran},
  year         = 2021,
  booktitle    = {43rd {IEEE/ACM} International Conference on Software Engineering: Companion Proceedings, {ICSE} Companion 2021, Madrid, Spain, May 25-28, 2021},
  publisher    = {{IEEE}},
  pages        = {65--68}
}
@inproceedings{CrashDroid,
  title        = {{Generating Reproducible and Replayable Bug Reports from Android Application Crashes}},
  author       = {White, Martin and Linares-Vásquez, Mario and Johnson, Peter and Bernal-Cárdenas, Carlos and Poshyvanyk, Denys},
  year         = 2015,
  booktitle    = {2015 IEEE 23rd International Conference on Program Comprehension},
  url          = {https://www.cs.wm.edu/semeru/data/ICPC15-CrashDroid}
}
@inproceedings{Wu21,
  title        = {{Screen Parsing: Towards Reverse Engineering of UI Models from Screenshots}},
  author       = {Wu, Jason and Zhang, Xiaoyi and Nichols, Jeff and Bigham, Jeffrey P},
  year         = 2021,
  booktitle    = {The 34th Annual ACM Symposium on User Interface Software and Technology},
  location     = {Virtual Event, USA},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {UIST '21},
  pages        = {470–483},
  doi          = {10.1145/3472749.3474763},
  isbn         = 9781450386357,
  url          = {https://doi.org/10.1145/3472749.3474763},
  abstract     = {Automated understanding of user interfaces (UIs) from their pixels can improve accessibility, enable task automation, and facilitate interface design without relying on developers to comprehensively provide metadata. A first step is to infer what UI elements exist on a screen, but current approaches are limited in how they infer how those elements are semantically grouped into structured interface definitions. In this paper, we motivate the problem of screen parsing, the task of predicting UI elements and their relationships from a screenshot. We describe our implementation of screen parsing and provide an effective training procedure that optimizes its performance. In an evaluation comparing the accuracy of the generated output, we find that our implementation significantly outperforms current systems (up to 23%). Finally, we show three example applications that are facilitated by screen parsing: (i) UI similarity search, (ii) accessibility enhancement, and (iii) code generation from UI screenshots.},
  numpages     = 14,
  keywords     = {user interface modeling, hierarchy prediction, ui semantics}
}
@inproceedings{roscript,
  title        = {{Roscript: A visual script driven truly non-intrusive robotic testing system for touch screen applications}},
  author       = {Qian, Ju and Shang, Zhengyu and Yan, Shuoyan and Wang, Yan and Chen, Lin},
  year         = 2020,
  booktitle    = {ICSE 2020},
  pages        = {297--308}
}
@inproceedings{mobiplay,
  title        = {{Mobiplay: A remote execution based record-and-replay tool for mobile applications}},
  author       = {Qin, Zhengrui and Tang, Yutao and Novak, Ed and Li, Qun},
  year         = 2016,
  booktitle    = {Proceedings of the 38th International Conference on Software Engineering},
  pages        = {571--582}
}
@inproceedings{VALERA,
  title        = {{Versatile yet lightweight record-and-replay for Android}},
  author       = {Yongjian Hu and Tanzirul Azim and Iulian Neamtiu},
  year         = 2015,
  booktitle    = {Proceedings of the 2015 {ACM} {SIGPLAN} International Conference on Object-Oriented Programming, Systems, Languages, and Applications, {OOPSLA} 2015, part of {SPLASH} 2015, Pittsburgh, PA, USA, October 25-30, 2015},
  publisher    = {{ACM}},
  pages        = {349--366},
  editor       = {Jonathan Aldrich and Patrick Eugster}
}
@inproceedings{Yu23,
  title        = {{CgT-GAN: CLIP-guided Text GAN for Image Captioning}},
  author       = {Yu, Jiarui and Li, Haoran and Hao, Yanbin and Zhu, Bin and Xu, Tong and He, Xiangnan},
  year         = 2023,
  booktitle    = {Proceedings of the 31st ACM International Conference on Multimedia},
  location     = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {MM '23},
  pages        = {2252–2263},
  doi          = {10.1145/3581783.3611891},
  isbn         = 9798400701085,
  url          = {https://doi.org/10.1145/3581783.3611891},
  abstract     = {The large-scale visual-language pre-trained model, Contrastive Language-Image Pre-training (CLIP), has significantly improved image captioning for scenarios without human-annotated image-caption pairs. Recent advanced CLIP-based image captioning without human annotations follows a text-only training paradigm, i.e., reconstructing text from shared embedding space. Nevertheless, these approaches are limited by the training/inference gap or huge storage requirements for text embeddings. Given that it is trivial to obtain images in the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates images into the training process to enable the model to "see" real visual modality. Particularly, we use adversarial training to teach CgT-GAN to mimic the phrases of an external text corpus and CLIP-based reward to provide semantic guidance. The caption generator is jointly rewarded based on the caption naturalness to human language calculated from the GAN's discriminator and the semantic guidance reward computed by the CLIP-based reward module. In addition to the cosine similarity as the semantic guidance reward (i.e., CLIP-cos), we further introduce a novel semantic guidance reward called CLIP-agg, which aligns the generated caption with a weighted text embedding by attentively aggregating the entire corpus. Experimental results on three subtasks (ZS-IC, In-UIC and Cross-UIC) show that CgT-GAN outperforms state-of-the-art methods significantly across all metrics. Code is available at https://github.com/Lihr747/CgtGAN.},
  numpages     = 12,
  keywords     = {clip, gan, image captioning, reinforcement learning}
}
@inproceedings{Li22,
  title        = {{VideoCLIP: A Cross-Attention Model for Fast Video-Text Retrieval Task with Image CLIP}},
  author       = {Li, Yikang and Hsiao, Jenhao and Ho, Chiuman},
  year         = 2022,
  booktitle    = {Proceedings of the 2022 International Conference on Multimedia Retrieval},
  location     = {Newark, NJ, USA},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {ICMR '22},
  pages        = {29–33},
  doi          = {10.1145/3512527.3531429},
  isbn         = 9781450392389,
  url          = {https://doi.org/10.1145/3512527.3531429},
  abstract     = {Video-text retrieval is an essential task in cross-modal information retrieval, i.e., retrieving relevant videos from a large and unlabelled dataset given textual queries. Existing methods that simply pool the image features (e.g., based on the CLIP encoder [14]) from frames to build the video descriptor often result in sub-optimal video-text search accuracy since the information among different modalities is not fully exchanged and aligned. In this paper, we proposed a novel dual-encoder model to address the challenging video-text retrieval problem, which uses a highly efficient cross-attention module to facilitate the information exchange between multiple modalities (i.e., video and text). The proposed VideoCLIP is evaluated on two benchmark video-text datasets, MSRVTT and DiDeMo, and the results show that our model can outperform existing state-of-the-art methods while the retrieval speed is much faster than the traditional query-agnostic search model.},
  numpages     = 5,
  keywords     = {video-text retrieval, transformer, query-agnostic, cross-attention, CLIP}
}
@misc{Radford21,
  title        = {{Learning Transferable Visual Models From Natural Language Supervision}},
  author       = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  year         = 2021,
  eprint       = {2103.00020},
  archiveprefix = {arXiv},
  primaryclass = {cs.CV}
}
@inproceedings{RANDR,
  title        = {{Late Breaking Results: Towards Practical Record and Replay for Mobile Applications}},
  author       = {Sahin, Onur and Aliyeva, Assel and Mathavan, Hariharan and Coskun, Ayse and Egele, Manuel},
  year         = 2019,
  booktitle    = {2019 56th ACM/IEEE Design Automation Conference (DAC)},
  pages        = {1--2},
  organization = {IEEE}
}
@inproceedings{sara,
  title        = {{Sara: self-replay augmented record and replay for android in industrial cases}},
  author       = {Guo, Jiaqi and Li, Shuyue and Lou, Jian-Guang and Yang, Zijiang and Liu, Ting},
  year         = 2019,
  booktitle    = {Proceedings of the 28th acm sigsoft international symposium on software testing and analysis},
  pages        = {90--100}
}
@inproceedings{takala2011experiences,
  title        = {{Experiences of system-level model-based GUI testing of an Android application}},
  author       = {Takala, Tommi and Katara, Mika and Harty, Julian},
  year         = 2011,
  booktitle    = {2011 Fourth IEEE International Conference on Software Testing, Verification and Validation},
  pages        = {377--386},
  organization = {IEEE}
}
@electronic{TEMA,
  title        = {{Tema Android Adapter}},
  year         = 2010,
  note         = {\url{https://github.com/tema-tut/tema-android-adapter}}
}
@inproceedings{Li20,
  title        = {{Mapping Natural Language Instructions to Mobile UI Action Sequences}},
  author       = {Li, Yang  and He, Jiacong  and Zhou, Xin  and Zhang, Yuan  and Baldridge, Jason},
  year         = 2020,
  month        = jul,
  booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  publisher    = {Association for Computational Linguistics},
  address      = {Online},
  pages        = {8198--8210},
  doi          = {10.18653/v1/2020.acl-main.729},
  url          = {https://aclanthology.org/2020.acl-main.729},
  editor       = {Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel},
  abstract     = {We present a new problem: grounding natural language instructions to mobile user interface actions, and create three new datasets for it. For full task evaluation, we create PixelHelp, a corpus that pairs English instructions with actions performed by people on a mobile UI emulator. To scale training, we decouple the language and action data by (a) annotating action phrase spans in How-To instructions and (b) synthesizing grounded descriptions of actions for mobile user interfaces. We use a Transformer to extract action phrase tuples from long-range natural language instructions. A grounding Transformer then contextually represents UI objects using both their content and screen position and connects them to object descriptions. Given a starting screen and instruction, our model achieves 70.59{\%} accuracy on predicting complete ground-truth action sequences in PixelHelp.}
}
@inproceedings{ORBITYang,
  title        = {{A grey-box approach for automated GUI-model generation of mobile applications}},
  author       = {Yang, Wei and Prasad, Mukul R and Xie, Tao},
  year         = 2013,
  booktitle    = {ICSE},
  pages        = {250--265},
  organization = {Springer}
}
@inproceedings{vet,
  title        = {{Vet: identifying and avoiding UI exploration tarpits}},
  author       = {Wang, Wenyu and Yang, Wei and Xu, Tianyin and Xie, Tao},
  year         = 2021,
  booktitle    = {ESEC/FSE 2021},
  pages        = {83--94}
}
@inproceedings{glib,
  title        = {{GLIB: towards automated test oracle for graphically-rich applications}},
  author       = {Chen, Ke and Li, Yufei and Chen, Yingfeng and Fan, Changjie and Hu, Zhipeng and Yang, Wei},
  year         = 2021,
  booktitle    = {ESEC/FSE 2021},
  pages        = {1093--1104}
}
@inproceedings{UIFlaky,
  title        = {{An empirical analysis of UI-based flaky tests}},
  author       = {Romano, Alan and Song, Zihe and Grandhi, Sampath and Yang, Wei and Wang, Weihang},
  year         = 2021,
  booktitle    = {2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
  pages        = {1585--1597},
  organization = {IEEE}
}
@article{wang2021speculating,
  title        = {{Speculating Ineffective UI Exploration via Trace Analysis}},
  author       = {Wang, Wenyu and Yang, Wei and Xu, Tianyin and Xie, Tao},
  year         = 2021,
  journal      = {arXiv e-prints},
  pages        = {arXiv--2102}
}
@inproceedings{wang2018empirical,
  title        = {{An empirical study of android test generation tools in industrial cases}},
  author       = {Wang, Wenyu and Li, Dengfeng and Yang, Wei and Cao, Yurui and Zhang, Zhenwen and Deng, Yuetang and Xie, Tao},
  year         = 2018,
  booktitle    = {ASE 2018},
  pages        = {738--748},
  organization = {IEEE}
}
@inproceedings{zheng2017automated,
  title        = {{Automated test input generation for android: Towards getting there in an industrial case}},
  author       = {Zheng, Haibing and Li, Dengfeng and Liang, Beihai and Zeng, Xia and Zheng, Wujie and Deng, Yuetang and Lam, Wing and Yang, Wei and Xie, Tao},
  year         = 2017,
  booktitle    = {ICSE-SEIP},
  pages        = {253--262},
  organization = {IEEE}
}
@article{Chen20,
  title        = {{Wireframe-based UI Design Search through Image Autoencoder}},
  author       = {Chen, Jieshan and Chen, Chunyang and Xing, Zhenchang and Xia, Xin and Zhu, Liming and Grundy, John and Wang, Jinshui},
  year         = 2020,
  month        = {jun},
  journal      = {ACM Trans. Softw. Eng. Methodol.},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  volume       = 29,
  number       = 3,
  doi          = {10.1145/3391613},
  issn         = {1049-331X},
  url          = {https://doi.org/10.1145/3391613},
  issue_date   = {July 2020},
  abstract     = {UI design is an integral part of software development. For many developers who do not have much UI design experience, exposing them to a large database of real-application UI designs can help them quickly build up a realistic understanding of the design space for a software feature and get design inspirations from existing applications. However, existing keyword-based, image-similarity-based, and component-matching-based methods cannot reliably find relevant high-fidelity UI designs in a large database alike to the UI wireframe that the developers sketch, in face of the great variations in UI designs. In this article, we propose a deep-learning-based UI design search engine to fill in the gap. The key innovation of our search engine is to train a wireframe image autoencoder using a large database of real-application UI designs, without the need for labeling relevant UI designs. We implement our approach for Android UI design search, and conduct extensive experiments with artificially created relevant UI designs and human evaluation of UI design search results. Our experiments confirm the superior performance of our search engine over existing image-similarity or component-matching-based methods and demonstrate the usefulness of our search engine in real-world UI design tasks.},
  articleno    = 19,
  numpages     = 31,
  keywords     = {deep learning, auto-encoder, UI search, Android}
}
@inproceedings{Huang19,
  title        = {{Swire: Sketch-based User Interface Retrieval}},
  author       = {Huang, Forrest and Canny, John F. and Nichols, Jeffrey},
  year         = 2019,
  booktitle    = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  location     = {Glasgow, Scotland Uk},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {CHI '19},
  pages        = {1–10},
  doi          = {10.1145/3290605.3300334},
  isbn         = 9781450359702,
  url          = {https://doi.org/10.1145/3290605.3300334},
  abstract     = {Sketches and real-world user interface examples are frequently used in multiple stages of the user interface design process. Unfortunately, finding relevant user interface examples, especially in large-scale datasets, is a highly challenging task because user interfaces have aesthetic and functional properties that are only indirectly reflected by their corresponding pixel data and meta-data. This paper introduces Swire, a sketch-based neural-network-driven technique for retrieving user interfaces. We collect the first large-scale user interface sketch dataset from the development of Swire that researchers can use to develop new sketch-based data-driven design interfaces and applications. Swire achieves high performance for querying user interfaces: for a known validation task it retrieves the most relevant example as within the top-10 results for over 60\% of queries. With this technique, for the first time designers can accurately retrieve relevant user interface examples with free-form sketches natural to their design workflows. We demonstrate several novel applications driven by Swire that could greatly augment the user interface design process.},
  numpages     = 10,
  keywords     = {user interface design, sketching, information retrieval, design examples, deep learning, data-driven design, computer vision}
}
@misc{enwiki:1031455921,
  title        = {{Sigmoid function --- Wikipedia, The Free Encyclopedia}},
  author       = {{Wikipedia contributors}},
  year         = 2021,
  url          = {https://en.wikipedia.org/w/index.php?title=Sigmoid_function&oldid=1031455921},
  note         = {[Online; accessed 9-July-2021]}
}
@article{10.1016/j.infsof.2004.11.006,
  title        = {{Parallel Testing of Distributed Software}},
  author       = {Lastovetsky, Alexey},
  year         = 2005,
  month        = jul,
  journal      = {Information and Software Technology},
  issue_date   = {July, 2005}
}
@inproceedings{10.1109/ICSE-Companion.2019.00055,
  title        = {{An Empirical Study on Leveraging Logs for Debugging Production Failures}},
  author       = {Chen, An Ran},
  year         = 2019,
  booktitle    = {ICSE}
}
@inproceedings{10.1109/ICSE-NIER.2019.00026,
  title        = {{Energy-Based Anomaly Detection a New Perspective for Predicting Software Failures}},
  author       = {Monni, Cristina and Pezz\`{e}, Mauro},
  year         = 2019,
  booktitle    = {ICSE-NIER}
}
@inproceedings{10.1145/1273463.1273468,
  title        = {{Debugging in Parallel}},
  author       = {Jones, James A. and Bowring, James F. and Harrold, Mary Jean},
  year         = 2007,
  booktitle    = {ISSTA}
}
@inproceedings{10.1145/1831708.1831732,
  title        = {{Parallel Symbolic Execution for Structural Test Generation}},
  author       = {Staats, Matt and Pundefinedsundefinedreanu, Corina},
  year         = 2010,
  booktitle    = {ISSTA}
}
@inproceedings{10.1145/1966445.1966463,
  title        = {{Parallel Symbolic Execution for Automated Real-World Software Testing}},
  author       = {Bucur, Stefan and Ureche, Vlad and Zamfir, Cristian and Candea, George},
  year         = 2011,
  booktitle    = {EuroSys}
}
@inproceedings{10.1145/2970276.2970313,
  title        = {{Automated Model-Based Android GUI Testing Using Multi-Level GUI Comparison Criteria}},
  author       = {Baek, Young-Min and Bae, Doo-Hwan},
  year         = 2016,
  booktitle    = {ASE}
}
@inproceedings{10.1145/3132747.3132768,
  title        = {{Pensieve: Non-Intrusive Failure Reproduction for Distributed Systems Using the Event Chaining Approach}},
  author       = {Zhang, Yongle and Makarov, Serguei and Ren, Xiang and Lion, David and Yuan, Ding},
  year         = 2017,
  booktitle    = {SOSP}
}
@inproceedings{10.1145/3133956.3134015,
  title        = {{DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning}},
  author       = {Du, Min and Li, Feifei and Zheng, Guineng and Srikumar, Vivek},
  year         = 2017,
  booktitle    = {CCS}
}
@inproceedings{10.1145/3242587.3242650,
  title        = {{Learning Design Semantics for Mobile Apps}},
  author       = {Liu, Thomas F. and Craft, Mark and Situ, Jason and Yumer, Ersin and Mech, Radomir and Kumar, Ranjitha},
  year         = 2018,
  booktitle    = {UIST}
}
@inproceedings{10.1145/3319535.3363193,
  title        = {{DeepIntent: Deep Icon-Behavior Learning for Detecting Intention-Behavior Discrepancy in Mobile Apps}},
  author       = {Xi, Shengqu and Yang, Shao and Xiao, Xusheng and Yao, Yuan and Xiong, Yayuan and Xu, Fengyuan and Wang, Haoyu and Gao, Peng and Liu, Zhuotao and Xu, Feng and Lu, Jian},
  year         = 2019,
  booktitle    = {CCS}
}
@inproceedings{10.1145/3338906.3338931,
  title        = {{Robust Log-Based Anomaly Detection on Unstable Log Data}},
  author       = {Zhang, Xu and Xu, Yong and Lin, Qingwei and Qiao, Bo and Zhang, Hongyu and Dang, Yingnong and Xie, Chunyu and Yang, Xinsheng and Cheng, Qian and Li, Ze and Chen, Junjie and He, Xiaoting and Yao, Randolph and Lou, Jian-Guang and Chintalapati, Murali and Shen, Furao and Zhang, Dongmei},
  year         = 2019,
  booktitle    = {ESEC/FSE}
}
@inproceedings{10.1145/3338906.3338961,
  title        = {{Latent Error Prediction and Fault Localization for Microservice Applications by Learning from System Trace Logs}},
  author       = {Zhou, Xiang and Peng, Xin and Xie, Tao and Sun, Jun and Ji, Chao and Liu, Dewei and Xiang, Qilin and He, Chuan},
  year         = 2019,
  booktitle    = {ESEC/FSE}
}
@inproceedings{10.1145/3341301.3359650,
  title        = {{The Inflection Point Hypothesis: A Principled Debugging Approach for Locating the Root Cause of a Failure}},
  author       = {Zhang, Yongle and Rodrigues, Kirk and Luo, Yu and Stumm, Michael and Yuan, Ding},
  year         = 2019,
  booktitle    = {SOSP}
}
@inproceedings{10.1145/3377811.3380402,
  title        = {{Time-Travel Testing of Android Apps}},
  author       = {Dong, Zhen and B\"{o}hme, Marcel and Cojocaru, Lucia and Roychoudhury, Abhik},
  year         = 2020,
  booktitle    = {ICSE}
}
@inproceedings{10.5555/2685048.2685099,
  title        = {{Lprof: A Non-Intrusive Request Flow Profiler for Distributed Systems}},
  author       = {Zhao, Xu and Zhang, Yongle and Lion, David and Ullah, Muhammad Faizan and Luo, Yu and Yuan, Ding and Stumm, Michael},
  year         = 2014,
  booktitle    = {OSDI}
}
@inproceedings{10.5555/3026877.3026924,
  title        = {{Non-Intrusive Performance Profiling for Entire Software Stacks Based on the Flow Reconstruction Principle}},
  author       = {Zhao, Xu and Rodrigues, Kirk and Luo, Yu and Yuan, Ding and Stumm, Michael},
  year         = 2016,
  booktitle    = {OSDI}
}
@inproceedings{222595,
  title        = {{REPT: Reverse Debugging of Failures in Deployed Software}},
  author       = {Weidong Cui and Xinyang Ge and Baris Kasikci and Ben Niu and Upamanyu Sharma and Ruoyu Wang and Insu Yun},
  year         = 2018,
  booktitle    = {OSDI}
}
@inproceedings{Su2021WS,
  title        = {{Benchmarking automated GUI testing for Android against real-world bugs}},
  author       = {Su, Ting and Wang, Jue and Su, Zhendong},
  year         = 2021,
  booktitle    = {ESEC/FSE}
}
@misc{AndroidMonkey,
  title        = {{Android Monkey}},
  author       = {Google},
  year         = 2022,
  url          = {https://developer.android.com/studio/test/monkey}
}
}
@inproceedings{10.5555/2486788.2486842,
  title        = {{Assisting Developers of Big Data Analytics Applications When Deploying on Hadoop Clouds}},
  author       = {Shang, Weiyi and Jiang, Zhen Ming and Hemmati, Hadi and Adams, Bram and Hassan, Ahmed E. and Martin, Patrick},
  year         = 2013,
  booktitle    = {ICSE}
}
@inproceedings{8809512,
  title        = {{Robust Anomaly Detection on Unreliable Data}},
  author       = {Zhao, Zilong and Cerf, Sophie and Birke, Robert and Robu, Bogdan and Bouchenak, Sara and Ben Mokhtar, Sonia and Chen, Lydia Y},
  year         = 2019,
  booktitle    = {DSN}
}
@inproceedings{10.1109/ICSE.2019.00041,
  title        = {{IconIntent: Automatic Identification of Sensitive UI Widgets Based on Icon Classification for Android Apps}},
  author       = {Xiao, Xusheng and Wang, Xiaoyin and Cao, Zhihao and Wang, Hanlin and Gao, Peng},
  year         = 2019,
  booktitle    = {ICSE}
}
@electronic{adb,
  title        = {{Android Debug Bridge (adb)}},
  author       = {Google},
  year         = 2018,
  url          = {https://developer.android.com/studio/command-line/adb}
}
@inproceedings{Adler:2011,
  title        = {{Code coverage analysis in practice for large systems}},
  author       = {Y. Adler and N. Behar and O. Raz and O. Shehory and N. Steindler and S. Ur and A. Zlotnick},
  year         = 2011,
  month        = {May}
}
@inproceedings{adler09:advanced,
  title        = {{Advanced Code Coverage Analysis Using Substring Holes}},
  author       = {Adler, Yoram and Farchi, Eitan and Klausner, Moshe and Pelleg, Dan and Raz, Orna and Shochat, Moran and Ur, Shmuel and Zlotnick, Aviad},
  year         = 2009,
  booktitle    = {ISSTA},
  acmid        = 1572278
}
@inproceedings{AimDroid,
  title        = {{AimDroid: Activity-Insulated Multi-level Automated Testing for Android Applications}},
  author       = {Gu, Tianxiao and Cao, Chun and Liu, Tianchi and Sun, Chengnian and Deng, Jing and Ma, Xiaoxing and Lü, Jian},
  year         = 2017,
  booktitle    = {ICSME}
}
@inproceedings{Amalfitano:2012,
  title        = {{Using GUI ripping for automated testing of Android applications}},
  author       = {Amalfitano, Domenico and Fasolino, Anna Rita and Tramontana, Porfirio and De Carmine, Salvatore and Memon, Atif M.},
  year         = 2012,
  booktitle    = {ASE}
}
@inproceedings{Anand:2012,
  title        = {{Automated concolic testing of smartphone apps}},
  author       = {Anand, Saswat and Naik, Mayur and Harrold, Mary Jean and Yang, Hongseok},
  year         = 2012,
  booktitle    = {FSE}
}
@article{andrieu2003introduction,
  title        = {{An Introduction to MCMC for Machine Learning}},
  author       = {Andrieu, Christophe and de Freitas, Nando and Doucet, Arnaud and Jordan, Michael I.},
  year         = 2003,
  month        = {Jan},
  day          = {01},
  journal      = {Machine Learning},
  volume       = 50,
  number       = 1
}
@electronic{android_graphics_path,
  title        = {{The Android graphics path}},
  author       = {Simmonds, Chris},
  year         = 2014,
  url          = {https://elinux.org/images/2/2b/Android\_graphics\_path--chis\_simmonds.pdf}
}
@electronic{android_market_share,
  title        = {{Share of Android Os of Global Smartphone Shipments from 1st Quarter 2011 to 2nd Quarter 2018}},
  author       = {IDC and Gartner},
  year         = 2019,
  url          = {https://www.statista.com/statistics/236027/global-smartphone-os-market-share-of-android/}
}
@electronic{AndroidAppComponents,
  title        = {{Android App Components}},
  author       = {Google},
  year         = 2018,
  url          = {https://developer.android.com/guide/components/fundamentals\#Components}
}
@electronic{AndroidView,
  title        = {{Android View}},
  author       = {Google},
  year         = 2021,
  url          = {https://developer.android.com/reference/android/view/View}
}
@electronic{AndroidViewGroup,
  title        = {{Android ViewGroup}},
  author       = {Google},
  year         = 2021,
  url          = {https://developer.android.com/reference/android/view/ViewGroup}
}
@article{angluin1987learning,
  title        = {{Learning regular sets from queries and counterexamples}},
  author       = {Dana Angluin},
  year         = 1987,
  journal      = {Information and Computation},
  volume       = 75,
  number       = 2,
  url          = {http://www.sciencedirect.com/science/article/pii/0890540187900526}
}
@electronic{aos,
  title        = {{Android Platform Architecture}},
  author       = {Google},
  year         = 2018,
  url          = {https://developer.android.com/guide/platform/}
}
@inproceedings{Azim:2013,
  title        = {{Targeted and Depth-first Exploration for Systematic Testing of Android Apps}},
  author       = {Azim, Tanzirul and Neamtiu, Iulian},
  year         = 2013,
  booktitle    = {OOPSLA}
}
@electronic{Barista,
  title        = {{Barista: An Android framework for making automated tests}},
  url          = {https://moquality.com/barista}
}
@book{brooks95:mythical,
  title        = {{The Mythical Man-Month (Anniversary Ed.)}},
  author       = {Brooks, Frederick P.},
  year         = 1995,
  publisher    = {Addison-Wesley Longman Publishing Co., Inc.}
}
@inproceedings{Cadar:2008,
  title        = {{KLEE: Unassisted and Automatic Generation of High-coverage Tests for Complex Systems Programs}},
  author       = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
  year         = 2008,
  booktitle    = {OSDI},
  url          = {http://dl.acm.org/citation.cfm?id=1855741.1855756},
  acmid        = 1855756
}
@inproceedings{Choi:2013,
  title        = {{Guided GUI Testing of Android Apps with Minimal Restart and Approximate Learning}},
  author       = {Choi, Wontae and Necula, George and Sen, Koushik},
  year         = 2013,
  booktitle    = {OOPSLA}
}
@inproceedings{ChoudharyGO15,
  title        = {{Automated Test Input Generation for Android: Are We There Yet?}},
  author       = {Choudhary, Shauvik Roy and Gorla, Alessandra and Orso, Alessandro},
  year         = 2015,
  booktitle    = {ASE}
}
@electronic{crashlytics,
  title        = {{Firebase Crashlytics}},
  author       = {Google},
  year         = 2020,
  url          = {https://firebase.google.com/docs/crashlytics}
}
@electronic{DalvikFormat,
  title        = {{Android Dalvik Executable format}},
  author       = {Google},
  year         = 2017,
  url          = {https://source.android.com/devices/tech/dalvik/dex-format}
}
@electronic{DalvikVM,
  title        = {{ART and Dalvik}},
  author       = {Google},
  year         = 2017,
  url          = {https://source.android.com/devices/tech/dalvik/}
}
@article{deb2002fast,
  title        = {{A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II}},
  author       = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  year         = 2002,
  month        = apr,
  journal      = {Trans. Evol. Comp},
  volume       = 6,
  number       = 2,
  issue_date   = {April 2002},
  acmid        = 2221582
}
@article{doi:10.1002/stvr.1471,
  title        = {{Parallel mutation testing}},
  author       = {Mateo, Pedro Reales and Usaola, Macario Polo},
  year         = 2013,
  journal      = {Software Testing, Verification and Reliability}
}
@inproceedings{DroidBot,
  title        = {{DroidBot: A Lightweight UI-guided Test Input Generator for Android}},
  author       = {Li, Yuanchun and Yang, Ziyue and Guo, Yao and Chen, Xiangqun},
  year         = 2017,
  booktitle    = {ICSE-C},
  acmid        = 3098352
}
@electronic{droidbot_github,
  title        = {{DroidBot: A lightweight test input generator for Android}},
  author       = {Li, Yuanchun and Yang, Ziyue and Guo, Yao and Chen, Xiangqun},
  year         = 2018,
  url          = {https://github.com/honeynet/droidbot}
}
@electronic{ella,
  title        = {{ELLA: A Tool for Binary Instrumentation of Android Apps}},
  author       = {Saswat Anand},
  year         = 2016,
  url          = {https://github.com/saswatanand/ella}
}
@electronic{emma,
  title        = {{EMMA: A free Java code-coverage tool}},
  author       = {Vlad Roubtsov},
  year         = 2006,
  url          = {http://emma.sourceforge.net}
}
@electronic{Espresso,
  title        = {{Espresso: A unit test framework for Android}},
  url          = {https://google.github.io/android-testing-support-library/docs/espresso}
}
@inproceedings{EvoDroid,
  title        = {{EvoDroid: Segmented Evolutionary Testing of Android Apps}},
  author       = {Mahmood, Riyadh and Mirzaei, Nariman and Malek, Sam},
  year         = 2014,
  booktitle    = {FSE},
  acmid        = 2635896
}
@inproceedings{Godefroid:2005,
  title        = {{DART: Directed Automated Random Testing}},
  author       = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
  year         = 2005,
  booktitle    = {PLDI},
  acmid        = 1065036
}
@electronic{googleplay,
  title        = {{Android Apps on Play Store}},
  author       = {Google},
  year         = 2018,
  url          = {https://play.google.com/store/apps}
}
@inproceedings{Gu:2019:PGT:3339505.3339542,
  title        = {{Practical GUI Testing of Android Applications via Model Abstraction and Refinement}},
  author       = {Gu, Tianxiao and Sun, Chengnian and Ma, Xiaoxing and Cao, Chun and Xu, Chang and Yao, Yuan and Zhang, Qirun and Lu, Jian and Su, Zhendong},
  year         = 2019,
  booktitle    = {ICSE},
  acmid        = 3339542
}
@inproceedings{Hao:2014,
  title        = {{PUMA: Programmable UI-automation for Large-scale Dynamic Analysis of Mobile Apps}},
  author       = {Hao, Shuai and Liu, Bin and Nath, Suman and Halfond, William G.J. and Govindan, Ramesh},
  year         = 2014,
  booktitle    = {MobiSys},
  acmid        = 2594390
}
@electronic{how_to_perform_mobile_automation_testing,
  title        = {{How to perform Mobile Automation Testing of the UI?}},
  author       = {Testsigma},
  year         = 2021,
  url          = {https://testsigma.com/blog/how-to-perform-mobile-automation-testing-of-the-ui/}
}
@electronic{How-to-Automate-Mobile-App-Testing,
  title        = {{How to Automate Mobile App Testing}},
  author       = {MoQuality},
  year         = 2021,
  url          = {https://www.moquality.com/blog/How-to-Automate-Mobile-App-Testing}
}
@inproceedings{inkumsah08:improving,
  title        = {{Improving Structural Testing of Object-Oriented Programs via Integrating Evolutionary Testing and Symbolic Execution}},
  author       = {K. Inkumsah and T. Xie},
  year         = 2008,
  month        = {Sept}
}
@article{King:1976,
  title        = {{Symbolic Execution and Program Testing}},
  author       = {King, James C.},
  year         = 1976,
  month        = jul,
  journal      = {Commun. ACM},
  volume       = 19,
  number       = 7,
  issue_date   = {July 1976},
  acmid        = 360252
}
@inproceedings{lam17:record,
  title        = {{Record and Replay for Android: Are We There Yet in Industrial Cases?}},
  author       = {Lam, Wing and Wu, Zhengkai and Li, Dengfeng and Wang, Wenyu and Zheng, Haibing and Luo, Hui and Yan, Peng and Deng, Yuetang and Xie, Tao},
  year         = 2017,
  booktitle    = {ESEC/FSE},
  acmid        = 3117769
}
@book{linear_programming,
  title        = {{Theory of Linear and Integer Programming}},
  author       = {Schrijver, Alexander},
  year         = 1986,
  publisher    = {John Wiley \& Sons, Inc.}
}
@electronic{logcat,
  title        = {{Logcat command-line tool}},
  author       = {Google},
  year         = 2021,
  url          = {https://developer.android.com/studio/command-line/logcat}
}
@inproceedings{Machiry:2013,
  title        = {{Dynodroid: an input generation system for Android apps}},
  author       = {Aravind Machiry and Rohan Tahiliani and Mayur Naik},
  year         = 2013,
  booktitle    = {Joint Meeting of the European Software Engineering Conference and the {ACM} {SIGSOFT} Symposium on the Foundations of Software Engineering, ESEC/FSE'13, Saint Petersburg, Russian Federation, August 18-26, 2013},
  publisher    = {{ACM}},
  pages        = {224--234},
  editor       = {Bertrand Meyer and Luciano Baresi and Mira Mezini}
}
@inproceedings{Mao:2016,
  title        = {{Sapienz: Multi-objective Automated Testing for Android Applications}},
  author       = {Mao, Ke and Harman, Mark and Jia, Yue},
  year         = 2016,
  booktitle    = {ISSTA},
  acmid        = 2931054
}
@inproceedings{mao2017crowd,
  title        = {{Crowd Intelligence Enhances Automated Mobile Testing}},
  author       = {Mao, Ke and Harman, Mark and Jia, Yue},
  year         = 2017,
  booktitle    = {ASE},
  url          = {http://dl.acm.org/citation.cfm?id=3155562.3155569},
  acmid        = 3155569
}
@electronic{mastering_the_art_of_mobile_testing,
  title        = {{Mastering the Art of Mobile Testing}},
  author       = {SmartBear},
  year         = 2021,
  url          = {https://smartbear.com/resources/ebooks/mastering-the-art-of-mobile-testing/}
}
@article{MirzaeiJPF2012,
  title        = {{Testing Android Apps Through Symbolic Execution}},
  author       = {Mirzaei, Nariman and Malek, Sam and P\u{a}s\u{a}reanu, Corina S. and Esfahani, Naeem and Mahmood, Riyadh},
  year         = 2012,
  month        = nov,
  journal      = {SIGSOFT Softw. Eng. Notes},
  volume       = 37,
  number       = 6,
  issue_date   = {November 2012},
  acmid        = 2382798
}
@electronic{MultipleDex,
  title        = {{Android 64K Method limit}},
  author       = {Google},
  year         = 2018,
  url          = {https://developer.android.com/studio/build/multidex}
}
@inproceedings{Orbit,
  title        = {{A Grey-box Approach for Automated GUI-model Generation of Mobile Applications}},
  author       = {Yang, Wei and Prasad, Mukul R. and Xie, Tao},
  year         = 2013,
  booktitle    = {FASE},
  acmid        = 2450338
}
@inproceedings{Packevicius2018TextSA,
  title        = {{Text Semantics and Layout Defects Detection in Android Apps Using Dynamic Execution and Screenshot Analysis}},
  author       = {Packevi{\v{c}}ius, {\v{S}}ar{\={u}}nas and Barisas, Dominykas and U{\v{s}}aniov, Andrej and Guogis, Evaldas and Barei{\v{s}}a, Eduardas},
  year         = 2018,
  booktitle    = {ICIST}
}
@electronic{py_uiautomator,
  title        = {{Python wrapper of Android uiautomator test tool}},
  author       = {He, Xiaocong},
  year         = 2018,
  url          = {https://github.com/xiaocong/uiautomator}
}
@electronic{Robolectric,
  title        = {{Robolectric: A unit test framework for Android}},
  url          = {http://robolectric.org}
}
@inproceedings{su2017guided,
  title        = {{Guided, Stochastic Model-based GUI Testing of Android Apps}},
  author       = {Su, Ting and Meng, Guozhu and Chen, Yuting and Wu, Ke and Yang, Weiming and Yao, Yao and Pu, Geguang and Liu, Yang and Su, Zhendong},
  year         = 2017,
  booktitle    = {ESEC/FSE},
  acmid        = 3106298
}
@inproceedings{Takala:2011,
  title        = {{Experiences of System-Level Model-Based GUI Testing of an Android Application}},
  author       = {Takala, Tommi and Katara, Mika and Harty, Julian},
  year         = 2011,
  booktitle    = {ICST},
  acmid        = 1990104
}
@electronic{ten_best_tools,
  title        = {{10 Best Android \& iOS Automation App Testing Tools}},
  author       = {Mobile Labs},
  year         = 2018,
  url          = {https://mobilelabsinc.com/blog/top-10-automated-testing-tools-for-mobile-app-testing}
}
@inproceedings{toller-paper,
  title        = {{An Infrastructure Approach to Improving Effectiveness of Android UI Testing Tools}},
  author       = {Wang, Wenyu and Lam, Wing and Xie, Tao},
  year         = 2021,
  booktitle    = {ISSTA}
}
@article{tree_edit_dist,
  title        = {{Simple Fast Algorithms for the Editing Distance Between Trees and Related Problems}},
  author       = {Zhang, Kaizhong and Shasha, Dennis},
  year         = 1989,
  month        = 12,
  journal      = {SIAM Journal on Computing}
}
@electronic{uiautomator,
  title        = {{UI Automator}},
  author       = {Google},
  year         = 2020,
  url          = {https://developer.android.com/training/testing/ui-automator}
}
@article{vallee1999soot,
  title        = {{Soot - a Java Bytecode Optimization Framework}},
  author       = {Vall{\'e}e-Rai, Raja and Co, Phong and Gagnon, Etienne and Hendren, Laurie and Lam, Patrick and Sundaresan, Vijay},
  year         = 1999,
  booktitle    = {CASCON},
  url          = {http://dl.acm.org/citation.cfm?id=781995.782008},
  acmid        = 782008
}
@inproceedings{vet-paper,
  title        = {{Vet: Identifying and Avoiding UI Exploration Tarpits}},
  author       = {Wang, Wenyu and Yang, Wei and Xu, Tianyin and Xie, Tao},
  year         = 2021,
  booktitle    = {ESEC/FSE 2021},
  location     = {Athens, Greece},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  pages        = {83–94},
  doi          = {10.1145/3468264.3468554},
  isbn         = 9781450385626,
  url          = {https://doi.org/10.1145/3468264.3468554},
  numpages     = 12,
  keywords     = {Android testing, trace analysis, UI testing}
}
@misc{vet-github,
  author       = {{VET Artifacts}},
  year         = 2021,
  note         = {\url{https://github.com/VET-UI-Testing/main}}
}
@inproceedings{Wang:2018:ESA:3238147.3240465,
  title        = {{An Empirical Study of Android Test Generation Tools in Industrial Cases}},
  author       = {Wang, Wenyu and Li, Dengfeng and Yang, Wei and Cao, Yurui and Zhang, Zhenwen and Deng, Yuetang and Xie, Tao},
  year         = 2018,
  booktitle    = {ASE},
  acmid        = 3240465
}
@electronic{wechat_users,
  title        = {{WeChat has hit 1 billion monthly active users}},
  author       = {Business Insider},
  year         = 2018,
  url          = {http://www.businessinsider.com/wechat-has-hit-1-billion-monthly-active-users-2018-3}
}
@article{whitley1994genetic,
  title        = {{A genetic algorithm tutorial}},
  author       = {Whitley, Darrell},
  year         = 1994,
  journal      = {Statistics and computing},
  volume       = 4,
  number       = 2
}
@book{wolsey1999integer,
  title        = {{Integer and combinatorial optimization}},
  author       = {Wolsey, Laurence A and Nemhauser, George L},
  year         = 1999,
  publisher    = {John Wiley \& Sons}
}
@inproceedings{yang13:grey,
  title        = {{A Grey-box Approach for Automated GUI-model Generation of Mobile Applications}},
  author       = {Yang, Wei and Prasad, Mukul R. and Xie, Tao},
  year         = 2013,
  booktitle    = {FASE},
  acmid        = 2450338
}
@inproceedings{Ye:2013:DFA:2536853.2536881,
  title        = {{DroidFuzzer: Fuzzing the Android Apps with Intent-Filter Tag}},
  author       = {Ye, Hui and Cheng, Shaoyin and Zhang, Lanbo and Jiang, Fan},
  year         = 2013,
  booktitle    = {MoMM},
  acmid        = 2536881
}
@inproceedings{Zeng:2016,
  title        = {{Automated Test Input Generation for Android: Are We Really There Yet in an Industrial Case?}},
  author       = {Zeng, Xia and Li, Dengfeng and Zheng, Wujie and Xia, Fan and Deng, Yuetang and Lam, Wing and Yang, Wei and Xie, Tao},
  year         = 2016,
  booktitle    = {FSE},
  acmid        = 2983958
}
@inproceedings{zheng17:automated,
  title        = {{Automated Test Input Generation for Android: Towards Getting There in an Industrial Case}},
  author       = {Zheng, Haibing and Li, Dengfeng and Liang, Beihai and Zeng, Xia and Zheng, Wujie and Deng, Yuetang and Lam, Wing and Yang, Wei and Xie, Tao},
  year         = 2017,
  booktitle    = {ICSE-SEIP},
  acmid        = 3103145
}
@inproceedings{fastbot,
  title        = {{Fastbot: A Multi-Agent Model-Based Test Generation System Beijing Bytedance Network Technology Co., Ltd.}},
  author       = {Cai, Tianqin and Zhang, Zhao and Yang, Ping},
  year         = 2020,
  booktitle    = {Proceedings of the IEEE/ACM 1st International Conference on Automation of Software Test},
  location     = {Seoul, Republic of Korea},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {AST '20},
  pages        = {93–96},
  doi          = {10.1145/3387903.3389308},
  isbn         = 9781450379571,
  url          = {https://doi.org/10.1145/3387903.3389308},
  abstract     = {Model-based test (MBT) generation techniques for automated GUI testing are of great value for app testing. Existing GUI model-based testing tools may fall into cyclic operations and run out of resources, when applied to apps with industrial complexity and scalability. In this work, we present a multi-agent GUI MBT system named Fastbot. Fastbot performs model construction on the server end. It applies multi-agent collaboration mechanism to speed up the model construction procedure. The proposed approach was applied on more than 20 applications from Bytedance with more than 1500 million monthly active users. Higher code coverage in less testing time is achieved with comparison of three other automated testing tools including Droidbot, Humanoid and Android Monkey.},
  numpages     = 4,
  keywords     = {Model-based GUI testing, dynamic DAG exploration, automatic testing, multi-agent collaboration, traversal algorithm}
}
@electronic{minitrace,
  title        = {{MiniTrace}},
  author       = {Tianxiao Gu},
  year         = 2021,
  note         = {\url{http://gutianxiao.com/ape}}
}
@electronic{android_marketshare,
  title        = {{Mobile Operating System Market Share Worldwide}},
  author       = {statcounter},
  year         = 2022,
  note         = {\url{https://gs.statcounter.com/os-market-share/mobile/worldwide}}
}
@electronic{android_emulator,
  title        = {{Android Emulator}},
  author       = {Google},
  year         = 2022,
  note         = {\url{https://developer.android.com/studio/run/emulator}}
}
@inproceedings{jing2014morpheus,
  title        = {{Morpheus: automatically generating heuristics to detect android emulators}},
  author       = {Jing, Yiming and Zhao, Ziming and Ahn, Gail-Joon and Hu, Hongxin},
  year         = 2014,
  booktitle    = {Proceedings of the 30th Annual Computer Security Applications Conference},
  pages        = {216--225}
}
@inproceedings{vidas2014evading,
  title        = {{Evading android runtime analysis via sandbox detection}},
  author       = {Vidas, Timothy and Christin, Nicolas},
  year         = 2014,
  booktitle    = {Proceedings of the 9th ACM symposium on Information, computer and communications security},
  pages        = {447--458}
}
@electronic{saucelab,
  title        = {{ Sauce Labs }},
  author       = {Sauce Labs},
  year         = 2022,
  note         = {\url{https://saucelabs.com/}}
}
@electronic{perfecto,
  title        = {{Perfecto: Web \& Mobile App Testing | Continuous Testing}},
  author       = {Perfecto},
  year         = 2022,
  note         = {\url{https://www.perfecto.io/}}
}
@electronic{lambdatest,
  title        = {{LambdaTest: Cross Browser Testing Cloud}},
  author       = {LambdaTest},
  year         = 2022,
  note         = {\url{https://www.lambdatest.com/}}
}
@electronic{kobiton,
  title        = {{Kobiton: Mobile Device Testing}},
  author       = {Kobiton},
  year         = 2022,
  note         = {\url{https://kobiton.com/}}
}
@electronic{awsfarm,
  title        = {{AWS Device Farm}},
  author       = {Amazon Web Services},
  year         = 2022,
  note         = {\url{https://aws.amazon.com/device-farm/}}
}
@inproceedings{wang2018software,
  title        = {{Software protection on the go: A large-scale empirical study on mobile app obfuscation}},
  author       = {Wang, Pei and Bao, Qinkun and Wang, Li and Wang, Shuai and Chen, Zhaofeng and Wei, Tao and Wu, Dinghao},
  year         = 2018,
  booktitle    = {2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)},
  pages        = {26--36},
  organization = {IEEE}
}
@misc{wordpress10302,
  title        = {{Fix NPE on login screen when tapping Help icon \#10302}},
  author       = {GitHub},
  year         = 2019,
  note         = {\url{https://github.com/wordpress-mobile/WordPress-Android/pull/10302}},
  howpublished = {Website}
}
@misc{frost,
  title        = {{Frost-for-Facebook}},
  author       = {GitHub},
  year         = 2020,
  note         = {\url{https://github.com/AllanWang/Frost-for-Facebook}},
  howpublished = {Website}
}
@misc{andbible703,
  title        = {{When in a note invoking find and creating search index crashes the app... \#703}},
  author       = {GitHub},
  year         = 2020,
  note         = {\url{https://github.com/AndBible/and-bible/issues/703}},
  howpublished = {Website}
}
@misc{openlauncher67,
  title        = {{Crash when opening sound panel while Do Not Disturb is activated \#67}},
  author       = {GitHub},
  year         = 2017,
  note         = {\url{https://github.com/OpenLauncherTeam/openlauncher/issues/67}},
  howpublished = {Website}
}
@misc{aphotomanager116,
  title        = {{App Crashes While click on the settings option. \#116}},
  author       = {GitHub},
  year         = 2018,
  note         = {\url{https://github.com/k3b/APhotoManager/issues/116}},
  howpublished = {Website}
}
@misc{ankidroid5756,
  title        = {{Crash on preview of card added to dynamic deck \#5756}},
  author       = {GitHub},
  year         = 2020,
  note         = {\url{https://github.com/ankidroid/Anki-Android/issues/5756}},
  howpublished = {Website}
}
@misc{ankidroid4977,
  title        = {{Crash in DeckPicker after CardBrowser on clean install \#4977}},
  author       = {GitHub},
  year         = 2018,
  note         = {\url{https://github.com/ankidroid/Anki-Android/issues/4977}},
  howpublished = {Website}
}
@misc{mfbook224,
  title        = {{The app crashes every time I try to copy or paste text - 4.0.2 and 4.0.3 \#224}},
  author       = {GitHub},
  year         = 2020,
  note         = {\url{https://github.com/ZeeRooo/MaterialFBook/issues/224}},
  howpublished = {Website}
}
@misc{jacoco,
  title        = {{Java Code Coverage Library}},
  author       = {JaCoCo},
  year         = 2010,
  note         = {\url{https://www.jacoco.org/jacoco/}},
  howpublished = {Website}
}
@misc{amaze1837,
  title        = {{file search crashing \#1837}},
  author       = {GitHub},
  year         = 2020,
  note         = {\url{https://github.com/TeamAmaze/AmazeFileManager/issues/1837}},
  howpublished = {Website}
}
@misc{robotium,
  title        = {{Robotium}},
  author       = {Robotium},
  year         = 2022,
  note         = {\url{https://github.com/robotiumtech/robotium}}
}
@inproceedings{gifdroid,
  title        = {{GIFdroid: Automated Replay of Visual Bug Reports for Android Apps}},
  author       = {Sidong Feng and Chunyang Chen},
  year         = 2022,
  booktitle    = {44th {IEEE/ACM} 44th International Conference on Software Engineering, {ICSE} 2022, Pittsburgh, PA, USA, May 25-27, 2022},
  publisher    = {{ACM}},
  pages        = {1045--1057}
}
%================NEW
@misc{li20spotlight,
  title        = {{Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus}},
  author       = {Gang Li and Yang Li},
  year         = 2023,
  eprint       = {2209.14927},
  archiveprefix = {arXiv},
  primaryclass = {cs.CV}
}


% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries
@inproceedings{1,
author = {Montague, Kyle and Nicolau, Hugo and Hanson, Vicki L.},
title = {Motor-Impaired Touchscreen Interactions in the Wild},
year = {2014},
isbn = {9781450327206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661334.2661362},
doi = {10.1145/2661334.2661362},
abstract = {Touchscreens are pervasive in mainstream technologies; they offer novel user interfaces and exciting gestural interactions. However, to interpret and distinguish between the vast ranges of gestural inputs, the devices require users to consistently perform interactions inline with the predefined location, movement and timing parameters of the gesture recognizers. For people with variable motor abilities, particularly hand tremors, performing these input gestures can be extremely challenging and impose limitations on the possible interactions the user can make with the device. In this paper, we examine touchscreen performance and interaction behaviors of motor-impaired users on mobile devices. The primary goal of this work is to measure and understand the variance of touchscreen interaction performances by people with motor-impairments. We conducted a four-week in-the-wild user study with nine participants using a mobile touchscreen device. A Sudoku stimulus application measured their interaction performance abilities during this time. Our results show that not only does interaction performance vary significantly between users, but also that an individual's interaction abilities are significantly different between device sessions. Finally, we propose and evaluate the effect of novel tap gesture recognizers to accommodate for individual variances in touchscreen interactions.},
booktitle = {Proceedings of the 16th International ACM SIGACCESS Conference on Computers \&amp; Accessibility},
pages = {123–130},
numpages = {8},
keywords = {touchscreen, in-the-wild, user models, motor-impaired},
location = {Rochester, New York, USA},
series = {ASSETS '14}
}
@inproceedings{2,
author = {Zhang, Xiao (Cosmo) and Fang, Kan and Francis, Gregory},
title = {Optimization of Switch Keyboards},
year = {2013},
isbn = {9781450324052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513383.2513394},
doi = {10.1145/2513383.2513394},
abstract = {Patients with motor control difficulties often "type" on a computer using a switch keyboard to guide a scanning cursor to text elements. We show how to optimize some parts of the design of switch keyboards by casting the design problem as mixed integer programming. A new algorithm to find an optimized design solution is approximately 3600 times faster than a previous algorithm, which was also susceptible to finding a non-optimal solution. The optimization requires a model of the probability of an entry error, and we show how to build such a model from experimental data. Example optimized keyboards are demonstrated.},
booktitle = {Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {60},
numpages = {2},
keywords = {"locked-in" patients, mixed integer programming, switch keyboard},
location = {Bellevue, Washington},
series = {ASSETS '13}
}

@inproceedings{3,
author = {Oh, Uran and Kane, Shaun K. and Findlater, Leah},
title = {Follow That Sound: Using Sonification and Corrective Verbal Feedback to Teach Touchscreen Gestures},
year = {2013},
isbn = {9781450324052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513383.2513455},
doi = {10.1145/2513383.2513455},
abstract = {While sighted users may learn to perform touchscreen gestures through observation (e.g., of other users or video tutorials), such mechanisms are inaccessible for users with visual impairments. As a result, learning to perform gestures can be challenging. We propose and evaluate two techniques to teach touchscreen gestures to users with visual impairments: (1) corrective verbal feedback using text-to-speech and automatic analysis of the user's drawn gesture; (2) gesture sonification to generate sound based on finger touches, creating an audio representation of a gesture. To refine and evaluate the techniques, we conducted two controlled lab studies. The first study, with 12 sighted participants, compared parameters for sonifying gestures in an eyes-free scenario and identified pitch + stereo panning as the best combination. In the second study, 6 blind and low-vision participants completed gesture replication tasks with the two feedback techniques. Subjective data and preliminary performance findings indicate that the techniques offer complementary advantages.},
booktitle = {Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {13},
numpages = {8},
keywords = {sonification, blindness, touchscreen, visual impairments, gestures},
location = {Bellevue, Washington},
series = {ASSETS '13}
}

@inproceedings{4,
author = {Nicolau, Hugo and Montague, Kyle and Guerreiro, Tiago and Rodrigues, Andr\'{e} and Hanson, Vicki L.},
title = {Typing Performance of Blind Users: An Analysis of Touch Behaviors, Learning Effect, and In-Situ Usage},
year = {2015},
isbn = {9781450334006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2700648.2809861},
doi = {10.1145/2700648.2809861},
abstract = {Non-visual text-entry for people with visual impairments has focused mostly on the comparison of input techniques reporting on performance measures, such as accuracy and speed. While researchers have been able to establish that non-visual input is slow and error prone, there is little understanding on how to improve it. To develop a richer characterization of typing performance, we conducted a longitudinal study with five novice blind users. For eight weeks, we collected in-situ usage data and conducted weekly laboratory assessment sessions. This paper presents a thorough analysis of typing performance that goes beyond traditional aggregated measures of text-entry and reports on character-level errors and touch measures. Our findings show that users improve over time, even though it is at a slow rate (0.3 WPM per week). Substitutions are the most common type of error and have a significant impact on entry rates. In addition to text input data, we analyzed touch behaviors, looking at touch contact points, exploration movements, and lift positions. We provide insights on why and how performance improvements and errors occur. Finally, we derive some implications that should inform the design of future virtual keyboards for non-visual input.},
booktitle = {Proceedings of the 17th International ACM SIGACCESS Conference on Computers \&amp; Accessibility},
pages = {273–280},
numpages = {8},
keywords = {performance, blind, touch, input, behavior, novice, text-entry},
location = {Lisbon, Portugal},
series = {ASSETS '15}
}

@inproceedings{5,
author = {Berke, Larwan and Caulfield, Christopher and Huenerfauth, Matt},
title = {Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings},
year = {2017},
isbn = {9781450349260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132525.3132541},
doi = {10.1145/3132525.3132541},
abstract = {Recent advances in Automatic Speech Recognition (ASR) have made this technology a potential solution for transcribing audio input in real-time for people who are Deaf or Hard of Hearing (DHH). However, ASR is imperfect; users must cope with errors in the output. While some prior research has studied ASR-generated transcriptions to provide captions for DHH people, there has not been a systematic study of how to best present captions that may include errors from ASR software nor how to make use of the ASR system's word-level confidence. We conducted two studies, with 21 and 107 DHH participants, to compare various methods of visually presenting the ASR output with certainty values. Participants answered subjective preference questions and provided feedback on how ASR captioning could be used with confidence display markup. Users preferred captioning styles with which they were already most familiar (that did not display confidence information), and they were concerned about the accuracy of ASR systems. While they expressed interest in systems that display word confidence during captions, they were concerned that text appearance changes may be distracting. The findings of this study should be useful for researchers and companies developing automated captioning systems for DHH users.},
booktitle = {Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {155–164},
numpages = {10},
keywords = {communication, feedback, automatic speech recognition, user study, deaf and hard of hearing, real-time captions},
location = {Baltimore, Maryland, USA},
series = {ASSETS '17}
}

@inproceedings{6,
author = {Montague, Kyle and Hanson, Vicki L. and Cobley, Andy},
title = {Designing for Individuals: Usable Touch-Screen Interaction through Shared User Models},
year = {2012},
isbn = {9781450313216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2384916.2384943},
doi = {10.1145/2384916.2384943},
abstract = {Mobile touch-screen devices are becoming increasingly popular across a diverse range of users. Whilst there is a wealth of information and utilities available via downloadable apps, there is still a large proportion of users with visual and motor impairments who are unable to use the technology fully due to their interaction needs. In this paper we present an evaluation of the use of shared user modelling and adaptive interfaces to improve the accessibility of mobile touch-screen technologies. By using abilities based information collected through application use and continually updating the user model and interface adaptations, it is easy for users to make applications aware of their needs and preferences. Three smart phone apps were created for this study and tested with 12 adults who had diverse visual and motor impairments. Results indicated significant benefits from the shared user models that can automatically adapt interfaces, across applications, to address usability needs.},
booktitle = {Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {151–158},
numpages = {8},
keywords = {mobile touch screens, adaptive interfaces, shared user modelling},
location = {Boulder, Colorado, USA},
series = {ASSETS '12}
}

@inproceedings{7,
author = {Szpiro, Sarit Felicia Anais and Hashash, Shafeka and Zhao, Yuhang and Azenkot, Shiri},
title = {How People with Low Vision Access Computing Devices: Understanding Challenges and Opportunities},
year = {2016},
isbn = {9781450341240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2982142.2982168},
doi = {10.1145/2982142.2982168},
abstract = {Low vision is a pervasive condition in which people have difficulty seeing even with corrective lenses. People with low vision frequently use mainstream computing devices, however how they use their devices to access information and whether digital low vision accessibility tools provide adequate support remains understudied. We addressed these questions with a contextual inquiry study. We observed 11 low vision participants using their smartphones, tablets, and computers when performing simple tasks such as reading email. We found that participants preferred accessing information visually than aurally (e.g., screen readers), and juggled a variety of accessibility tools. However, accessibility tools did not provide them with appropriate support. Moreover, participants had to constantly perform multiple gestures in order to see content comfortably. These challenges made participants inefficient-they were slow and often made mistakes; even tech savvy participants felt frustrated and not in control. Our findings reveal the unique needs of low vision people, which differ from those of people with no vision and design opportunities for improving low vision accessibility tools.},
booktitle = {Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {171–180},
numpages = {10},
keywords = {computing devices, contextual inquiry, low vision, accessibility},
location = {Reno, Nevada, USA},
series = {ASSETS '16}
}

@inproceedings{8,
author = {Norman, Kirk and Arber, Yevgeniy and Kuber, Ravi},
title = {How Accessible is the Process of Web Interface Design?},
year = {2013},
isbn = {9781450324052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513383.2513385},
doi = {10.1145/2513383.2513385},
abstract = {This paper describes a data gathering study, examining the experiences and day-to-day challenges faced by blind web interface developers when designing sites and online applications. Findings have revealed that considerable amounts of time and cognitive effort can be spent checking code in text editing software and examining the content presented via the web browser. Participants highlighted the burden experienced from committing large sections of code to memory, and the restrictions associated with assistive technologies when performing collaborative tasks with sighted developers and clients. Our future work aims to focus on the development of a multimodal web editing and browsing solution, designed to support both blind and sighted parties during the design process.},
booktitle = {Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {51},
numpages = {2},
keywords = {HTML, accessibility, blind, web development, screen reader},
location = {Bellevue, Washington},
series = {ASSETS '13}
}

@inproceedings{9,
author = {Salehnamadi, Navid and Alshayban, Abdulaziz and Lin, Jun-Wei and Ahmed, Iftekhar and Branham, Stacy and Malek, Sam},
title = {Latte: Use-Case and Assistive-Service Driven Automated Accessibility Testing Framework for Android},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445455},
doi = {10.1145/3411764.3445455},
abstract = { For 15% of the world population with disabilities, accessibility is arguably the most critical software quality attribute. The ever-growing reliance of users with disability on mobile apps further underscores the need for accessible software in this domain. Existing automated accessibility assessment techniques primarily aim to detect violations of predefined guidelines, thereby produce a massive amount of accessibility warnings that often overlook the way software is actually used by users with disability. This paper presents a novel, high-fidelity form of accessibility testing for Android apps, called Latte, that automatically reuses tests written to evaluate an app’s functional correctness to assess its accessibility as well. Latte first extracts the use case corresponding to each test, and then executes each use case in the way disabled users would, i.e., using assistive services. Our empirical evaluation on real-world Android apps demonstrates Latte’s effectiveness in detecting substantially more useful defects than prior techniques.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {274},
numpages = {11},
keywords = {Accessibility, Automated Testing, Mobile Application},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10,
author = {Zhang, Xiaoyi and de Greef, Lilian and Swearngin, Amanda and White, Samuel and Murray, Kyle and Yu, Lisa and Shan, Qi and Nichols, Jeffrey and Wu, Jason and Fleizach, Chris and Everitt, Aaron and Bigham, Jeffrey P},
title = {Screen Recognition: Creating Accessibility Metadata for Mobile Applications from Pixels},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445186},
doi = {10.1145/3411764.3445186},
abstract = { Many accessibility features available on mobile platforms require applications (apps) to provide complete and accurate metadata describing user interface (UI) components. Unfortunately, many apps do not provide sufficient metadata for accessibility features to work as expected. In this paper, we explore inferring accessibility metadata for mobile apps from their pixels, as the visual interfaces often best reflect an app’s full functionality. We trained a robust, fast, memory-efficient, on-device model to detect UI elements using a dataset of 77,637 screens (from 4,068 iPhone apps) that we collected and annotated. To further improve UI detections and add semantic information, we introduced heuristics (e.g., UI grouping and ordering) and additional models (e.g., recognize UI content, state, interactivity). We built Screen Recognition to generate accessibility metadata to augment iOS VoiceOver. In a study with 9 screen reader users, we validated that our approach improves the accessibility of existing mobile apps, enabling even previously inaccessible apps to be used. },
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {275},
numpages = {15},
keywords = {accessibility enhancement, ui detection, mobile accessibility},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{11,
author = {Park, Kyudong and Goh, Taedong and So, Hyo-Jeong},
title = {Toward Accessible Mobile Application Design: Developing Mobile Application Accessibility Guidelines for People with Visual Impairment},
year = {2014},
isbn = {9788968487521},
publisher = {Hanbit Media, Inc.},
address = {Seoul, KOR},
abstract = {While the use of Smartphones has improved the life of people with disabilities, several mobile content and applications remain inaccessible to people with visual impairment. Toward the overarching goal of accessible mobile application design, this two-phased study attempts to develop mobile application accessibility guidelines for people with visual impairment. First, we investigated how people with visual impairment use mobile phones. Four participants with visual impairment performed specified tasks. Their usage patterns and follow-up interviews were analyzed. Serious accessibility problems were found in both typing and VoiceOver functions. Second, we evaluated and developed systematic guidelines and standards of designing accessible mobile applications through a heuristic walkthrough method. Four experts with extensive experiences and knowledge about mobile application development/design used the VoiceOver function with iPhone for 5 days, and then walked through thought-provoking tasks. In conclusion, we propose a set of 10 heuristics for developing accessible mobile applications and suggest a critical need for internationally-agreed guidelines and standards to improve the current mobile environment for people with disabilities.},
booktitle = {Proceedings of HCI Korea},
pages = {31–38},
numpages = {8},
keywords = {evaluation, heuristic walkthrough, design guideline, standardization, VoiceOver},
location = {Seoul, Republic of Korea},
series = {HCIK '15}
}

@inproceedings{12,
author = {Peng, Yi-Hao and Lin, Muh-Tarng and Chen, Yi and Chen, TzuChuan and Ku, Pin Sung and Taele, Paul and Lim, Chin Guan and Chen, Mike Y.},
title = {PersonalTouch: Improving Touchscreen Usability by Personalizing Accessibility Settings Based on Individual User's Touchscreen Interaction},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300913},
doi = {10.1145/3290605.3300913},
abstract = {Modern touchscreen devices have recently introduced customizable touchscreen settings to improve accessibility for users with motor impairments. For example, iOS 10 introduced the following four Touch Accommodation settings: 1) Hold Duration, 2) Ignore Repeat, 3) Tap Assistance, and 4) Tap Assistance Gesture Delay. These four independent settings lead to a total of more than 1 million possible configurations, making it impractical to manually determine the optimal settings. We present PersonalTouch, which collects and analyzes touchscreen gestures performed by individual users, and recommends personalized, optimal touchscreen accessibility settings. Results from our user study show that PersonalTouch significantly improves touch input success rate for users with motor impairments (20.2%, N=12, p=.00054) and for users without motor impairments (1.28%, N=12, p=.032).},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {motor impairment, accessibility, touch-screen interaction, personalization},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{13,
author = {Vatavu, Radu-Daniel},
title = {Improving Gesture Recognition Accuracy on Touch Screens for Users with Low Vision},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025941},
doi = {10.1145/3025453.3025941},
abstract = {We contribute in this work on gesture recognition to improve the accessibility of touch screens for people with low vision. We examine the accuracy of popular recognizers for gestures produced by people with and without visual impairments, and we show that the user-independent accuracy of $P, the best recognizer among those evaluated, is small for people with low vision (83.8%), despite $P being very effective for gestures produced by people without visual impairments (95.9%). By carefully analyzing the gesture articulations produced by people with low vision, we inform key algorithmic revisions for the P recognizer, which we call P+. We show significant accuracy improvements of $P+ for gestures produced by people with low vision, from 83.8% to 94.7% on average and up to 98.2%, and 3x faster execution times compared to P.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {4667–4679},
numpages = {13},
keywords = {P+, recognition, visual impairments, P, gesture recognition, algorithms, evaluation, point clouds, touch screens, recognition accuracy, low vision, 1, touch gestures},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{14,
author = {Yeh, Tom and Chang, Tsung-Hsiang and Xie, Bo and Walsh, Greg and Watkins, Ivan and Wongsuphasawat, Krist and Huang, Man and Davis, Larry S. and Bederson, Benjamin B.},
title = {Creating Contextual Help for GUIs Using Screenshots},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047214},
doi = {10.1145/2047196.2047214},
abstract = {Contextual help is effective for learning how to use GUIs by showing instructions and highlights on the actual interface rather than in a separate viewer. However, end-users and third-party tech support typically cannot create contextual help to assist other users because it requires programming skill and source code access. We present a creation tool for contextual help that allows users to apply common computer skills-taking screenshots and writing simple scripts. We perform pixel analysis on screenshots to make this tool applicable to a wide range of applications and platforms without source code access. We evaluated the tool's usability with three groups of participants: developers, in-structors, and tech support. We further validated the applicability of our tool with 60 real tasks supported by the tech support of a university campus.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {145–154},
numpages = {10},
keywords = {pixel analysis, help, contextual help},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{15,
author = {Zhang, Xiaoyi and Ross, Anne Spencer and Fogarty, James},
title = {Robust Annotation of Mobile Application Interfaces in Methods for Accessibility Repair and Enhancement},
year = {2018},
isbn = {9781450359481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242587.3242616},
doi = {10.1145/3242587.3242616},
abstract = {Accessibility issues in mobile apps make those apps difficult or impossible to access for many people. Examples include elements that fail to provide alternative text for a screen reader, navigation orders that are difficult, or custom widgets that leave key functionality inaccessible. Social annotation techniques have demonstrated compelling approaches to such accessibility concerns in the web, but have been difficult to apply in mobile apps because of the challenges of robustly annotating interfaces. This research develops methods for robust annotation of mobile app interface elements. Designed for use in runtime interface modification, our methods are based in screen identifiers, element identifiers, and screen equivalence heuristics. We implement initial developer tools for annotating mobile app accessibility metadata, evaluate our current screen equivalence heuristics in a dataset of 2038 screens collected from 50 mobile apps, present three case studies implementing runtime repair of common accessibility issues, and examine repair of real-world accessibility issues in 26 apps. These contributions overall demonstrate strong opportunities for social annotation in mobile accessibility.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {609–621},
numpages = {13},
keywords = {runtime modification, accessibility, robust annotation},
location = {Berlin, Germany},
series = {UIST '18}
}

@inproceedings{16,
author = {Alshayban, Abdulaziz and Ahmed, Iftekhar and Malek, Sam},
title = {Accessibility Issues in Android Apps: State of Affairs, Sentiments, and Ways Forward},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380392},
doi = {10.1145/3377811.3380392},
abstract = {Mobile apps are an integral component of our daily life. Ability to use mobile apps is important for everyone, but arguably even more so for approximately 15% of the world population with disabilities. This paper presents the results of a large-scale empirical study aimed at understanding accessibility of Android apps from three complementary perspectives. First, we analyze the prevalence of accessibility issues in over 1, 000 Android apps. We find that almost all apps are riddled with accessibility issues, hindering their use by disabled people. We then investigate the developer sentiments through a survey aimed at understanding the root causes of so many accessibility issues. We find that in large part developers are unaware of accessibility design principles and analysis tools, and the organizations in which they are employed do not place a premium on accessibility. We finally investigate user ratings and comments on app stores. We find that due to the disproportionately small number of users with disabilities, user ratings and app popularity are not indicative of the extent of accessibility issues in apps. We conclude the paper with several observations that form the foundation for future research and development.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1323–1334},
numpages = {12},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{17,
author = {Chen, Jieshan and Chen, Chunyang and Xing, Zhenchang and Xu, Xiwei and Zhu, Liming and Li, Guoqiang and Wang, Jinshui},
title = {Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380327},
doi = {10.1145/3377811.3380327},
abstract = {According to the World Health Organization(WHO), it is estimated that approximately 1.3 billion people live with some forms of vision impairment globally, of whom 36 million are blind. Due to their disability, engaging these minority into the society is a challenging problem. The recent rise of smart mobile phones provides a new solution by enabling blind users' convenient access to the information and service for understanding the world. Users with vision impairment can adopt the screen reader embedded in the mobile operating systems to read the content of each screen within the app, and use gestures to interact with the phone. However, the prerequisite of using screen readers is that developers have to add natural-language labels to the image-based components when they are developing the app. Unfortunately, more than 77% apps have issues of missing labels, according to our analysis of 10,408 Android apps. Most of these issues are caused by developers' lack of awareness and knowledge in considering the minority. And even if developers want to add the labels to UI components, they may not come up with concise and clear description as most of them are of no visual issues. To overcome these challenges, we develop a deep-learning based model, called LabelDroid, to automatically predict the labels of image-based buttons by learning from large-scale commercial apps in Google Play. The experimental results show that our model can make accurate predictions and the generated labels are of higher quality than that from real Android developers.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {322–334},
numpages = {13},
keywords = {accessibility, image-based buttons, content description, user interface, neural networks},
location = {Seoul, South Korea},
series = {ICSE '20}
}
@INPROCEEDINGS{18,  author={Vendome, Christopher and Solano, Diana and Liñán, Santiago and Linares-Vásquez, Mario},  booktitle={2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)},   title={Can Everyone use my app? An Empirical Study on Accessibility in Android Apps},   year={2019},  volume={},  number={},  pages={41-52},  doi={10.1109/ICSME.2019.00014}}

@book{19,
  abstract = {The book presents a broad perspective on software systems engineering, concentrating on widely used techniques for developing large-scale systems. Building on the widely acclaimed strengths of the 8th edition, the 9th edition updates readers with the latest developments in the field while remaining the most current Software Engineering text in the market with quality trusted coverage and practical case studies. This text is structured into 6 parts: Introduction; Requirements Engineering; Design; Software Development; Verification and Validation; Management. An up-to-date reference for software engineers.},
  added-at = {2017-05-14T14:19:12.000+0200},
  address = {Harlow, England},
  author = {Sommerville, Ian},
  biburl = {https://www.bibsonomy.org/bibtex/2094b37a4bb8b242d7694cdc9142e0d80/flint63},
  edition = 9,
  file = {eBook:2010/Sommerville10.pdf:PDF;Amazon Search inside:http\://www.amazon.de/gp/reader/0137035152/:URL},
  groups = {public},
  interhash = {3cb472cdd9d5e4f37480b38846de450e},
  intrahash = {094b37a4bb8b242d7694cdc9142e0d80},
  isbn = {978-0-13-703515-1},
  keywords = {01841 105 book shelf software engineering development intro},
  publisher = {Addison-Wesley},
  timestamp = {2018-04-16T12:01:54.000+0200},
  title = {Software Engineering},
  username = {flint63},
  year = 2010
}

@book{20,
  abstract = {First, businesses discovered quality as a key competitive edge; next came service. This title reveals how smart design is the next competitive frontier. 'The Design of Everyday Things' is a primer on how and why some products satisfy customers while others only frustrate them.},
  added-at = {2012-09-28T16:01:03.000+0200},
  address = {[New York]},
  author = {Norman, Donald A.},
  biburl = {https://www.bibsonomy.org/bibtex/2e51bef4e8b0c0ea3c13e8b5e6a561bed/schmidt2},
  description = {The Design of Everyday Things: Amazon.de: Don Norman: Englische Bücher},
  interhash = {c6fd1560a6892285580779b96140f88c},
  intrahash = {e51bef4e8b0c0ea3c13e8b5e6a561bed},
  isbn = {0465067107 9780465067107},
  keywords = {books design lang:en product_design toread ui usability ux},
  publisher = {Basic Books},
  refid = {215302602},
  timestamp = {2012-09-28T16:01:03.000+0200},
  title = {The design of everyday things},
  url = {http://www.amazon.de/The-Design-Everyday-Things-Norman/dp/0465067107/ref=wl_it_dp_o_pC_S_nC?ie=UTF8\&colid=151193SNGKJT9\&coliid=I262V9ZRW8HR2C},
  year = 2002
}



@AINPROCEEDINGS{21,  author={Kong, Pingfan and Li, Li and Gao, Jun and Liu, Kui and Bissyandé, Tegawendé F. and Klein, Jacques},  journal={IEEE Transactions on Reliability},   title={Automated Testing of Android Apps: A Systematic Literature Review},   year={2019},  volume={68},  number={1},  pages={45-66},  doi={10.1109/TR.2018.2865733}}


@INPROCEEDINGS{22,
  abstract = {Guidelines, techniques, and methods have been presented in the literature in recent years to contribute to the development of accessible software and to promote digital inclusion. Considering that software product quality depends on the quality of the development process, researchers have investigated how to include accessibility during the software development process in order to obtain accessible software. Two Systematic Literature Reviews (SLR) have been conducted in the past to identify such research initiatives. This paper presents a new SLR, considering the period from 2011 to 2019. The review of 94 primary studies showed the distribution of publications on different phases of the software life cycle, mainly the design and testing phases. The study also identified, for the first time, papers about accessibility and software process establishment. This result reinforces that, in fact, accessibility is not characterized as a property of the final software only. Instead, it evolves over the software life cycle. Besides, this study aims to provide designers and developers with an updated view of methods, tools, and other assets that contribute to process enrichment, valuing accessibility, as well as shows the gaps and challenges which deserve to be investigated.},
  author = {D{\'e}bora Maria Barroso Paiva and Andr{\'e} Pimenta Freire and Renata Pontin {de Mattos Fortes}},
  doi = {https://doi.org/10.1016/j.jss.2020.110819},
  issn = {0164-1212},
  journal = {Journal of Systems and Software},
  keywords = {Accessibility, Software Engineering, Systematic Literature Review, Design for disabilities, Methods for accessibility},
  pages = {110819},
  title = {Accessibility and Software Engineering Processes: A Systematic Literature Review},
  url = {https://www.sciencedirect.com/science/article/pii/S0164121220302168},
  volume = {171},
  year = {2021}
  Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2020.110819}}

@url{23,
  year = {2019},
  Title = {ADAlaws \url{https://www.ada.gov/cguide.htm}}}

@url{24,
year = {2011},
Title = {The Current State of Cell Phone Accessibility \url{https://www.afb.org/aw/12/6/15926}}}

@url{25,
year = {2022},
Title = {Accessibility \url{https://www.apple.com/accessibility/}}}

@url{26,
year = {2022},
Title = {Accessibility \url{https://support.google.com/accessibility}}}

@url{27,
year = {2018},
Title = {ADA Web Accessibility Lawsuit Recap Report \url{https://blog.usablenet.com/2018- ada- web- accessibility- lawsuit- recap- report}}}

@url{28,
year = {2011},
Title = {World Report on Disability \url{http://www.who.int/disabilities/world_report/2011/en/}}}

@url{29,
year = {2019},
Title = {Google TalkBack source code \url{https://github.com/google/talkback}}}

@url{30,
year = {2019},
Title = {VoiceOver \url{https://cloud.google.com/translate/docs/}}}

@url{31,
year = {2019},
Title = {Adaptivity and layout - visual design - IOS - human interface guidelines - apple developer \url{https://developer.apple.com/design/human-interface-guidelines/ios/visual-design/adaptivity-and-layout/}}}

@url{32,
year = {2019},
Title = {“Design for Android: android developers,” \url{https://developer.android.com/design}}}

@inproceedings{33,
author = {Azenkot, Shiri and Wobbrock, Jacob O. and Prasain, Sanjana and Ladner, Richard E.},
title = {Input Finger Detection for Nonvisual Touch Screen Text Entry in <i>Perkinput</i>},
year = {2012},
isbn = {9781450314206},
publisher = {Canadian Information Processing Society},
address = {CAN},
abstract = {We present Input Finger Detection (IFD), a novel technique for nonvisual touch screen input, and its application, the Perkinput text entry method. With IFD, signals are input into a device with multi-point touches, where each finger represents one bit, either touching the screen or not. Maximum likelihood and tracking algorithms are used to detect which fingers touch the screen based on user-set reference points. The Perkinput text entry method uses the 6-bit Braille encoding with audio feedback, enabling one- and two-handed input. A longitudinal evaluation with 8 blind participants who are proficient in Braille showed that one-handed Perkinput was significantly faster and more accurate than iPhone's VoiceOver. Furthermore, in a case study to evaluate expert performance, one user reached an average session speed of 17.56 words per minute (WPM) with an average uncorrected error rate of just 0.14% using one hand for input. The same participant reached an average session speed of 38.0 WPM with two-handed input and an error rate of just 0.26%. Her fastest phrase was entered at 52.4 WPM and no errors.},
booktitle = {Proceedings of Graphics Interface 2012},
pages = {121–129},
numpages = {9},
keywords = {blind, mobile devices, text entry},
location = {Toronto, Ontario, Canada},
series = {GI '12}
}

@url{34,
year = {2022},
Title = {“Live stream your virtual event online: Vimeo enterprise,” \url{https://vimeo.com/enterprise/stream-virtual-events}}}

@inproceedings{35,
author = {Bragg, Danielle and Caselli, Naomi and Gallagher, John W. and Goldberg, Miriam and Oka, Courtney J. and Thies, William},
title = {ASL Sea Battle: Gamifying Sign Language Data Collection},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445416},
doi = {10.1145/3411764.3445416},
abstract = {The development of accurate machine learning models for sign languages like American Sign Language (ASL) has the potential to break down communication barriers for deaf signers. However, to date, no such models have been robust enough for real-world use. The primary barrier to enabling real-world applications is the lack of appropriate training data. Existing training sets suffer from several shortcomings: small size, limited signer diversity, lack of real-world settings, and missing or inaccurate labels. In this work, we present ASL Sea Battle, a sign language game designed to collect datasets that overcome these barriers, while also providing fun and education to users. We conduct a user study to explore the data quality that the game collects, and the user experience of playing the game. Our results suggest that ASL Sea Battle can reliably collect and label real-world sign language videos, and provides fun and education at the expense of data throughput.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {271},
numpages = {13},
keywords = {crowdsourcing, ASL, game, data, sign language, machine learning},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{36,
author = {Fok, Raymond and Kaur, Harmanpreet and Palani, Skanda and Mott, Martez E. and Lasecki, Walter S.},
title = {Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users},
year = {2018},
isbn = {9781450356503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234695.3236343},
doi = {10.1145/3234695.3236343},
abstract = {Mobile, wearable, and other ubiquitous computing devices are increasingly creating a context in which conventional keyboard and screen-based inputs are being replaced in favor of more natural speech-based interactions. Digital personal assistants use speech to control a wide range of functionality, from environmental controls to information access. However, many deaf and hard-of-hearing users have speech patterns that vary from those of hearing users due to incomplete acoustic feedback from their own voices. Because automatic speech recognition (ASR) systems are largely trained using speech from hearing individuals, speech-controlled technologies are typically inaccessible to deaf users. Prior work has focused on providing deaf users access to aural output via real-time captioning or signing, but little has been done to improve users' ability to provide input to these systems' speech-based interfaces. Further, the vocalization patterns of deaf speech often make accurate recognition intractable for both automated systems and human listeners, making traditional approaches to mitigate ASR limitations, such as human captionists, less effective. To bridge this accessibility gap, we investigate the limitations of common speech recognition approaches and techniques---both automatic and human-powered---when applied to deaf speech. We then explore the effectiveness of an iterative crowdsourcing workflow, and characterize the potential for groups to collectively exceed the performance of individuals. This paper contributes a better understanding of the challenges of deaf speech recognition and provides insights for future system development in this space.},
booktitle = {Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {57–67},
numpages = {11},
keywords = {human computation, automatic speech recognition, deaf speech, deaf and hard-of-hearing, accessibility},
location = {Galway, Ireland},
series = {ASSETS '18}
}

@inproceedings{37,
author = {MacKenzie, I. Scott and Soukoreff, R. William and Helga, Joanna},
title = {1 Thumb, 4 Buttons, 20 Words per Minute: Design and Evaluation of H4-Writer},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047258},
doi = {10.1145/2047196.2047258},
abstract = {We present what we believe is the most efficient and quickest four-key text entry method available. H4-Writer uses Huffman coding to assign minimized key sequences to letters, with full access to error correction, punctuation, digits, modes, etc. The key sequences are learned quickly, and support eyes-free entry. With KSPC = 2.321, the effort to enter text is comparable to multitap on a mobile phone keypad; yet multitap requires nine keys. In a longitudinal study with six participants, an average text entry speed of 20.4 wpm was observed in the 10th session. Error rates were under 1%. To improve external validity, an extended session was included that required input of punctuation and other symbols. Entry speed dropped only by about 3 wpm, suggesting participants quickly leveraged their acquired skill with H4-Writer to access advanced features.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {471–480},
numpages = {10},
keywords = {mobile text entry, small devices, text entry, Huffman coding},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}
@INPROCEEDINGS{38,  author={Bajammal, Mohammad and Mesbah, Ali},  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},   title={Semantic Web Accessibility Testing via Hierarchical Visual Analysis},   year={2021},  volume={},  number={},  pages={1610-1621},  doi={10.1109/ICSE43902.2021.00143}}
@url{39,
year = {2022},
Title = {JAWS \url{https://www.freedomscientific.com/products/software/jaws/}}}
@url{40,
year = {2022},
Title = {NV Access \url{https://www.nvaccess.org/download/}}}

@inproceedings{41,
author = {Wu, Jason and Zhang, Xiaoyi and Nichols, Jeff and Bigham, Jeffrey P},
title = {Screen Parsing: Towards Reverse Engineering of UI Models from Screenshots},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474763},
doi = {10.1145/3472749.3474763},
abstract = { Automated understanding of user interfaces (UIs) from their pixels can improve accessibility, enable task automation, and facilitate interface design without relying on developers to comprehensively provide metadata. A first step is to infer what UI elements exist on a screen, but current approaches are limited in how they infer how those elements are semantically grouped into structured interface definitions. In this paper, we motivate the problem of screen parsing, the task of predicting UI elements and their relationships from a screenshot. We describe our implementation of screen parsing and provide an effective training procedure that optimizes its performance. In an evaluation comparing the accuracy of the generated output, we find that our implementation significantly outperforms current systems (up to 23%). Finally, we show three example applications that are facilitated by screen parsing: (i) UI similarity search, (ii) accessibility enhancement, and (iii) code generation from UI screenshots.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {470–483},
numpages = {14},
keywords = {user interface modeling, hierarchy prediction, ui semantics},
location = {Virtual Event, USA},
series = {UIST '21}
}
@inproceedings{42,
author = {Moran, Kevin and Li, Boyang and Bernal-C\'{a}rdenas, Carlos and Jelf, Dan and Poshyvanyk, Denys},
title = {Automated Reporting of GUI Design Violations for Mobile Apps},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180246},
doi = {10.1145/3180155.3180246},
abstract = {The inception of a mobile app often takes form of a mock-up of the Graphical User Interface (GUI), represented as a static image delineating the proper layout and style of GUI widgets that satisfy requirements. Following this initial mock-up, the design artifacts are then handed off to developers whose goal is to accurately implement these GUIs and the desired functionality in code. Given the sizable abstraction gap between mock-ups and code, developers often introduce mistakes related to the GUI that can negatively impact an app's success in highly competitive marketplaces. Moreover, such mistakes are common in the evolutionary context of rapidly changing apps. This leads to the time-consuming and laborious task of design teams verifying that each screen of an app was implemented according to intended design specifications.This paper introduces a novel, automated approach for verifying whether the GUI of a mobile app was implemented according to its intended design. Our approach resolves GUI-related information from both implemented apps and mock-ups and uses computer vision techniques to identify common errors in the implementations of mobile GUIs. We implemented this approach for Android in a tool called Gvt and carried out both a controlled empirical evaluation with open-source apps as well as an industrial evaluation with designers and developers from Huawei. The results show that Gvt solves an important, difficult, and highly practical problem with remarkable efficiency and accuracy and is both useful and scalable from the point of view of industrial designers and developers. The tool is currently used by over one-thousand industrial designers &amp; developers at Huawei to improve the quality of their mobile apps.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {165–175},
numpages = {11},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}
@article{43, title={UIBert: Learning Generic Multimodal Representations for UI Understanding}, url={https://arxiv.org/abs/2107.13731}, DOI={10.48550/ARXIV.2107.13731}, abstractNote={To improve the accessibility of smart devices and to simplify their usage, building models which understand user interfaces (UIs) and assist users to complete their tasks is critical. However, unique challenges are proposed by UI-specific characteristics, such as how to effectively leverage multimodal UI features that involve image, text, and structural metadata and how to achieve good performance when high-quality labeled data is unavailable. To address such challenges we introduce UIBert, a transformer-based joint image-text model trained through novel pre-training tasks on large-scale unlabeled UI data to learn generic feature representations for a UI and its components. Our key intuition is that the heterogeneous features in a UI are self-aligned, i.e., the image and text features of UI components, are predictive of each other. We propose five pretraining tasks utilizing this self-alignment among different features of a UI component and across various components in the same UI. We evaluate our method on nine real-world downstream UI tasks where UIBert outperforms strong multimodal baselines by up to 9.26% accuracy.}, publisher={arXiv}, author={Bai, Chongyang and Zang, Xiaoxue and Xu, Ying and Sunkara, Srinivas and Rastogi, Abhinav and Chen, Jindong and Arcas, Blaise Aguera y}, year={2021} }

@inproceedings{44,
author = {Gajos, Krzysztof Z. and Wobbrock, Jacob O. and Weld, Daniel S.},
title = {Automatically Generating User Interfaces Adapted to Users' Motor and Vision Capabilities},
year = {2007},
isbn = {9781595936790},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294211.1294253},
doi = {10.1145/1294211.1294253},
abstract = {Most of today's GUIs are designed for the typical, able-bodied user; atypical users are, for the most part, left to adapt as best they can, perhaps using specialized assistive technologies as an aid. In this paper, we present an alternative approach: SUPPLE++ automatically generates interfaces which are tailored to an individual's motor capabilities and can be easily adjusted to accommodate varying vision capabilities. SUPPLE++ models users. motor capabilities based on a onetime motor performance test and uses this model in an optimization process, generating a personalized interface. A preliminary study indicates that while there is still room for improvement, SUPPLE++ allowed one user to complete tasks that she could not perform using a standard interface, while for the remaining users it resulted in an average time savings of 20%, ranging from an slowdown of 3% to a speedup of 43%.},
booktitle = {Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology},
pages = {231–240},
numpages = {10},
keywords = {supple, motor impairments, optimization, multiple impairments, vision impairments, decision theory},
location = {Newport, Rhode Island, USA},
series = {UIST '07}
}

@INPROCEEDINGS{45,  author={Bajammal, Mohammad and Mesbah, Ali},  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},   title={Semantic Web Accessibility Testing via Hierarchical Visual Analysis},   year={2021},  volume={},  number={},  pages={1610-1621},  doi={10.1109/ICSE43902.2021.00143}}

@inproceedings{46,
author = {Biagiola, Matteo and Stocco, Andrea and Ricca, Filippo and Tonella, Paolo},
title = {Diversity-Based Web Test Generation},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338970},
doi = {10.1145/3338906.3338970},
abstract = {Existing web test generators derive test paths from a navigational model of the web application, completed with either manually or randomly generated input values. However, manual test data selection is costly, while random generation often results in infeasible input sequences, which are rejected by the application under test. Random and search-based generation can achieve the desired level of model coverage only after a large number of test execution at- tempts, each slowed down by the need to interact with the browser during test execution. In this work, we present a novel web test generation algorithm that pre-selects the most promising candidate test cases based on their diversity from previously generated tests. As such, only the test cases that explore diverse behaviours of the application are considered for in-browser execution. We have implemented our approach in a tool called DIG. Our empirical evaluation on six real-world web applications shows that DIG achieves higher coverage and fault detection rates significantly earlier than crawling-based and search-based web test generators.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {142–153},
numpages = {12},
keywords = {diversity, web testing, test generation, page object},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inbook{47,
author = {Mazumder, Seshadri and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C. V.},
title = {Translating Sign Language Videos to Talking Faces},
year = {2021},
isbn = {9781450375962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490035.3490286},
abstract = {Communication with the deaf community relies profoundly on the interpretation of sign languages performed by the signers. In light of the recent breakthroughs in sign language translations, we propose a pipeline that we term "Translating Sign Language Videos to Talking Faces". In this context, we improve the existing sign language translation systems by using POS tags to improve language modeling. We further extend the challenge to develop a system that can interpret a video from a signer to an avatar speaking in spoken languages. We focus on the translation systems that attempt to translate sign languages to text without glosses, an expensive annotation form. We critically analyze two state-of-the-art architectures, and based on their limitations, we improvise the systems. We propose a two-stage approach to translate sign language into intermediate text followed by a language model to get the final predictions. Quantitative evaluations on the challenging benchmarks on RWTH-PHOENIX-Weather 2014 T show that the translation accuracy of the texts generated by our translation model improves the state-of-the-art models by approximately 3 points. We then build a working text to talking face generation pipeline by bringing together multiple existing modules. The overall pipeline is capable of generating talking face videos with speech from sign language poses. Additional materials about this project including the codes and a demo video can be found in https://seshadri-c.github.io/SLV2TF/},
booktitle = {Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {27},
numpages = {10}
}
@inproceedings{48,
author = {Race, Lauren and James, Amber and Hayward, Andrew and El-Amin, Kia and Patterson, Maya Gold and Mershon, Theresa},
title = {Designing Sensory and Social Tools for Neurodivergent Individuals in Social Media Environments},
year = {2021},
isbn = {9781450383066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3441852.3476546},
doi = {10.1145/3441852.3476546},
abstract = {Sensory guides and social narratives are learning tools that provide sensory and social support to neurodivergent individuals. These tools—and their design guidelines—have historically been developed for physical environments, such as museums and classrooms. They lack support for social media environments, where sensory stimuli and social contexts can be complex and uncertain. We address these challenges by designing a novel social media sensory guide and social narrative, specifically adapted for social media interaction. We leverage our use case, Twitter Spaces—an audio-only conversation feature in beta. The goal of this pilot study is to determine whether neurodivergent users want sensory guides and social narratives adapted for social media, and if users find them helpful in setting expectations for social media interaction. We evaluate these tools with eight neurodivergent Twitter users, using tasks and thinking aloud. Results indicate a strong potential for adoption of both tools among neurodivergent individuals to reduce overstimulation in social media environments.},
booktitle = {The 23rd International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {61},
numpages = {5},
keywords = {Accessibility, Neurodiversity, Social Media, Cognitive Disability, Design},
location = {Virtual Event, USA},
series = {ASSETS '21}
}
 @url{49, title={Web content accessibility guidelines (WCAG) 2.1}, url={https://www.w3.org/TR/WCAG21/}, journal={W3C}} 

 @url{50, title={Overview of assistive technology for autism}, url={https://www.verywellhealth.com/assistive-technology-for-autism-5076159}, journal={Verywell Health}, publisher={Verywell Health}, author={Rudy, Lisa Jo}, year={2021}, month={Feb}} 

@article{Krishnavajjala24,
  doi = {10.48550/ARXIV.2403.13690},
  url = {https://arxiv.org/abs/2403.13690},
  author = {Krishnavajjala,  Arun and Mansur,  SM Hasan and Jose,  Justin and Moran,  Kevin},
  keywords = {Software Engineering (cs.SE),  Computer Vision and Pattern Recognition (cs.CV),  Human-Computer Interaction (cs.HC),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {MotorEase: Automated Detection of Motor Impairment Accessibility Issues in Mobile App UIs},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@inproceedings{Schoop22,
  title        = {{Predicting and Explaining Mobile UI Tappability with Vision Modeling and Saliency Analysis}},
  author       = {Schoop, Eldon and Zhou, Xin and Li, Gang and Chen, Zhourong and Hartmann, Bjoern and Li, Yang},
  year         = 2022,
  booktitle    = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  location     = {<conf-loc>, <city>New Orleans</city>, <state>LA</state>, <country>USA</country>, </conf-loc>},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {CHI '22},
  doi          = {10.1145/3491102.3517497},
  isbn         = 9781450391573,
  url          = {https://doi.org/10.1145/3491102.3517497},
  abstract     = {UI designers often correct false affordances and improve the discoverability of features when users have trouble determining if elements are tappable. We contribute a novel system that models the perceived tappability of mobile UI elements with a vision-based deep neural network and helps provide design insights with dataset-level and instance-level explanations of model predictions. Our system retrieves designs from similar mobile UI examples from our dataset using the latent space of our model. We also contribute a novel use of an interpretability algorithm, XRAI, to generate a heatmap of UI elements that contribute to a given tappability prediction. Through several examples, we show how our system can help automate elements of UI usability analysis and provide insights for designers to iterate their designs. In addition, we share findings from an exploratory evaluation with professional designers to learn how AI-based tools can aid UI design and evaluation for tappability issues.},
  articleno    = 36,
  numpages     = 21,
  keywords     = {Deep Learning, Explainable AI, Interpretability, Mobile UIs}
}
@misc{li2021vut,
  title        = {{VUT: Versatile UI Transformer for Multi-Modal Multi-Task User Interface Modeling}},
  author       = {Yang Li and Gang Li and Xin Zhou and Mostafa Dehghani and Alexey Gritsenko},
  year         = 2021,
  eprint       = {2112.05692},
  archiveprefix = {arXiv},
  primaryclass = {cs.CV}
}
@misc{bai2021uibert,
  title        = {{UIBert: Learning Generic Multimodal Representations for UI Understanding}},
  author       = {Chongyang Bai and Xiaoxue Zang and Ying Xu and Srinivas Sunkara and Abhinav Rastogi and Jindong Chen and Blaise Aguera y Arcas},
  year         = 2021,
  eprint       = {2107.13731},
  archiveprefix = {arXiv},
  primaryclass = {cs.CV}
}
@misc{REPO,
  title        = {{Anonymous Repository}},
  note         = {Available at: \url{https://anonymous.4open.science/r/UIEmbedding-B4AE/README.md}}
}
