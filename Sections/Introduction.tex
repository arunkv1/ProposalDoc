
\section{Introduction}
\label{sec:introduction}

Software has become an integral part of everyone's lives and its impact continues to grow. Software takes many forms such as web applications, smartphone applications, and desktop or operating system applications, etc \cite{Szpiro16}. Our everyday lives depend on the use of critical software applications which allow us to bank, invest, get news, and communicate with others. Touch screen devices such as smartphones and tablets provide a quick and easy means of access to important information and functions within our daily lives. 


Software accessibility has become more important as more users are dependent on smartphones and computers. People of different abilities have found it difficult to use software the way it is currently developed and designed \cite{Park14}. According to the world health organization (WHO), 15\% of people have some disability \cite{WHO}, making software accessibility more important to ensure all users are able to use applications as intended.Though software engineers and companies are ethically motivated to create more accessible software, the United States Government is also making efforts to require public websites and services to be accessible \cite{ADALaws}. The government in conjunction with the American with Disabilities Act (ADA) introduced legislation which "prohibits discrimination on the basis of disability in the activities of public accommodations" \cite{ADALaws,Alshayban20}. This has lead to a 180\% increase in more accessible software as of 2018 \cite{ADAWeb}. This change in policy increases a need for more accessible software and tools that will help developers make their applications more accessible.

Software engineering research is constantly innovating how software is made and tested. Software testing and developer tools are constantly evolving and are making their way into the accessibility space. Software testing has been around for decades, but has recently grown into a more complex field with the use of computer vision and machine learning techniques to automatically generate and run tests. This exciting new way of testing could provide an ample amount of ways to test for accessibility guideline violations without developers needing to explicitly check for violations on their own. An extremely important part of software engineering research is the constant need for new, robust developer tools to assist developers in the process of designing and creating software. These tools allow developers to have quick access to information and functions that make the development process more efficient. With the evolution of machine learning techniques, research is able to tackle issues that have needed a more complex understanding of the screen and User Interface (UI) in order to detect accessibility issues accurately. 

The primary techniques that enable modern approaches for computational understanding of UIs are deep learning techniques for both computer vision and natural language. That is, researchers have begun to adapt models trained to understand open-domain image and text to the task of comprehending user interfaces. 

The crux of the above challenges is adapting \textit{generalized} models, which have been shown to effectively learn patterns across a varied range of data domains, to the specific domain of user interfaces. As such, the methods mentioned often \textit{attempt} to capture rich contextual information conveyed through the visual elements of the screen. UI's are designed as graphical interfaces with specific layouts and visual properties that capture important semantic information about the affordances of the UI. That is, the structural, visual, and lexical properties of elements on the screen provide contextual information about each screen's function. 
Another underlying challenge in learning patterns from user interfaces is the \textit{variability} in designs that convey similar semantic meanings. For example, even a screen as simple as a login screen, which is typically comprised of two text fields and a button, can be instantiated through a wide variety of different visual designs and lexicon. While current UI embedding techniques, such as VUT and UIBert aim to address these challenges through embedding UI hierarchy information, this information is often flattened into representations that make it difficult to deal with the large variety of semantically similar, yet characteristically different patterns that are found across UI designs.

In this thesis, I aim develop tools for developers to assist them in making their own software more accessible and tackle future problems which require a deeper semantic understanding of screen. This thesis revolves around using newer techniques to tackle previously unexplored problems. I propose a tool that uses computer vision techniques to identify motor-impairment accessibility issues in Android mobile apps and demonstrate its efficacy by outperforming state of the art baselines while maintaining high accuracy. Additionally, I propose a computer vision based screen understanding tool to aid in screen recognition and retrieval tasks in search tasks at a high percentage. 

%The thesis proposal is organized as follows: Section~\ref{sec:background} provides an introduction of works towards the problem of test flakiness. The main contributions of the thesis are discussed in Section~\ref{sec:thesis}. Section~\ref{sec:detectFlakyTests} provides a summary of the findings related to the detection of flaky tests. Section~\ref{sec:livingTestFlakiness} emphasizes current research on how to identify flaky \emph{failures} and explores techniques for their detection. The main key points for my future work, which form the remainder of my PhD, are discussed in Section~\ref{sec:categotize}. Lastly, the current plan for the remaining phase of my PhD is outlined in Section~\ref{sec:researchPlan}.
