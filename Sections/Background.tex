\section{Background}
\label{sec:background}


Test flakiness is a timely and relevant research problem in software testing. The first empirical study focusing on defining flaky tests was conducted by Luo et al. \cite{luo2014empirical}. They investigate that of 201 historical commits, 51 open-source projects contain logs of fixed flaky tests. They classify these flaky tests into different categories such as Async Wait and Test Order dependent \cite{luo2014empirical}.   

Flaky tests have various impacts on the process of software development at the developer, team, and organizational levels. The degree to which both awareness of flaky test results and the measurement of flaky test side-effects can greatly aid developers in making the right decision when fixing an issue. Eck et al. \cite{eck2019understanding} show that the majority of developers who participate in their study face flaky tests yearly (109 out of 120) and 58\% out of 109 deal with flaky tests on a monthly basis. Eck et al. \cite{eck2019understanding} also show that 79\% of 109 participants consider flaky tests as a moderate-to-serious problem and that 77\% believe flaky tests are a time-consuming problem as they are hard to reproduce. Developers who encounter flaky tests have different opinions regarding the reliability of the test suite. Based on the study, 77\% of developers consider the test suite to be not fully reliable if it contains at least one flaky test.


Flaky tests are hard to reproduce which make them difficult for developers to debug. Some characteristics of software environments such as operating systems (where a test suite has been executed) may play a critical role to either hide or reveal a flaky test. Out of 311 flaky tests from the dataset of flaky tests Lam et al. \cite{lam2019root} provide, 97 flaky tests could not be reproduced locally with 100 runs. This emphasizes the \emph{difficulty} of establishing a particular number of runs in order to observe flaky tests. If a flaky test has been detected after $n$-number of runs on-server, then how is a developer to determine the number of runs to observe the same flaky test in different environments? Also, this implies that most studies that limit their runs may not have observed all of the possible flaky test scenarios in their test suites. This demonstrates how observing flaky tests can be extremely challenging.


With the effort to fix flaky tests, do all flaky tests be fixed before releasing the software? There are many implications of why some of the already-detected flaky tests still exist in further software releases. First, developers may not be able to resolve the flaky test because the current resources are not sufficient enough to fix all detected flaky tests or that developers need to involve many decisions at the team or organization levels such as providing external resources. The second implication is related to how developers evalxuate their proposed fixes. For example, developers usually use the rerun technique to see if their changes eliminate flakiness in a test suite. However, the rerun technique is not guaranteed to confirm if a test is not flaky. It is also true that a test may flake due to various reasons in the future and some of them may not be observed yet. Another explanation is that developers could be aware of the side-effects of a specific flaky test and decide to keep it after developers measure the cost of fixing the flaky test. The worst implication that a developer can make is to just ignore the flaky test without any potential work toward fixing or analyzing it. Throve et al. \cite{thorve2018empirical} report that 13\% of the total number of commits that deal with flaky tests do so by just skipping them or removing the tests that cause flakiness.


\subsection{Detecting Test Flakiness}


There are significant studies proposing tools to help developers detect flaky tests without manual inspections. Each of these tools has its own mechanism to detect flaky tests. This section presents some of these effective tools. 
% \abdul{I put these works, Any others syggestions that should be included in detailss? Should I use two section like 2.1) Detecting Flaky Tests and 2.2) Detecting Flaky Failures ?}


\textbf{DeFlaker}. Bell et al. \cite{bell2018deflaker} propose \emph{DeFlaker}, a new approach to detect flaky tests from the first failure without the need to rerun. Their approach uses execution coverage to detect flaky tests. Specifically, if a test fails but does not cover any changed code, the tool labels the test as flaky.
To accomplish this, their tool consists of three main phases. First, \emph{DeFlaker} uses a syntactic diff from a version control system to determine which changes the tool needs to track. Then, \emph{DeFlaker} starts to collect coverage of each change identified from the previous phase. In the final stage, test outcomes and coverage information are analyzed to determine if a test is flaky or not. Detecting flaky tests by \emph{DeFlaker} requires collecting complete statement coverage for each line of code. Bell et al. \cite{bell2018deflaker} believe that collecting coverage for all lines of code can be expensive. In order to make their tool light-weighted, they designed the tool to collect differential coverage, which means collecting only the coverage of the changed lines instead of all lines of code. Bell et al. \cite{bell2018deflaker} also consider that syntactic change information may be insufficient and there is a need to monitor even some syntactically unchanged lines. For instance, if there are changes made to the inheritance structure of a class or method, this may have different dynamic results. Bell et al. \cite{bell2018deflaker} have evaluated their tool on 5,966 builds of 26 open-source projects. They have found 87 previously unknown flaky tests in 10 of these projects. They also show that \emph{DeFlaker} was able to detect 1,874 flaky tests from 4,846 previously known failures, with a low false alarm rate (1.5\%). Their evaluation includes a strong comparison of the rerun experiment which was a main contribution in their work. The evaluation also considers the performance (overhead of running) of their tool by showing that \emph{DeFlaker} was very fast, with an average slowdown of only 4.6\% across all of selected projects. 



% \subsection{iDFlakies}
\textbf{iDFlakies}. Lam et al. \cite{lam2019idflakies} provide a framework called \emph{iDFlakies} to detect and partially classify flaky tests. Their framework relies on rerun by reordering tests during their executions. With a time limit for rerunning certain test suites, \emph{iDFlakies} runs a test suite based on the original order and with reordering approaches. If a test passes and fails within the same order of tests, \emph{iDFlakies} labels this test as a non-order dependency (NOD) flaky test. The second case is when a test fails during reordering the tests while it passes with the original order. In this case, \emph{iDFlakies} labels this test as an order-dependency (OD) flaky test. In the process of changing the order of tests, \emph{iDFlakies} shows that NOD flaky tests can be detected in the way as they may keep their original orders e.g. reorder test \#6 with \#7 while all tests from \#1 to \#5 have the same orders and test \#3 for example passes and fails. \emph{iDFlakies} can only partially classify the flaky tests to OD and NOD and it does not have any further classification for NOD flaky tests. Lam et al. \cite{lam2019idflakies} mention that \emph{iDFlakies} does not randomly reorder tests during their executions. There are four different configurations of rerun tests in addition to rerunning based on the original order of tests. The first configuration is called random-class, where \emph{iDFlakies} runs test classes in random order but it keeps the order of methods in each class in their original order. Random-class-method is another configuration by hierarchically randomizing the order of the test classes first and then the methods within test classes without interleaving methods from different classes. The third one is called reverse-class where all classes are run in reverse orders and keep the methods in each class with their original order. The last one is called the reverse-class-method which reverses the order of all test classes and methods from the original order. The tool is configured to run up to a limit number of rounds (the amount of reruns needed), run time (how long developers can rerun each project), or based on the minimum of both. The depth-first technique is used when, for example, a number of rounds $x$ is given, \emph{iDFlakies} runs on the first module up to $x$-times before rerunning another module. Their evaluation has been based on Java projects which build with Maven and use JUnit. As a result of applying \emph{iDFlakies} on 82 different projects (111 modules in total), 422 flaky tests have been detected. In detail, 213 (50.5\%) are classified as OD flaky tests and 209 (49.5\%) as NOD flaky tests. Lam et al. \cite{lam2019idflakies} find that the random-class-method is the most effective configuration to detect OD flaky tests. In their studies, they used a time limit for rerun different ordering of configurations by 57 hours per project. 


\textbf{The Vocabulary of Flaky Tests}. Pinto et al. \cite{Pinto2020WhatIT} apply machine learning algorithms in order to find flaky test ``vocabularies" that aid in predicting new flaky tests. The approach is defined as a classification problem. This means that supervised learning algorithms are needed to have prior knowledge of some existing flaky and non flaky tests (known as a training dataset) to be able to predict the status of flakiness for unseen tests (known as a testing dataset). Collecting the vocabulary list is done by applying text analysis on the syntax of given flaky and non-flaky tests. In other words, test contents are processed through natural language processing (NLP) techniques, including identifier splitting, stemming, and stop word removal, to turn the content of each test to a list of vocabulary (called tokens). In addition to collecting tokens, this approach collects Java keywords from each test and considers the length of tests (line of codes) as extra details to help the classifier distinguish between flaky and non-flaky tests. This approach simply turns each test to a list of tokens, Java keywords, and a variable called test length. Pinto et al. \cite{Pinto2020WhatIT} aim to measure how well a flaky test classifier can learn from test contents. This approach has been evaluated based on the F1-score (measuring the accuracy of prediction performance based on the recall and precision scores) and information gain (known as the usefulness of a single token/Java keyword in terms of helping the classifier to identify a flaky test). By applying multiple supervised learning algorithms, these metrics can show the strength of flaky test predictors. This approach uses the dataset provided by Bell et al. \cite{bell2018deflaker} as a source to evaluate their approach. Pinto et al. \cite{Pinto2020WhatIT} find that their approach was effective to distinguish between flaky and non-flaky tests by achieving an F1-score of 0.95 for the identification of flaky tests. They find that the Random Forest (RF) is the most effective algorithm used toward this classification problem. They used different outcomes of processing NLP techniques as an input of the classifier. For example, they measure the effectiveness of using only Java keywords in the learning phase or using the whole set of tokens without stop words removals.


