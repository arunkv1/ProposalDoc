\newpage
\section{Thesis}
\label{sec:thesis}

\subsection{Problem Statement}
Advancements in software accessibility present new and exciting challenges to create tools for developers that transcend past the static usage of the code and metadata in applications. Existing work has made a step in the direction of making software more accessible, but has yet to leverage new, state-of-the art techniques to augment and detect software accessibility issues in applications. Most importantly, previous methods have relied on previously existing accessibility frameworks on the devices. Given the limitations of current research and the critical role that accessibility in mobile devices plays in an ever-changing and growing population of disabled users, it is important to identify comprehensive and effective solutions to address issues in software accessibility. 


\subsection{Thesis Statement}

The thesis statement is that new machine learning and computer vision-based approaches to semantic screen understanding can address the problem of software accessibility by way of detecting, locating, and augmenting UI screens. To investigate this, I perform a search of existing accessibility focused developer tools and tailor my research direction to cater to limitations in the current research. By understanding the needs in the current state of research, i discuss and propose a series of solutions which use computer vision and UI comprehension techniques. Upon evaluation, I discuss each proposed solution alongside their research questions as shown below. 


\subsubsection{Detecting Motor-Impairment Accessibility Guideline Violations}

I started my research in detecting flaky tests by conducting a study of the current state of tools that cater to software accessibility.
Following this search, I deduce that the motor-impaired community is not represented as much as low vision or deaf and hard of hearing users. Table \ref{tab:guidelines} shows the list of of motor impairment guidelines and which have been implemented in prior work. I notice that motor-impairment guideline detectors have not been implemented in past works. This leads us to believe that prior work has not attempted this due to the lack of sufficient data and computer-vision based techniques to facilitate the detection of guideline violations. 
I use this analysis to propose MotorEase. MotorEase is a novel motor-impairment guideline violation detector that uses a computer-vision approach to detect four different accessibility guideline violations within Android applications. The tool uses outputs from existing Android app testing tools to make it seamless for developers to test their applications for motor-impairment accessibility issues. This tool is evaluated on a series of manually annotated screenshots. MotorEase demonstrates a high accuracy in detecting motor-impairment guideline issues in mobile applications while leveraging the visual semantics of the screen. 
To address the challenge of detecting these guideline violations, we analyze and evaluate the results of MotorEase with these research questions: 


\begin{description}
		\item{\textbf{RQ$_1$} \textit{How accurate is the Expanding Section detector?}}
		\item{\textbf{RQ$_2$} \textit{How accurate is the Visual Touch Target detector?}}
        \item{\textbf{RQ$_3$} \textit{How accurate is the Persisting Element detector?}}
        \item{\textbf{RQ$_4$} \textit{How accurate is the UI Element Distance detector?}}
		\item{\textbf{RQ$_5$} \textit{Does MotorEase identify a limited number of false positive and negative violations?}}
\end{description}

The related findings of the detection of motor-impairment guideline violations are detailed in Section~\ref{sec:MotorEase}. The answers to all these questions are primarily summarized from the paper "MotorEase: Automated Detection of Motor Impairment Accessibility Issues in Mobile App UIs" \cite{Krishnavajjala24}. This wok was submitted ad accepted at the International Conference on Software Engineering (ICSE 2024). 




\subsubsection{Structurally motivated UI Embedding}
With the increased need for  screen understanding as shown in Sections ~\ref{sec:background, sec:introduction}, I aim to propose a UI embedding for app screens. Prior screen embeddings treat embeddings as images and not as UIs with rich layout and structural information. Some methods of screen embeddings consider the text on the screen and create text embedding for each screen. However, these methods provide a limited semantic understanding of the screen. Some approaches attempt to combine the text and image features of a UI to create a UI embedding, however, were these techniques fail to leverage the layout of the components of the screen. We take an intuitive approach of identifying structure in a UI and create FRAME, an embedding for UI screens which has a structure bias. This approach creates an embedding that can assist in screen retrieval and similarity tasks. I evaluate the embedding with these four different research questions: 

\begin{description}
  
    \item \textbf{RQ$_1$}: \textit{How does FRAME perform against other baselines in screen retrieval tasks?}
    \item \textbf{RQ$_2$}: \textit{How does FRAME perform on various types of screens against the best baseline?}
    \item \textbf{RQ$_3$}: \textit{How important is the structural embedding propagation between graphs in order to enhance the embedding?}
    \item \textbf{RQ$_4$}: \textit{How well does FRAME leverage its ability to abstract the screen and disregard styling?}

 \end{description}


The research and its findings are detailed in Section~\ref{sec:FRAME}. This section also introduces the proposed approaches for failure de-duplication and the methodology for addressing the research questions is discussed. This wok was submitted and is currently under review at the ACM Symposium on User Interface Software and Technology(UIST 2024). 


\subsubsection{UI Search}

We combine the intuition in the previous two solutions to create a comprehensive solution for UI design. Many developers begin the development process with a wireframe design of the UI. This UI design is often just an image and is easy to manipulate if needed. I am currently developing and testing SearchAccess, a UI search engine which allows developers to find similar screens to their mockup that are more accessible. However, SeachAccess not only a search engine, it is a visual environment to view accessibility issues in any given UI. This is a powerful tool that will allow developers to identify accessibility issues in their mobile app mockups and make changes inspired by similar, more accessible, UIs via search. Many times, developers tend to dismiss accessibility changes because the app is already developed and making changes is time consuming. The existence of a computer vision based approach to identify accessibility issues with only the mockup will encourage developers to make accessible decisions before code gets written for the application. To test SearchAccess, we present some initial evaluation for the tool and intended future work. We present a list of research questions to evaluate the efficacy of SearchAccess: 

\begin{description}
  
    \item \textbf{RQ$_1$}: \textit{How does SearchAccess perform in screen retrieval tasks?}
    \item \textbf{RQ$_2$}: \textit{How accurate are the detectors in SeachAccess?}
    \item \textbf{RQ$_3$}: \textit{Are developers able to identify accessibility issues within their UI designs?}
    \item \textbf{RQ$_4$}: \textit{Do developers benefit from UI search when looking to make their apps more accesible?}
 \end{description}
The discussion of these research questions, along with detailed insights on how I approach them, can be found in Section~\ref{sec:SearchAcces}.


\subsubsection{Literature Review on Developer Tools}

Given the research efforts above, it is important to understand the impact being made in the field of machine-learning based developer tools to make applications more accessible. I am currently assembling a Systemic Literature Review (SLR) which analyzes the current state of developer tools that facilitate the detection, testing, and augmentation of applications for accessibility issues. This SLR will provide a foundational understanding of the importance this research can provide. It will look at the available tools, the users they target, input/output formats, and experimental methodologies. I aim to present a comprehensive review of the research area at the intersection of machine learning, software engineering, and accessibility to establish the foundation for future work within this research area. This work will attempt to present the answers to these research questions: 

\begin{description}
  
    \item \textbf{RQ$_1$}: \textit{Is the research targeted at automating developer activities, enhancing existing software, or creating guidelines for developers?}
    \item \textbf{RQ$_2$}: \textit{What software domains does research on accessibility typically target?}
    \item \textbf{RQ$_3$}: \textit{Which populations of users with accessibility needs has software engineering targeted?}
    \item \textbf{RQ$_4$}: \textit{What type of data do studies use and how can the quality of data and its collection suggest an impact on the research in the field?}
    \item \textbf{RQ$_5$}: \textit{What are the primary means of evaluation for research that targets users with accessibility needs.?}

 \end{description}
The discussion of these research questions, along with detailed insights on how I approach them, can be found in Section~\ref{sec:SLR}.



