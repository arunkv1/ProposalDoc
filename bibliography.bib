@inproceedings{Chiou:CHI23,
author = {Chiou, Paul T. and Alotaibi, Ali S. and Halfond, William G.J.},
title = {BAGEL: An Approach to Automatically Detect Navigation-Based Web Accessibility Barriers for Keyboard Users},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580749},
doi = {10.1145/3544548.3580749},
abstract = {The Web has become an essential part of many people’s daily lives, enabling them to complete everyday and essential tasks online and access important information resources. The ability to navigate the Web via the keyboard interface is critical to people with various types of disabilities. However, modern websites often violate web accessibility guidelines for keyboard navigability. In this paper, we present a novel approach for automatically detecting web accessibility barriers that prevent or hinder keyboard users’ ability to navigate web pages. An extensive evaluation of our technique on real-world subjects showed that our technique was able to detect navigation-based keyboard accessibility barriers in web applications with high precision and recall.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {45},
numpages = {17},
keywords = {Web Accessibility, WCAG, Keyboard Navigation, Software Testing},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{wang_vet_2021,
	address = {Athens Greece},
	title = {Vet: identifying and avoiding {UI} exploration tarpits},
	isbn = {978-1-4503-8562-6},
	shorttitle = {Vet},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468554},
	doi = {10.1145/3468264.3468554},
	abstract = {Despite over a decade of research, it is still challenging for mobile UI testing tools to achieve satisfactory effectiveness, especially on industrial apps with rich features and large code bases. Our experiences suggest that existing mobile UI testing tools are prone to exploration tarpits, where the tools get stuck with a small fraction of app functionalities for an extensive amount of time. For example, a tool logs out an app at early stages without being able to log back in, and since then the tool gets stuck with exploring the app’s pre-login functionalities (i.e., exploration tarpits) instead of its main functionalities. While tool vendors/users can manually hardcode rules for the tools to avoid specific exploration tarpits, these rules can hardly generalize, being fragile in face of diverted testing environments, fast app iterations, and the demand of batch testing product lines. To identify and resolve exploration tarpits, we propose Vet, a general approach including a supporting system for the given specific Android UI testing tool on the given specific app under test (AUT). Vet runs the tool on the AUT for some time and records UI traces, based on which Vet identifies exploration tarpits by recognizing their patterns in the UI traces. Vet then pinpoints the actions (e.g., clicking logout) or the screens that lead to or exhibit exploration tarpits. In subsequent test runs, Vet guides the testing tool to prevent or recover from exploration tarpits. From our evaluation with state-of-the-art Android UI testing tools on popular industrial apps, Vet identifies exploration tarpits that cost up to 98.6\% testing time budget. These exploration tarpits reveal not only limitations in UI exploration strategies but also defects in tool implementations. Vet automatically addresses the identified exploration tarpits, enabling each evaluated tool to achieve higher code coverage and improve crash-triggering capabilities.},
	language = {en},
	urldate = {2023-02-12},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Wang, Wenyu and Yang, Wei and Xu, Tianyin and Xie, Tao},
	month = aug,
	year = {2021},
	pages = {83--94},
}

@inproceedings{mansur2023aidui,
  title={Aidui: Toward automated recognition of dark patterns in user interfaces},
  author={Mansur, SM Hasan and Salma, Sabiha and Awofisayo, Damilola and Moran, Kevin},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  pages={1958--1970},
  year={2023},
  organization={IEEE}
}

@article{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@ARTICLE{Chen22,
  author={Chen, Sen and Chen, Chunyang and Fan, Lingling and Fan, Mingming and Zhan, Xian and Liu, Yang},
  journal={IEEE Transactions on Software Engineering}, 
  title={Accessible or Not? An Empirical Investigation of Android App Accessibility}, 
  year={2022},
  volume={48},
  number={10},
  pages={3954-3968},
  doi={10.1109/TSE.2021.3108162}}
@inproceedings{Deka:2017:Rico,
 author = {Deka, Biplab and Huang, Zifeng and Franzen, Chad and Hibschman, Joshua and Afergan, Daniel and Li, Yang and Nichols, Jeffrey and Kumar, Ranjitha},
 title = {Rico: A Mobile App Dataset for Building Data-Driven Design Applications},
 booktitle = {Proceedings of the 30th Annual Symposium on User Interface Software and Technology},
 series = {UIST '17},
 year = {2017},
 keywords = {Mobile app design; design mining; design search; app
datasets},
}

https://support.google.com/accessibility/android/answer/6122836?hl=en


@techreport{kitchenham2007guidelines,
  abstract = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
  added-at = {2019-11-16T00:31:45.000+0100},
  author = {Kitchenham, Barbara Ann and Charters, Stuart},
  biburl = {https://www.bibsonomy.org/bibtex/23f4b30c0fe1435b642467af4cca120ef/jpmor},
  citeulike-article-id = {3955888},
  day = 09,
  institution = {Keele University and Durham University Joint Report},
  interhash = {aed0229656ada843d3e3f24e5e5c9eb9},
  intrahash = {3f4b30c0fe1435b642467af4cca120ef},
  keywords = {engineering evidence evidence-based literature real review software systematic},
  language = {English},
  month = {07},
  number = {EBSE 2007-001},
  posted-at = {2009-01-28 11:17:05},
  priority = {2},
  school = {Keele University},
  timestamp = {2020-10-07T13:36:50.000+0200},
  title = {Guidelines for performing Systematic Literature Reviews in Software Engineering},
  url = {https://www.elsevier.com/__data/promis_misc/525444systematicreviewsguide.pdf},
  year = 2007
}



@url{switch-access,
year = {Android Switch Access Service},
Title = {https://support.google.com/accessibility/android/answer/6122836?hl=en}}
}

@url{univ-design,
year = {Universal Design Definition and Guidelines},
Title = {https://www.section508.gov/blog/Universal-Design-What-is-it/}}
}

@url{Fdroid,
year = {F-droid},
Title = {\url{https://f-droid.org/}}
}

@url{GooglePlayStore,
year = {Google Play Store},
Title = {\url{https://play.google.com/store}}
}

@url{LabelStudio,
year = {LabelStudio},
Title = {\url{https://labelstud.io}}
}

@inproceedings{Cardenas:ICSE'20, author = {Bernal-C\'{a}rdenas, Carlos and Cooper, Nathan and Moran, Kevin and Chaparro, Oscar and Marcus, Andrian and Poshyvanyk, Denys}, title = {Translating Video Recordings of Mobile App Usages into Replayable Scenarios}, year = {2020}, isbn = {9781450371216}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3377811.3380328}, doi = {10.1145/3377811.3380328}, abstract = {Screen recordings of mobile applications are easy to obtain and capture a wealth of information pertinent to software developers (e.g., bugs or feature requests), making them a popular mechanism for crowdsourced app feedback. Thus, these videos are becoming a common artifact that developers must manage. In light of unique mobile development constraints, including swift release cycles and rapidly evolving platforms, automated techniques for analyzing all types of rich software artifacts provide benefit to mobile developers. Unfortunately, automatically analyzing screen recordings presents serious challenges, due to their graphical nature, compared to other types of (textual) artifacts. To address these challenges, this paper introduces V2S, a lightweight, automated approach for translating video recordings of Android app usages into replayable scenarios. V2S is based primarily on computer vision techniques and adapts recent solutions for object detection and image classification to detect and classify user actions captured in a video, and convert these into a replayable test scenario. We performed an extensive evaluation of V2S involving 175 videos depicting 3,534 GUI-based actions collected from users exercising features and reproducing bugs from over 80 popular Android apps. Our results illustrate that V2S can accurately replay scenarios from screen recordings, and is capable of reproducing ≈89% of our collected videos with minimal overhead. A case study with three industrial partners illustrates the potential usefulness of V2S from the viewpoint of developers.}, booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering}, pages = {309–321}, numpages = {13}, keywords = {screen recordings, bug reporting, object detection}, location = {Seoul, South Korea}, series = {ICSE '20} }

@inproceedings{Deka:UIST'17, author = {Deka, Biplab and Huang, Zifeng and Franzen, Chad and Hibschman, Joshua and Afergan, Daniel and Li, Yang and Nichols, Jeffrey and Kumar, Ranjitha}, title = {Rico: A Mobile App Dataset for Building Data-Driven Design Applications}, year = {2017}, isbn = {9781450349819}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3126594.3126651}, doi = {10.1145/3126594.3126651}, abstract = {Data-driven models help mobile app designers understand best practices and trends, and can be used to make predictions about design performance and support the creation of adaptive UIs. This paper presents Rico, the largest repository of mobile app designs to date, created to support five classes of data-driven applications: design search, UI layout generation, UI code generation, user interaction modeling, and user perception prediction. To create Rico, we built a system that combines crowdsourcing and automation to scalably mine design and interaction data from Android apps at runtime. The Rico dataset contains design data from more than 9.7k Android apps spanning 27 categories. It exposes visual, textual, structural, and interactive design properties of more than 72k unique UI screens. To demonstrate the kinds of applications that Rico enables, we present results from training an autoencoder for UI layout similarity, which supports query- by-example search over UIs.}, booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, pages = {845–854}, numpages = {10}, keywords = {app datasets, design mining, design search, mobile app design}, location = {Qu\'{e}bec City, QC, Canada}, series = {UIST '17} }


@inproceedings{zhou2017east,
  title={East: an efficient and accurate scene text detector},
  author={Zhou, Xinyu and Yao, Cong and Wen, He and Wang, Yuzhi and Zhou, Shuchang and He, Weiran and Liang, Jiajun},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={5551--5560},
  year={2017}
}

@inproceedings{Mehralian:FSE'21, author = {Mehralian, Forough and Salehnamadi, Navid and Malek, Sam}, title = {Data-Driven Accessibility Repair Revisited: On the Effectiveness of Generating Labels for Icons in Android Apps}, year = {2021}, isbn = {9781450385626}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3468264.3468604}, doi = {10.1145/3468264.3468604}, abstract = {Mobile apps are playing an increasingly important role in our daily lives, including the lives of approximately 304 million users worldwide that are either completely blind or suffer from some form of visual impairment. These users rely on screen readers to interact with apps. Screen readers, however, cannot describe the image icons that appear on the screen, unless those icons are accompanied with developer-provided textual labels. A prior study of over 5,000 Android apps found that in around 50% of the apps, less than 10% of the icons are labeled. To address this problem, a recent award-winning approach, called LabelDroid, employed deep-learning techniques to train a model on a dataset of existing icons with labels to automatically generate labels for visually similar, unlabeled icons. In this work, we empirically study the nature of icon labels in terms of distribution and their dependency on different sources of information. We then assess the effectiveness of LabelDroid in predicting labels for unlabeled icons. We find that icon images are insufficient in representing icon labels, while other sources of information from the icon usage context can enrich images in determining proper tokens for labels. We propose the first context-aware label generation approach, called COALA, that incorporates several sources of information from the icon in generating accurate labels. Our experiments show that although COALA significantly outperforms LabelDroid in both user study and automatic evaluation, further research is needed. We suggest that future studies should be more cautious when basing their approach on automatically extracted labeled data.}, booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering}, pages = {107–118}, numpages = {12}, keywords = {Alternative Text, Screen Reader, Accessibility, Deep Learning, Android}, location = {Athens, Greece}, series = {ESEC/FSE 2021} }



@Inproceedings{chen2020unblind,
author = {Chen, Jieshan and Chen, Chunyang and Xing, Zhenchang and Xu, Xiwei and Zhu, Liming and Li, Guoqiang and Wang, Jinshui},
title = {Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380327},
doi = {10.1145/3377811.3380327},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {322–334},
numpages = {13},
keywords = {accessibility, neural networks, content description, user interface, image-based buttons},
location = {Seoul, South Korea},
series = {ICSE '20}
}


@INPROCEEDINGS{Salehnamadi:ASE'22,
  author={Salehnamadi, Navid and Mehralian, Forough and Malek, Sam},
  booktitle={37th IEEE/ACM International Conference on Automated Software Engineering (ASE 2022)}, 
  title={Groundhog: An Automated Accessibility Crawler for Mobile Apps}, 
  year={2022},
  volume={},
  number={},
  pages={},
  doi={}}


@INPROCEEDINGS{Eler18,
  author={Eler, Marcelo Medeiros and Rojas, Jose Miguel and Ge, Yan and Fraser, Gordon},
  booktitle={2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)}, 
  title={Automated Accessibility Testing of Mobile Apps}, 
  year={2018},
  volume={},
  number={},
  pages={116-126},
  doi={10.1109/ICST.2018.00021}}


@inproceedings{Lin:ASE'20, author = {Lin, Jun-Wei and Salehnamadi, Navid and Malek, Sam}, title = {Test Automation in Open-Source Android Apps: A Large-Scale Empirical Study}, year = {2021}, isbn = {9781450367684}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3324884.3416623}, doi = {10.1145/3324884.3416623}, abstract = {Automated testing of mobile apps has received significant attention in recent years from researchers and practitioners alike. In this paper, we report on the largest empirical study to date, aimed at understanding the test automation culture prevalent among mobile app developers. We systematically examined more than 3.5 million repositories on GitHub and identified more than 12, 000 non-trivial and real-world Android apps. We then analyzed these non-trivial apps to investigate (1) the prevalence of adoption of test automation; (2) working habits of mobile app developers in regards to automated testing; and (3) the correlation between the adoption of test automation and the popularity of projects. Among others, we found that (1) only 8% of the mobile app development projects leverage automated testing practices; (2) developers tend to follow the same test automation practices across projects; and (3) popular projects, measured in terms of the number of contributors, stars, and forks on GitHub, are more likely to adopt test automation practices. To understand the rationale behind our observations, we further conducted a survey with 148 professional and experienced developers contributing to the subject apps. Our findings shed light on the current practices and future research directions pertaining to test automation for mobile app development.}, booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering}, pages = {1078–1089}, numpages = {12}, keywords = {automated testing, mobile apps, empirical study, Android}, location = {Virtual Event, Australia}, series = {ASE '20} }

@inproceedings{Su:FSE'17, author = {Su, Ting and Meng, Guozhu and Chen, Yuting and Wu, Ke and Yang, Weiming and Yao, Yao and Pu, Geguang and Liu, Yang and Su, Zhendong}, title = {Guided, Stochastic Model-Based GUI Testing of Android Apps}, year = {2017}, isbn = {9781450351058}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3106237.3106298}, doi = {10.1145/3106237.3106298}, abstract = {Mobile apps are ubiquitous, operate in complex environments and are developed under the time-to-market pressure. Ensuring their correctness and reliability thus becomes an important challenge. This paper introduces Stoat, a novel guided approach to perform stochastic model-based testing on Android apps. Stoat operates in two phases: (1) Given an app as input, it uses dynamic analysis enhanced by a weighted UI exploration strategy and static analysis to reverse engineer a stochastic model of the app's GUI interactions; and (2) it adapts Gibbs sampling to iteratively mutate/refine the stochastic model and guides test generation from the mutated models toward achieving high code and model coverage and exhibiting diverse sequences. During testing, system-level events are randomly injected to further enhance the testing effectiveness. Stoat was evaluated on 93 open-source apps. The results show (1) the models produced by Stoat cover 17~31% more code than those by existing modeling tools; (2) Stoat detects 3X more unique crashes than two state-of-the-art testing tools, Monkey and Sapienz. Furthermore, Stoat tested 1661 most popular Google Play apps, and detected 2110 previously unknown and unique crashes. So far, 43 developers have responded that they are investigating our reports. 20 of reported crashes have been confirmed, and 8 already fixed.}, booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering}, pages = {245–256}, numpages = {12}, keywords = {Mobile Apps, GUI Testing, Model-based Testing}, location = {Paderborn, Germany}, series = {ESEC/FSE 2017} }

@inproceedings{Gu:ICSE'19, author = {Gu, Tianxiao and Sun, Chengnian and Ma, Xiaoxing and Cao, Chun and Xu, Chang and Yao, Yuan and Zhang, Qirun and Lu, Jian and Su, Zhendong}, title = {Practical GUI Testing of Android Applications via Model Abstraction and Refinement}, year = {2019}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/ICSE.2019.00042}, doi = {10.1109/ICSE.2019.00042}, abstract = {This paper introduces a new, fully automated model-based approach for effective testing of Android apps. Different from existing model-based approaches that guide testing with a static GUI model (i.e., the model does not evolve its abstraction during testing, and is thus often imprecise), our approach dynamically optimizes the model by leveraging the runtime information during testing. This capability of model evolution significantly improves model precision, and thus dramatically enhances the testing effectiveness compared to existing approaches, which our evaluation confirms. We have realized our technique in a practical tool, Ape. On 15 large, widely-used apps from the Google Play Store, Ape outperforms the state-of-the-art Android GUI testing tools in terms of both testing coverage and the number of detected unique crashes. To further demonstrate Ape's effectiveness and usability, we conduct another evaluation of Ape on 1,316 popular apps, where it found 537 unique crashes. Out of the 38 reported crashes, 13 have been fixed and 5 have been confirmed.}, booktitle = {Proceedings of the 41st International Conference on Software Engineering}, pages = {269–280}, numpages = {12}, keywords = {GUI testing, CEGAR, mobile app testing}, location = {Montreal, Quebec, Canada}, series = {ICSE '19} }

@misc{appendix,
  author = {Anonymous},
  title = {MotorEase Anonymous Repository},
  year = {2023},
  publisher = {Anonymous 4 Open Science},
  journal = {Anonymous 4 Open Science},
  howpublished = {\url{https://anonymous.4open.science/r/MotorEease-ICSE24/README.md}}
}

@url{instaStats, title={Instagram stats: Ranking in google play, downloads; users}, url={https://www.similarweb.com/app/google-play/com.instagram.android/statistics/}, journal={Similarweb}} 

@url{AppleAccess, title={Accessibility: Apple Human Interface Guidelines}, url={https://developer.apple.com/design/human-interface-guidelines/foundations/accessibility/}} 

@url{GoogleScanner, title={Accessibility Scanner}, url={https://play.google.com/store/apps/details?id=com.google.android.apps.accessibility.auditor}} 



@inproceedings{Montague14,
author = {Montague, Kyle and Nicolau, Hugo and Hanson, Vicki L.},
title = {Motor-Impaired Touchscreen Interactions in the Wild},
year = {2014},
isbn = {9781450327206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661334.2661362},
doi = {10.1145/2661334.2661362},
abstract = {Touchscreens are pervasive in mainstream technologies; they offer novel user interfaces and exciting gestural interactions. However, to interpret and distinguish between the vast ranges of gestural inputs, the devices require users to consistently perform interactions inline with the predefined location, movement and timing parameters of the gesture recognizers. For people with variable motor abilities, particularly hand tremors, performing these input gestures can be extremely challenging and impose limitations on the possible interactions the user can make with the device. In this paper, we examine touchscreen performance and interaction behaviors of motor-impaired users on mobile devices. The primary goal of this work is to measure and understand the variance of touchscreen interaction performances by people with motor-impairments. We conducted a four-week in-the-wild user study with nine participants using a mobile touchscreen device. A Sudoku stimulus application measured their interaction performance abilities during this time. Our results show that not only does interaction performance vary significantly between users, but also that an individual's interaction abilities are significantly different between device sessions. Finally, we propose and evaluate the effect of novel tap gesture recognizers to accommodate for individual variances in touchscreen interactions.},
booktitle = {Proceedings of the 16th International ACM SIGACCESS Conference on Computers ; Accessibility},
pages = {123–130},
numpages = {8},
keywords = {touchscreen, in-the-wild, user models, motor-impaired},
location = {Rochester, New York, USA},
series = {ASSETS '14}
}
@inproceedings{Zhang13,
author = {Zhang, Xiao (Cosmo) and Fang, Kan and Francis, Gregory},
title = {Optimization of Switch Keyboards},
year = {2013},
isbn = {9781450324052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513383.2513394},
doi = {10.1145/2513383.2513394},
abstract = {Patients with motor control difficulties often "type" on a computer using a switch keyboard to guide a scanning cursor to text elements. We show how to optimize some parts of the design of switch keyboards by casting the design problem as mixed integer programming. A new algorithm to find an optimized design solution is approximately 3600 times faster than a previous algorithm, which was also susceptible to finding a non-optimal solution. The optimization requires a model of the probability of an entry error, and we show how to build such a model from experimental data. Example optimized keyboards are demonstrated.},
booktitle = {Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {60},
numpages = {2},
keywords = {"locked-in" patients, mixed integer programming, switch keyboard},
location = {Bellevue, Washington},
series = {ASSETS '13}
}

@inproceedings{Oh13,
author = {Oh, Uran and Kane, Shaun K. and Findlater, Leah},
title = {Follow That Sound: Using Sonification and Corrective Verbal Feedback to Teach Touchscreen Gestures},
year = {2013},
isbn = {9781450324052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513383.2513455},
doi = {10.1145/2513383.2513455},
abstract = {While sighted users may learn to perform touchscreen gestures through observation (e.g., of other users or video tutorials), such mechanisms are inaccessible for users with visual impairments. As a result, learning to perform gestures can be challenging. We propose and evaluate two techniques to teach touchscreen gestures to users with visual impairments: (1) corrective verbal feedback using text-to-speech and automatic analysis of the user's drawn gesture; (2) gesture sonification to generate sound based on finger touches, creating an audio representation of a gesture. To refine and evaluate the techniques, we conducted two controlled lab studies. The first study, with 12 sighted participants, compared parameters for sonifying gestures in an eyes-free scenario and identified pitch + stereo panning as the best combination. In the second study, 6 blind and low-vision participants completed gesture replication tasks with the two feedback techniques. Subjective data and preliminary performance findings indicate that the techniques offer complementary advantages.},
booktitle = {Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {13},
numpages = {8},
keywords = {sonification, blindness, touchscreen, visual impairments, gestures},
location = {Bellevue, Washington},
series = {ASSETS '13}
}


@inproceedings{Montague12,
author = {Montague, Kyle and Hanson, Vicki L. and Cobley, Andy},
title = {Designing for Individuals: Usable Touch-Screen Interaction through Shared User Models},
year = {2012},
isbn = {9781450313216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2384916.2384943},
doi = {10.1145/2384916.2384943},
abstract = {Mobile touch-screen devices are becoming increasingly popular across a diverse range of users. Whilst there is a wealth of information and utilities available via downloadable apps, there is still a large proportion of users with visual and motor impairments who are unable to use the technology fully due to their interaction needs. In this paper we present an evaluation of the use of shared user modelling and adaptive interfaces to improve the accessibility of mobile touch-screen technologies. By using abilities based information collected through application use and continually updating the user model and interface adaptations, it is easy for users to make applications aware of their needs and preferences. Three smart phone apps were created for this study and tested with 12 adults who had diverse visual and motor impairments. Results indicated significant benefits from the shared user models that can automatically adapt interfaces, across applications, to address usability needs.},
booktitle = {Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {151–158},
numpages = {8},
keywords = {mobile touch screens, adaptive interfaces, shared user modelling},
location = {Boulder, Colorado, USA},
series = {ASSETS '12}
}

@inproceedings{Szpiro16,
author = {Szpiro, Sarit Felicia Anais and Hashash, Shafeka and Zhao, Yuhang and Azenkot, Shiri},
title = {How People with Low Vision Access Computing Devices: Understanding Challenges and Opportunities},
year = {2016},
isbn = {9781450341240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2982142.2982168},
doi = {10.1145/2982142.2982168},
abstract = {Low vision is a pervasive condition in which people have difficulty seeing even with corrective lenses. People with low vision frequently use mainstream computing devices, however how they use their devices to access information and whether digital low vision accessibility tools provide adequate support remains understudied. We addressed these questions with a contextual inquiry study. We observed 11 low vision participants using their smartphones, tablets, and computers when performing simple tasks such as reading email. We found that participants preferred accessing information visually than aurally (e.g., screen readers), and juggled a variety of accessibility tools. However, accessibility tools did not provide them with appropriate support. Moreover, participants had to constantly perform multiple gestures in order to see content comfortably. These challenges made participants inefficient-they were slow and often made mistakes; even tech savvy participants felt frustrated and not in control. Our findings reveal the unique needs of low vision people, which differ from those of people with no vision and design opportunities for improving low vision accessibility tools.},
booktitle = {Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {171–180},
numpages = {10},
keywords = {computing devices, contextual inquiry, low vision, accessibility},
location = {Reno, Nevada, USA},
series = {ASSETS '16}
}

@inproceedings{Norman13,
author = {Norman, Kirk and Arber, Yevgeniy and Kuber, Ravi},
title = {How Accessible is the Process of Web Interface Design?},
year = {2013},
isbn = {9781450324052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513383.2513385},
doi = {10.1145/2513383.2513385},
abstract = {This paper describes a data gathering study, examining the experiences and day-to-day challenges faced by blind web interface developers when designing sites and online applications. Findings have revealed that considerable amounts of time and cognitive effort can be spent checking code in text editing software and examining the content presented via the web browser. Participants highlighted the burden experienced from committing large sections of code to memory, and the restrictions associated with assistive technologies when performing collaborative tasks with sighted developers and clients. Our future work aims to focus on the development of a multimodal web editing and browsing solution, designed to support both blind and sighted parties during the design process.},
booktitle = {Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {51},
numpages = {2},
keywords = {HTML, accessibility, blind, web development, screen reader},
location = {Bellevue, Washington},
series = {ASSETS '13}
}

@inproceedings{Salehnamadi21,
author = {Salehnamadi, Navid and Alshayban, Abdulaziz and Lin, Jun-Wei and Ahmed, Iftekhar and Branham, Stacy and Malek, Sam},
title = {Latte: Use-Case and Assistive-Service Driven Automated Accessibility Testing Framework for Android},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445455},
doi = {10.1145/3411764.3445455},
abstract = { For 15% of the world population with disabilities, accessibility is arguably the most critical software quality attribute. The ever-growing reliance of users with disability on mobile apps further underscores the need for accessible software in this domain. Existing automated accessibility assessment techniques primarily aim to detect violations of predefined guidelines, thereby produce a massive amount of accessibility warnings that often overlook the way software is actually used by users with disability. This paper presents a novel, high-fidelity form of accessibility testing for Android apps, called Latte, that automatically reuses tests written to evaluate an app’s functional correctness to assess its accessibility as well. Latte first extracts the use case corresponding to each test, and then executes each use case in the way disabled users would, i.e., using assistive services. Our empirical evaluation on real-world Android apps demonstrates Latte’s effectiveness in detecting substantially more useful defects than prior techniques.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {274},
numpages = {11},
keywords = {Accessibility, Automated Testing, Mobile Application},
location = {Yokohama, Japan},
series = {CHI '21}
}
@url{TalkBack,
year = {2019},
Title = {Google TalkBack source code \url{https://github.com/google/talkback}}}

@inproceedings{Wu21,
author = {Wu, Jason and Zhang, Xiaoyi and Nichols, Jeff and Bigham, Jeffrey P},
title = {Screen Parsing: Towards Reverse Engineering of UI Models from Screenshots},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474763},
doi = {10.1145/3472749.3474763},
abstract = { Automated understanding of user interfaces (UIs) from their pixels can improve accessibility, enable task automation, and facilitate interface design without relying on developers to comprehensively provide metadata. A first step is to infer what UI elements exist on a screen, but current approaches are limited in how they infer how those elements are semantically grouped into structured interface definitions. In this paper, we motivate the problem of screen parsing, the task of predicting UI elements and their relationships from a screenshot. We describe our implementation of screen parsing and provide an effective training procedure that optimizes its performance. In an evaluation comparing the accuracy of the generated output, we find that our implementation significantly outperforms current systems (up to 23%). Finally, we show three example applications that are facilitated by screen parsing: (i) UI similarity search, (ii) accessibility enhancement, and (iii) code generation from UI screenshots.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {470–483},
numpages = {14},
keywords = {user interface modeling, hierarchy prediction, ui semantics},
location = {Virtual Event, USA},
series = {UIST '21}
}


@inproceedings{Zhang21,
author = {Zhang, Xiaoyi and de Greef, Lilian and Swearngin, Amanda and White, Samuel and Murray, Kyle and Yu, Lisa and Shan, Qi and Nichols, Jeffrey and Wu, Jason and Fleizach, Chris and Everitt, Aaron and Bigham, Jeffrey P},
title = {Screen Recognition: Creating Accessibility Metadata for Mobile Applications from Pixels},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445186},
doi = {10.1145/3411764.3445186},
abstract = { Many accessibility features available on mobile platforms require applications (apps) to provide complete and accurate metadata describing user interface (UI) components. Unfortunately, many apps do not provide sufficient metadata for accessibility features to work as expected. In this paper, we explore inferring accessibility metadata for mobile apps from their pixels, as the visual interfaces often best reflect an app’s full functionality. We trained a robust, fast, memory-efficient, on-device model to detect UI elements using a dataset of 77,637 screens (from 4,068 iPhone apps) that we collected and annotated. To further improve UI detections and add semantic information, we introduced heuristics (e.g., UI grouping and ordering) and additional models (e.g., recognize UI content, state, interactivity). We built Screen Recognition to generate accessibility metadata to augment iOS VoiceOver. In a study with 9 screen reader users, we validated that our approach improves the accessibility of existing mobile apps, enabling even previously inaccessible apps to be used. },
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {275},
numpages = {15},
keywords = {accessibility enhancement, ui detection, mobile accessibility},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{Park14,
author = {Park, Kyudong and Goh, Taedong and So, Hyo-Jeong},
title = {Toward Accessible Mobile Application Design: Developing Mobile Application Accessibility Guidelines for People with Visual Impairment},
year = {2014},
isbn = {9788968487521},
publisher = {Hanbit Media, Inc.},
address = {Seoul, KOR},
abstract = {While the use of Smartphones has improved the life of people with disabilities, several mobile content and applications remain inaccessible to people with visual impairment. Toward the overarching goal of accessible mobile application design, this two-phased study attempts to develop mobile application accessibility guidelines for people with visual impairment. First, we investigated how people with visual impairment use mobile phones. Four participants with visual impairment performed specified tasks. Their usage patterns and follow-up interviews were analyzed. Serious accessibility problems were found in both typing and VoiceOver functions. Second, we evaluated and developed systematic guidelines and standards of designing accessible mobile applications through a heuristic walkthrough method. Four experts with extensive experiences and knowledge about mobile application development/design used the VoiceOver function with iPhone for 5 days, and then walked through thought-provoking tasks. In conclusion, we propose a set of 10 heuristics for developing accessible mobile applications and suggest a critical need for internationally-agreed guidelines and standards to improve the current mobile environment for people with disabilities.},
booktitle = {Proceedings of HCI Korea},
pages = {31–38},
numpages = {8},
keywords = {evaluation, heuristic walkthrough, design guideline, standardization, VoiceOver},
location = {Seoul, Republic of Korea},
series = {HCIK '15}
}

@inproceedings{Chen20,
author = {Chen, Jieshan and Chen, Chunyang and Xing, Zhenchang and Xu, Xiwei and Zhu, Liming and Li, Guoqiang and Wang, Jinshui},
title = {Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380327},
doi = {10.1145/3377811.3380327},
abstract = {According to the World Health Organization(WHO), it is estimated that approximately 1.3 billion people live with some forms of vision impairment globally, of whom 36 million are blind. Due to their disability, engaging these minority into the society is a challenging problem. The recent rise of smart mobile phones provides a new solution by enabling blind users' convenient access to the information and service for understanding the world. Users with vision impairment can adopt the screen reader embedded in the mobile operating systems to read the content of each screen within the app, and use gestures to interact with the phone. However, the prerequisite of using screen readers is that developers have to add natural-language labels to the image-based components when they are developing the app. Unfortunately, more than 77% apps have issues of missing labels, according to our analysis of 10,408 Android apps. Most of these issues are caused by developers' lack of awareness and knowledge in considering the minority. And even if developers want to add the labels to UI components, they may not come up with concise and clear description as most of them are of no visual issues. To overcome these challenges, we develop a deep-learning based model, called LabelDroid, to automatically predict the labels of image-based buttons by learning from large-scale commercial apps in Google Play. The experimental results show that our model can make accurate predictions and the generated labels are of higher quality than that from real Android developers.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {322–334},
numpages = {13},
keywords = {accessibility, image-based buttons, content description, user interface, neural networks},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{Peng19,
author = {Peng, Yi-Hao and Lin, Muh-Tarng and Chen, Yi and Chen, TzuChuan and Ku, Pin Sung and Taele, Paul and Lim, Chin Guan and Chen, Mike Y.},
title = {PersonalTouch: Improving Touchscreen Usability by Personalizing Accessibility Settings Based on Individual User's Touchscreen Interaction},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300913},
doi = {10.1145/3290605.3300913},
abstract = {Modern touchscreen devices have recently introduced customizable touchscreen settings to improve accessibility for users with motor impairments. For example, iOS 10 introduced the following four Touch Accommodation settings: 1) Hold Duration, 2) Ignore Repeat, 3) Tap Assistance, and 4) Tap Assistance Gesture Delay. These four independent settings lead to a total of more than 1 million possible configurations, making it impractical to manually determine the optimal settings. We present PersonalTouch, which collects and analyzes touchscreen gestures performed by individual users, and recommends personalized, optimal touchscreen accessibility settings. Results from our user study show that PersonalTouch significantly improves touch input success rate for users with motor impairments (20.2%, N=12, p=.00054) and for users without motor impairments (1.28%, N=12, p=.032).},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {motor impairment, accessibility, touch-screen interaction, personalization},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{Zhang18,
author = {Zhang, Xiaoyi and Ross, Anne Spencer and Fogarty, James},
title = {Robust Annotation of Mobile Application Interfaces in Methods for Accessibility Repair and Enhancement},
year = {2018},
isbn = {9781450359481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242587.3242616},
doi = {10.1145/3242587.3242616},
abstract = {Accessibility issues in mobile apps make those apps difficult or impossible to access for many people. Examples include elements that fail to provide alternative text for a screen reader, navigation orders that are difficult, or custom widgets that leave key functionality inaccessible. Social annotation techniques have demonstrated compelling approaches to such accessibility concerns in the web, but have been difficult to apply in mobile apps because of the challenges of robustly annotating interfaces. This research develops methods for robust annotation of mobile app interface elements. Designed for use in runtime interface modification, our methods are based in screen identifiers, element identifiers, and screen equivalence heuristics. We implement initial developer tools for annotating mobile app accessibility metadata, evaluate our current screen equivalence heuristics in a dataset of 2038 screens collected from 50 mobile apps, present three case studies implementing runtime repair of common accessibility issues, and examine repair of real-world accessibility issues in 26 apps. These contributions overall demonstrate strong opportunities for social annotation in mobile accessibility.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {609–621},
numpages = {13},
keywords = {runtime modification, accessibility, robust annotation},
location = {Berlin, Germany},
series = {UIST '18}
}

@inproceedings{Alshayban20,
author = {Alshayban, Abdulaziz and Ahmed, Iftekhar and Malek, Sam},
title = {Accessibility Issues in Android Apps: State of Affairs, Sentiments, and Ways Forward},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380392},
doi = {10.1145/3377811.3380392},
abstract = {Mobile apps are an integral component of our daily life. Ability to use mobile apps is important for everyone, but arguably even more so for approximately 15% of the world population with disabilities. This paper presents the results of a large-scale empirical study aimed at understanding accessibility of Android apps from three complementary perspectives. First, we analyze the prevalence of accessibility issues in over 1, 000 Android apps. We find that almost all apps are riddled with accessibility issues, hindering their use by disabled people. We then investigate the developer sentiments through a survey aimed at understanding the root causes of so many accessibility issues. We find that in large part developers are unaware of accessibility design principles and analysis tools, and the organizations in which they are employed do not place a premium on accessibility. We finally investigate user ratings and comments on app stores. We find that due to the disproportionately small number of users with disabilities, user ratings and app popularity are not indicative of the extent of accessibility issues in apps. We conclude the paper with several observations that form the foundation for future research and development.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1323–1334},
numpages = {12},
location = {Seoul, South Korea},
series = {ICSE '20}
}


@INPROCEEDINGS{Debora21,
  abstract = {Guidelines, techniques, and methods have been presented in the literature in recent years to contribute to the development of accessible software and to promote digital inclusion. Considering that software product quality depends on the quality of the development process, researchers have investigated how to include accessibility during the software development process in order to obtain accessible software. Two Systematic Literature Reviews (SLR) have been conducted in the past to identify such research initiatives. This paper presents a new SLR, considering the period from 2011 to 2019. The review of 94 primary studies showed the distribution of publications on different phases of the software life cycle, mainly the design and testing phases. The study also identified, for the first time, papers about accessibility and software process establishment. This result reinforces that, in fact, accessibility is not characterized as a property of the final software only. Instead, it evolves over the software life cycle. Besides, this study aims to provide designers and developers with an updated view of methods, tools, and other assets that contribute to process enrichment, valuing accessibility, as well as shows the gaps and challenges which deserve to be investigated.},
  author = {D{\'e}bora Maria Barroso Paiva and Andr{\'e} Pimenta Freire and Renata Pontin {de Mattos Fortes}},
  doi = {https://doi.org/10.1016/j.jss.2020.110819},
  issn = {0164-1212},
  journal = {Journal of Systems and Software},
  keywords = {Accessibility, Software Engineering, Systematic Literature Review, Design for disabilities, Methods for accessibility},
  pages = {110819},
  title = {Accessibility and Software Engineering Processes: A Systematic Literature Review},
  url = {https://www.sciencedirect.com/science/article/pii/S0164121220302168},
  volume = {171},
  year = {2021}
  Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2020.110819}}

@url{ADALaws,
  year = {2019},
  Title = {ADAlaws \url{https://www.ada.gov/cguide.htm}}}

@url{CurrentState,
year = {2011},
Title = {The Current State of Cell Phone Accessibility \url{https://www.afb.org/aw/12/6/15926}}}

@url{HarvardAccess,
year = {2022},
Title = {Motor Impairment \url{https://accessibility.huit.harvard.edu/disabilities/motor-impairment?page=1}}}


@url{GoogleAccess,
year = {2022},
Title = {Accessibility \url{https://developer.android.com/guide/topics/ui/accessibility}}}

@url{voiceover,
year = {2019},
Title = {VoiceOver \url{https://cloud.google.com/translate/docs/}}}

@inproceedings{Ross18,
  doi = {10.1145/3234695.3236364},
  url = {https://doi.org/10.1145/3234695.3236364},
  year = {2018},
  month = oct,
  publisher = {{ACM}},
  author = {Anne Spencer Ross and Xiaoyi Zhang and James Fogarty and Jacob O. Wobbrock},
  title = {Examining Image-Based Button Labeling for Accessibility in Android Apps through Large-Scale Analysis},
  booktitle = {Proceedings of the 20th International {ACM} {SIGACCESS} Conference on Computers and Accessibility}
}


@url{ADAWeb,
year = {2018},
Title = {ADA Web Accessibility Lawsuit Recap Report \url{https://blog.usablenet.com/2018- ada- web- accessibility- lawsuit- recap- report}}}
 


 @url{MsftAccess,
 year = {2022},
 Title = {Accessibility Techonology and Tools | Microsoft Accessibility. Promoting disability inclusion \url{<https://www.microsoft.com/en-us/accessibility}}
 }

@url{AccessGov,
 year = {2022},
 Title = {Accessibility Guide \url{https://accessibility.18f.gov/checklist/accessibility}}
 }


@url{WHO,
year = {2011},
Title = {World Report on Disability \url{http://www.who.int/disabilities/world_report/2011/en/}}}

@url{IOSDesign,
year = {2019},
Title = {Adaptivity and layout - visual design - IOS - human interface guidelines - apple developer \url{https://developer.apple.com/design/human-interface-guidelines/ios/visual-design/adaptivity-and-layout/}}}

@url{ANDRDesign,
year = {2019},
Title = {“Design for Android: android developers,” \url{https://developer.android.com/design}}}

@inproceedings{MacKenzie11,
author = {MacKenzie, I. Scott and Soukoreff, R. William and Helga, Joanna},
title = {1 Thumb, 4 Buttons, 20 Words per Minute: Design and Evaluation of H4-Writer},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047258},
doi = {10.1145/2047196.2047258},
abstract = {We present what we believe is the most efficient and quickest four-key text entry method available. H4-Writer uses Huffman coding to assign minimized key sequences to letters, with full access to error correction, punctuation, digits, modes, etc. The key sequences are learned quickly, and support eyes-free entry. With KSPC = 2.321, the effort to enter text is comparable to multitap on a mobile phone keypad; yet multitap requires nine keys. In a longitudinal study with six participants, an average text entry speed of 20.4 wpm was observed in the 10th session. Error rates were under 1%. To improve external validity, an extended session was included that required input of punctuation and other symbols. Entry speed dropped only by about 3 wpm, suggesting participants quickly leveraged their acquired skill with H4-Writer to access advanced features.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {471–480},
numpages = {10},
keywords = {mobile text entry, small devices, text entry, Huffman coding},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}
}
@inproceedings{Moran18,
author = {Moran, Kevin and Li, Boyang and Bernal-C\'{a}rdenas, Carlos and Jelf, Dan and Poshyvanyk, Denys},
title = {Automated Reporting of GUI Design Violations for Mobile Apps},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180246},
doi = {10.1145/3180155.3180246},
abstract = {The inception of a mobile app often takes form of a mock-up of the Graphical User Interface (GUI), represented as a static image delineating the proper layout and style of GUI widgets that satisfy requirements. Following this initial mock-up, the design artifacts are then handed off to developers whose goal is to accurately implement these GUIs and the desired functionality in code. Given the sizable abstraction gap between mock-ups and code, developers often introduce mistakes related to the GUI that can negatively impact an app's success in highly competitive marketplaces. Moreover, such mistakes are common in the evolutionary context of rapidly changing apps. This leads to the time-consuming and laborious task of design teams verifying that each screen of an app was implemented according to intended design specifications.This paper introduces a novel, automated approach for verifying whether the GUI of a mobile app was implemented according to its intended design. Our approach resolves GUI-related information from both implemented apps and mock-ups and uses computer vision techniques to identify common errors in the implementations of mobile GUIs. We implemented this approach for Android in a tool called Gvt and carried out both a controlled empirical evaluation with open-source apps as well as an industrial evaluation with designers and developers from Huawei. The results show that Gvt solves an important, difficult, and highly practical problem with remarkable efficiency and accuracy and is both useful and scalable from the point of view of industrial designers and developers. The tool is currently used by over one-thousand industrial designers amp; developers at Huawei to improve the quality of their mobile apps.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {165–175},
numpages = {11},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{Nunes15,
  doi = {10.1007/s10209-015-0440-1},
  url = {https://doi.org/10.1007/s10209-015-0440-1},
  year = {2015},
  month = oct,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {15},
  number = {4},
  pages = {659--679},
  author = {Francisco Nunes and Paula Alexandra Silva and Jo{\~{a}}o Cevada and Ana Correia Barros and Lu{\'{\i}}s Teixeira},
  title = {User interface design guidelines for smartphone applications for people with Parkinson's disease},
  journal = {Universal Access in the Information Society}
}

@inproceedings{Eler18,
  doi = {10.1109/icst.2018.00021},
  url = {https://doi.org/10.1109/icst.2018.00021},
  year = {2018},
  month = apr,
  publisher = {{IEEE}},
  author = {Marcelo Medeiros Eler and Jose Miguel Rojas and Yan Ge and Gordon Fraser},
  title = {Automated Accessibility Testing of Mobile Apps},
  booktitle = {2018 {IEEE} 11th International Conference on Software Testing,  Verification and Validation ({ICST})}
}

@inproceedings{glove,
  author       = {Jeffrey Pennington and
                  Richard Socher and
                  Christopher D. Manning},
  editor       = {Alessandro Moschitti and
                  Bo Pang and
                  Walter Daelemans},
  title        = {Glove: Global Vectors for Word Representation},
  booktitle    = {Proceedings of the 2014 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2014, October 25-29, 2014, Doha, Qatar,
                  {A} meeting of SIGDAT, a Special Interest Group of the {ACL}},
  pages        = {1532--1543},
  publisher    = {{ACL}},
  year         = {2014},
  url          = {https://doi.org/10.3115/v1/d14-1162},
  doi          = {10.3115/v1/d14-1162},
  timestamp    = {Fri, 06 Aug 2021 00:40:40 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/PenningtonSM14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Kane11,
  doi = {10.1145/2047196.2047232},
  url = {https://doi.org/10.1145/2047196.2047232},
  year = {2011},
  month = oct,
  publisher = {{ACM}},
  author = {Shaun K. Kane and Meredith Ringel Morris and Annuska Z. Perkins and Daniel Wigdor and Richard E. Ladner and Jacob O. Wobbrock},
  title = {Access overlays},
  booktitle = {Proceedings of the 24th annual {ACM} symposium on User interface software and technology}
}

@inproceedings{Abascal11,
author = {Abascal, Julio and Aizpurua, Amaia and Cearreta, Idoia and Gamecho, Borja and Garay-Vitoria, Nestor and Mi\~{n}\'{o}n, Ra\'{u}l},
title = {Automatically Generating Tailored Accessible User Interfaces for Ubiquitous Services},
year = {2011},
isbn = {9781450309202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2049536.2049570},
doi = {10.1145/2049536.2049570},
abstract = {Ambient Assisted Living environments provide support to people with disabilities and elderly people, usually at home. This concept can be extended to public spaces, where ubiquitous accessible services allow people with disabilities to access intelligent machines such as information kiosks. One of the key issues in achieving full accessibility is the instantaneous generation of an adapted accessible interface suited to the specific user that requests the service. In this paper we present the method used by the EGOKI interface generator to select the most suitable interaction resources and modalities for each user in the automatic creation of the interface. The validation of the interfaces generated for four different types of users is presented and discussed.},
booktitle = {The Proceedings of the 13th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {187–194},
numpages = {8},
keywords = {ubiquitous computing, automatic user interface generation, adaptive systems, accessible user interfaces},
location = {Dundee, Scotland, UK},
series = {ASSETS '11}
}


@inproceedings{Gajos07,
author = {Gajos, Krzysztof Z. and Wobbrock, Jacob O. and Weld, Daniel S.},
title = {Automatically Generating User Interfaces Adapted to Users' Motor and Vision Capabilities},
year = {2007},
isbn = {9781595936790},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294211.1294253},
doi = {10.1145/1294211.1294253},
abstract = {Most of today's GUIs are designed for the typical, able-bodied user; atypical users are, for the most part, left to adapt as best they can, perhaps using specialized assistive technologies as an aid. In this paper, we present an alternative approach: SUPPLE++ automatically generates interfaces which are tailored to an individual's motor capabilities and can be easily adjusted to accommodate varying vision capabilities. SUPPLE++ models users. motor capabilities based on a onetime motor performance test and uses this model in an optimization process, generating a personalized interface. A preliminary study indicates that while there is still room for improvement, SUPPLE++ allowed one user to complete tasks that she could not perform using a standard interface, while for the remaining users it resulted in an average time savings of 20%, ranging from an slowdown of 3% to a speedup of 43%.},
booktitle = {Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology},
pages = {231–240},
numpages = {10},
keywords = {supple, motor impairments, optimization, multiple impairments, vision impairments, decision theory},
location = {Newport, Rhode Island, USA},
series = {UIST '07}
}

 @url{WebGuide, 
 year = {2022},
 title={Web content accessibility guidelines (WCAG)}, url={https://www.w3.org/TR/WCAG21/}, journal={W3C}} 


@INPROCEEDINGS{Moran:ICST'16,
  author={Moran, Kevin and Linares-Vásquez, Mario and Bernal-Cárdenas, Carlos and Vendome, Christopher and Poshyvanyk, Denys},
  booktitle={2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
  title={Automatically Discovering, Reporting and Reproducing Android Application Crashes}, 
  year={2016},
  volume={},
  number={},
  pages={33-44},
  doi={10.1109/ICST.2016.34}}


@article{crashscope,
  doi = {10.48550/ARXIV.1801.06428},
  url = {https://arxiv.org/abs/1801.06428},
  author = {Moran,  Kevin and Linares-Vasquez,  Mario and Bernal-Cardenas,  Carlos and Vendome,  Christopher and Poshyvanyk,  Denys},
  keywords = {Software Engineering (cs.SE),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {CrashScope: A Practical Tool for Automated Testing of Android Applications},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}
@inproceedings{Parhi06,
author = {Parhi, Pekka and Karlson, Amy K. and Bederson, Benjamin B.},
title = {Target Size Study for One-Handed Thumb Use on Small Touchscreen Devices},
year = {2006},
isbn = {1595933905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1152215.1152260},
doi = {10.1145/1152215.1152260},
abstract = {This paper describes a two-phase study conducted to determine optimal target sizes for one-handed thumb use of mobile handheld devices equipped with a touch-sensitive screen. Similar studies have provided recommendations for target sizes when using a mobile device with two hands plus a stylus, and interacting with a desktop-sized display with an index finger, but never for thumbs when holding a small device in a single hand. The first phase explored the required target size for single-target (discrete) pointing tasks, such as activating buttons, radio buttons or checkboxes. The second phase investigated optimal sizes for widgets used for tasks that involve a sequence of taps (serial), such as text entry. Since holding a device in one hand constrains thumb movement, we varied target positions to determine if performance depended on screen location. The results showed that while speed generally improved as targets grew, there were no significant differences in error rate between target sizes =9.6 mm in discrete tasks and targets =7.7 mm in serial tasks. Along with subjective ratings and the findings on hit response variability, we found that target size of 9.2 mm for discrete tasks and targets of 9.6 mm for serial tasks should be sufficiently large for one-handed thumb use on touchscreen-based handhelds without degrading performance and preference.},
booktitle = {Proceedings of the 8th Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {203–210},
numpages = {8},
keywords = {key size, keypads, mobile devices, one-handed, touch screens},
location = {Helsinki, Finland},
series = {MobileHCI '06}
}

@inproceedings{Kong21,
author = {Kong, Junhan and Zhong, Mingyuan and Fogarty, James and Wobbrock, Jacob O.},
title = {New Metrics for Understanding Touch by People with and without Limited Fine Motor Function},
year = {2021},
isbn = {9781450383066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3441852.3476559},
doi = {10.1145/3441852.3476559},
abstract = {Current performance measures with touch-based systems usually focus on overall performance, such as touch accuracy and target acquisition speed. But a touch is not an atomic event; it is a process that unfolds over time, and this process can be characterized to gain insight into users’ touch behaviors. To this end, our work proposes 13 target-agnostic touch performance metrics to characterize what happens during a touch. These metrics are: touch direction, variability, drift, duration, extent, absolute/signed area change, area variability, area deviation, absolute/signed angle change, angle variability, and angle deviation. Unlike traditional touch performance measures that treat a touch as a single (x, y) coordinate, we regard a touch as a time series of ovals that occur from finger-down to finger-up. We provide a mathematical formula and intuitive description for each metric we propose. To evaluate our metrics, we run an analysis on a publicly available dataset containing touch inputs by people with and without limited fine motor function, finding our metrics helpful in characterizing different fine motor control challenges. Our metrics can be useful to designers and evaluators of touch-based systems, particularly when making touch screens accessible to all forms of touch.},
booktitle = {Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {80},
numpages = {4},
keywords = {limited fine motor function, Understanding touch},
location = {Virtual Event, USA},
series = {ASSETS '21}
}

@misc{UIED,
  author={MulongXie},
  title={UIED - UI element detection, detecting UI elements from UI screenshots or drawnings},
  year={2021},
  url={https://github.com/MulongXie/UIED},
}


 @misc{ocr, title={Detect text in images|cloud vision API|google cloud}, url={https://cloud.google.com/vision/docs/ocr}, journal={Google}, publisher={Google}} 

@inproceedings{FlrezAristizbal19,
  doi = {10.1145/3290605.3300240},
  url = {https://doi.org/10.1145/3290605.3300240},
  year = {2019},
  month = may,
  publisher = {{ACM}},
  author = {Leandro Fl{\'{o}}rez-Aristiz{\'{a}}bal and Sandra Cano and C{\'{e}}sar A. Collazos and Andr{\'{e}}s F. Solano and Stephen Brewster},
  title = {{DesignABILITY}},
  booktitle = {Proceedings of the 2019 {CHI} Conference on Human Factors in Computing Systems}
}

@inproceedings{Milne18,
  doi = {10.1145/3173574.3173643},
  url = {https://doi.org/10.1145/3173574.3173643},
  year = {2018},
  month = apr,
  publisher = {{ACM}},
  author = {Lauren R. Milne and Richard E. Ladner},
  title = {Blocks4All},
  booktitle = {CHI'18}
}


@inproceedings{Li21,
  doi = {10.1145/3411764.3445520},
  url = {https://doi.org/10.1145/3411764.3445520},
  year = {2021},
  month = may,
  publisher = {{ACM}},
  author = {Junchen Li and Garreth W. Tigwell and Kristen Shinohara},
  title = {Accessibility of High-Fidelity Prototyping Tools},
  booktitle = {Proceedings of the 2021 {CHI} Conference on Human Factors in Computing Systems}
}

@inproceedings{Pavel20,
  doi = {10.1145/3379337.3415864},
  url = {https://doi.org/10.1145/3379337.3415864},
  year = {2020},
  month = oct,
  publisher = {{ACM}},
  author = {Amy Pavel and Gabriel Reyes and Jeffrey P. Bigham},
  title = {Rescribe},
  booktitle = {Proceedings of the 33rd Annual {ACM} Symposium on User Interface Software and Technology}
}

@inproceedings{Chiou21,
author = {Chiou, Paul T. and Alotaibi, Ali S. and Halfond, William G. J.},
title = {Detecting and Localizing Keyboard Accessibility Failures in Web Applications},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468581},
doi = {10.1145/3468264.3468581},
abstract = {The keyboard is the most universally supported input method operable by people with disabilities. Yet, many popular websites lack keyboard accessible mechanism, which could cause failures that make the website unusable. In this paper, we present a novel approach for automatically detecting and localizing keyboard accessibility failures in web applications. Our extensive evaluation of our technique on real world web pages showed that our technique was able to detect keyboard failures in web applications with high precision and recall and was able to accurately identify the underlying elements in the web pages that led to the observed problems.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {855–867},
numpages = {13},
keywords = {Web Accessibility, Software Testing, WCAG, Keyboard Navigation},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}


@inproceedings{Astler11,
author = {Astler, Douglas and Chau, Harrison and Hsu, Kailin and Hua, Alvin and Kannan, Andrew and Lei, Lydia and Nathanson, Melissa and Paryavi, Esmaeel and Rosen, Michelle and Unno, Hayato and Wang, Carol and Zaidi, Khadija and Zhang, Xuemin and Tang, Cha-Min},
title = {Increased Accessibility to Nonverbal Communication through Facial and Expression Recognition Technologies for Blind/Visually Impaired Subjects},
year = {2011},
isbn = {9781450309202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2049536.2049596},
doi = {10.1145/2049536.2049596},
abstract = {Conversation between two individuals requires verbal dialogue; the majority of human communication however consists of non-verbal cues such as gestures and facial expressions. Blind individuals are thus hindered in their interaction capabilities. To address this, we are building a computer vision system with facial recognition and expression algorithms to relay nonverbal messages to a blind user. The device will communicate the identities and facial expressions of communication partners in realtime. In order to ensure that this device will be useful to the blind community, we conducted surveys and interviews and we are working with subjects to test prototypes of the device. This paper describes the algorithms and design concepts incorporated in this device, and it provides a commentary on early survey and interview results. A corresponding poster with demonstration stills is exhibited at this conference.},
booktitle = {The Proceedings of the 13th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {259–260},
numpages = {2},
keywords = {blindness, face recognition, computer vision, assistive technologies, expression recognition},
location = {Dundee, Scotland, UK},
series = {ASSETS '11}
}


@url{torchvision , year = {2022}, title={Torchvision- torchvision main documentation}, url={https://pytorch.org/vision/stable/index.html}, journal={torchvision - Torchvision main documentation}} 
@inproceedings{Calvo16,
author = {Calvo, Rocio and Seyedarabi, Faezeh and Savva, Andreas},
title = {Beyond Web Content Accessibility Guidelines: Expert Accessibility Reviews},
year = {2016},
isbn = {9781450347488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019943.3019955},
doi = {10.1145/3019943.3019955},
abstract = {Despite the existence of web accessibility guidelines - e.g. Web Content Accessibility Guidelines - to help developers and designers to create more accessible websites, many websites are still not accessible. Studies showed that guidelines do not cover all the problems encounter by users with disabilities. This study investigates the problems found by seven accessibility experts in 62 accessibility evaluation reviews of mobile and desktop websites as well as mobile applications. Each evaluation conducted an accessibility review using Web Content Accessibility Guidelines 2.0 AA. In addition, experts highlighted potential issues which were not covered by these guidelines but could be potential issues encountered by people with disabilities. The study depicts those issues that were raised by experts during those audits but were not covered by the WCAG 2.0 guidelines. Finally, the study provides additional knowledge into potential problems as identified by experts and recommends techniques that developers and designers can use to create more accessible websites.},
booktitle = {Proceedings of the 7th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
pages = {77–84},
numpages = {8},
keywords = {Expert Evaluation, Heuristic Evaluation, Web Content Accessibility Guidelines, Web Accessibility},
location = {Vila Real, Portugal},
series = {DSAI 2016}
}

@article{Sarsenbayeva22,
author = {Sarsenbayeva, Zhanna and van Berkel, Niels and Velloso, Eduardo and Goncalves, Jorge and Kostakos, Vassilis},
title = {Methodological Standards in Accessibility Research on Motor Impairments: A Survey},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3543509},
doi = {10.1145/3543509},
abstract = {The design and evaluation of accessibility technology is a core component of the Computer Science landscape, aiming to ensure that digital innovations are accessible to all. One of the most prominent and long-lasting areas of accessibility research focuses on motor impairments, deficiencies that affect the ability to move, manipulate objects, and interact with the physical world. In this survey paper, we present an extensive overview of the last two decades of research into accessibility for people with motor impairments. Following a structured selection process, we analysed the study details as reported in 177 relevant papers. Based on this analysis, we critically assess user representation, measurement instruments, and existing barriers that exist in accessibility research. Finally, we discuss future directions for accessibility research within the Computer Science domain.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {may},
keywords = {Accessibility, methodology, motor impairments, motor-impaired users}
}

@article{Steriadis03,
author = {Steriadis, Constantine E. and Constantinou, Philip},
title = {Designing Human-Computer Interfaces for Quadriplegic People},
year = {2003},
issue_date = {June 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/772047.772049},
doi = {10.1145/772047.772049},
abstract = {The need for participation in an emerging Information Society has led to several research efforts for designing accessibility solutions for disabled people. In this paper we present a method for developing Human-Computer Interfaces (HCIs) for quadriplegic people in modern programming environments. The presented method accommodates the design of scanning interfaces with modern programming tools, leading to flexible interfaces with improved appearance and it is based on the use of specially designed software objects called "wifsids" (Widgets For Single-switch Input Devices). The wifsid structure is demonstrated and 4 types of wifsids are analyzed. Developed software applications are to be operated by single-switch activations that are captured through the wifsids, with the employment of several modes of the scanning technique. We also demonstrate the "Autonomia" software application, that has been developed according to the specific methodology. The basic snapshots of this application are analyzed, in order to demonstrate how the wifsids cooperate with the scanning process in a user-friendly environment that enables a quadriplegic person to access an ordinary computer system.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {jun},
pages = {87–118},
numpages = {32},
keywords = {augmentative communications, wifsid, scanning selection, quadriplegic people, disability, word-prediction, single-switch input, Accessibility, assistive technology, graphical keyboard, mouse simulation, motor-impaired users}
}

@ARTICLE{Song18,
  title     = "Crowdsourcing-based web accessibility evaluation with golden
               maximum likelihood inference",
  author    = "Song, Shuyi and Bu, Jiajun and Artmeier, Andreas and Shi, Keyue
               and Wang, Ye and Yu, Zhi and Wang, Can",
  abstract  = "Web accessibility evaluation examines how well websites comply
               with accessibility guidelines which help people with
               disabilities to perceive, navigate and contribute to the Web.
               This demanding task usually requires manual assessment by
               experts with many years of training and experience. However, not
               enough experts are available to carry out the increasing number
               of evaluation projects while non-experts often have different
               opinions about the presence of accessibility barriers.
               Addressing these issues, we introduce a crowdsourcing system
               with a novel truth inference algorithm to derive reliable and
               accurate assessments from conflicting opinions of evaluators.
               Extensive evaluation on 23,901 complex tasks assessed by 50
               people with and without disabilities shows that our approach
               outperforms state of the art approaches. In addition, we
               conducted surveys to identify frequent barriers that people with
               disabilities are facing in their daily lives and the difficulty
               to access Web pages when they encounter these barriers. The
               frequencies and severities of barriers correlate with their
               derived importance in our evaluation project.",
  journal   = "Proc. ACM Hum. Comput. Interact.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  2,
  number    = "CSCW",
  pages     = "1--21",
  month     =  nov,
  year      =  2018,
  copyright = "http://www.acm.org/publications/policies/copyright\_policy\#Background",
  language  = "en"
}
@INCOLLECTION{Correia20,
  title     = "Virtual assistants: An accessibility assessment in virtual
               assistants for people with motor disability on mobile devices",
  booktitle = "Advances in Intelligent Systems and Computing",
  author    = "Correia, Walter and Macedo, Jefte and Penha, Marcelo and
               Quintino, Jonysberg and Pellegrini, Fernanda and Anjos, Marcelo
               and Florentin, Fabiana and Santos, Andre and Da Silva, Fabio Q B",
  publisher = "Springer International Publishing",
  pages     = "239--249",
  series    = "Advances in intelligent systems and computing",
  year      =  2020,
  address   = "Cham"
}

@INPROCEEDINGS{Perez16,
  title           = "A usability evaluation of two virtual aids to enhance
                     cursor accessibility for people with motor impairments",
  booktitle       = "Proceedings of the 13th International Web for All
                     Conference",
  author          = "P{\'e}rez, J Eduardo and Valencia, Xabier and Arrue,
                     Myriam and Abascal, Julio",
  abstract        = "Basic actions in the context of Web browsing, such as
                     pointing at and clicking on links, can be seriously
                     hindered by dexterity impairments affecting the use of
                     hands and arms. In this paper, we present two different
                     virtual aids for assisting motor-impaired users when
                     pointing at and clicking on links. One of them, the
                     ``circular cursor'', aims at reducing the level of
                     accuracy required for clicking on links, whereas the other
                     one, the ``cross cursor'', aims at reducing target
                     distance for pointing at them. We conducted a web-based
                     usability testing for both cursors with 9 motor-impaired
                     and 6 able-bodied users applying their usual pointing
                     device (4 keyboard, 4 joystick, 1 trackball and 6 mouse).
                     The results show that motor-impaired participants mainly
                     preferred one of either of the two variants proposed to
                     the traditional cursor without any virtual aid for Web
                     browsing.",
  publisher       = "ACM",
  month           =  apr,
  year            =  2016,
  address         = "New York, NY, USA",
  copyright       = "http://www.acm.org/publications/policies/copyright\_policy\#Background",
  conference      = "W4A'16: International Web for All Conference",
  location        = "Montreal Canada"
}

@INPROCEEDINGS{Kim13,
  title           = "Surveying the accessibility of touchscreen games for
                     persons with motor impairments",
  booktitle       = "Proceedings of the 15th International {ACM} {SIGACCESS}
                     Conference on Computers and Accessibility",
  author          = "Kim, Yoojin and Sutreja, Nita and Froehlich, Jon and
                     Findlater, Leah",
  publisher       = "ACM",
  month           =  oct,
  year            =  2013,
  address         = "New York, NY, USA",
  conference      = "ASSETS '13: The 15th International ACM SIGACCESS
                     Conference on Computers and Accessibility",
  location        = "Bellevue Washington"
}

@INPROCEEDINGS{Creed14,
  title           = "Enhancing multi-touch table accessibility for wheelchair
                     users",
  booktitle       = "Proceedings of the 16th international {ACM} {SIGACCESS}
                     conference on Computers \& accessibility - {ASSETS} '14",
  author          = "Creed, Chris and Beale, Russell",
  abstract        = "Wheelchair users can find accessing digital content on
                     large multi-touch tables particularly difficult and
                     frustrating due to their limited reach. We present work in
                     progress that is exploring the potential of enhancing
                     touch table accessibility through the use of mid-air
                     gesturing technology. An overview of an experimental
                     prototype is provided along with the key findings from an
                     evaluation conducted with fifteen wheelchair users at a
                     public library and heritage centre.",
  publisher       = "ACM Press",
  year            =  2014,
  address         = "New York, New York, USA",
  conference      = "the 16th international ACM SIGACCESS conference",
  location        = "Rochester, New York, USA"
}

@INPROCEEDINGS{Brewster96,
  title           = "Enhancing scanning input with non-speech sounds",
  booktitle       = "Proceedings of the second annual {ACM} conference on
                     Assistive technologies - Assets '96",
  author          = "Brewster, Stephen A and Raty, Veli-Pekka and Kortekangas,
                     Atte",
  abstract        = "This paper proposes the addition of non-speech sounds to
                     aid people who use scanning as their method of input.
                     Scanning input is a temporal task; users have to press a
                     switch when a cursor is over the required target. However,
                     it is usually presented as a spatial task with the items
                     to be scanned laid-out in a grid. Research has shown that
                     for temporal tasks the auditory modality is often better
                     than the visual. This paper investigates this by adding
                     non-speech sound to a visual scanning system. It also
                     shows how our natural abilities to perceive rhythms can be
                     supported so that they can be used to aid the scanning
                     process. Structured audio messages called Earcons were
                     used for the sound output. The results from a preliminary
                     investigation were favourable, indicating that the idea is
                     feasible and further research should be undertaken.",
  publisher       = "ACM Press",
  year            =  1996,
  address         = "New York, New York, USA",
  conference      = "the second annual ACM conference",
  location        = "Vancouver, British Columbia, Canada"
}

@INPROCEEDINGS{Folmer11,
  title           = "Navigating a {3D} avatar using a single switch",
  booktitle       = "Proceedings of the 6th International Conference on
                     Foundations of Digital Games - {FDG} '11",
  author          = "Folmer, Eelke and Liu, Fangzhou and Ellis, Barrie",
  publisher       = "ACM Press",
  year            =  2011,
  address         = "New York, New York, USA",
  conference      = "the 6th International Conference",
  location        = "Bordeaux, France"
}

@ARTICLE{Lopez17,
  title     = "Design and development of one-switch video games for children
               with severe motor disabilities",
  author    = "L{\'o}pez, Sebasti{\'a}n Aced and Corno, Fulvio and De Russis,
               Luigi",
  abstract  = "Video games are not just played for fun; they have become a
               handy instrument for the cognitive, emotional, and social
               development of children. However, several barriers prevent many
               children with disabilities from playing action-oriented video
               games, alone or with their peers. In particular, children with
               severe motor disabilities, who rely on one-switch interaction
               for accessing electronic devices, find fast-paced games that
               require rapid decision-making and timely responses, completely
               unplayable. This article contributes to lowering such barriers
               by presenting GNomon ( G aming NOMON ), a software framework
               based on the NOMON mode of interaction that allows the creation
               of action-oriented single-switch video games. The article
               reports the results of two studies that evaluate the playability
               and rehabilitation suitability of GNomon-based video games. The
               playability of GNomon-based games is evaluated by assessing
               their learnability, effectiveness, errors, satisfaction,
               memorability, and enjoyability with a group of eight children
               with severe motor disabilities. The suitability for pediatric
               rehabilitation is determined by means of a focus group with a
               team of speech therapists, physiotherapists, and psychologists
               from a Local Health Agency in Turin, Italy. The results of the
               playability study are positive: All children had fun playing
               GNomon-based video games, and seven of eight were able to
               interact and play autonomously. The results of the
               rehabilitation-suitability study also entail that GNomon-based
               games can be exploited in training hand-eye coordination and
               maintenance of selective attention over time. The article
               finally offers critical hindsight and reflections and shows
               possible new future game concepts.",
  journal   = "ACM Trans. Access. Comput.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  10,
  number    =  4,
  pages     = "1--42",
  month     =  oct,
  year      =  2017,
  copyright = "http://www.acm.org/publications/policies/copyright\_policy\#Background",
  language  = "en"
}

@INPROCEEDINGS{axeray,  author={Bajammal, Mohammad and Mesbah, Ali},  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},   title={Semantic Web Accessibility Testing via Hierarchical Visual Analysis},   year={2021},  volume={},  number={},  pages={1610-1621},  doi={10.1109/ICSE43902.2021.00143}}

@inproceedings{Vatvu17,
author = {Vatavu, Radu-Daniel},
title = {Improving Gesture Recognition Accuracy on Touch Screens for Users with Low Vision},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025941},
doi = {10.1145/3025453.3025941},
abstract = {We contribute in this work on gesture recognition to improve the accessibility of touch screens for people with low vision. We examine the accuracy of popular recognizers for gestures produced by people with and without visual impairments, and we show that the user-independent accuracy of $P, the best recognizer among those evaluated, is small for people with low vision (83.8%), despite $P being very effective for gestures produced by people without visual impairments (95.9%). By carefully analyzing the gesture articulations produced by people with low vision, we inform key algorithmic revisions for the P recognizer, which we call P+. We show significant accuracy improvements of $P+ for gestures produced by people with low vision, from 83.8% to 94.7% on average and up to 98.2%, and 3x faster execution times compared to P.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {4667–4679},
numpages = {13},
keywords = {P+, recognition, visual impairments, P, gesture recognition, algorithms, evaluation, point clouds, touch screens, recognition accuracy, low vision, 1, touch gestures},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{Park14,
author = {Park, Kyudong and Goh, Taedong and So, Hyo-Jeong},
title = {Toward Accessible Mobile Application Design: Developing Mobile Application Accessibility Guidelines for People with Visual Impairment},
year = {2014},
isbn = {9788968487521},
publisher = {Hanbit Media, Inc.},
address = {Seoul, KOR},
abstract = {While the use of Smartphones has improved the life of people with disabilities, several mobile content and applications remain inaccessible to people with visual impairment. Toward the overarching goal of accessible mobile application design, this two-phased study attempts to develop mobile application accessibility guidelines for people with visual impairment. First, we investigated how people with visual impairment use mobile phones. Four participants with visual impairment performed specified tasks. Their usage patterns and follow-up interviews were analyzed. Serious accessibility problems were found in both typing and VoiceOver functions. Second, we evaluated and developed systematic guidelines and standards of designing accessible mobile applications through a heuristic walkthrough method. Four experts with extensive experiences and knowledge about mobile application development/design used the VoiceOver function with iPhone for 5 days, and then walked through thought-provoking tasks. In conclusion, we propose a set of 10 heuristics for developing accessible mobile applications and suggest a critical need for internationally-agreed guidelines and standards to improve the current mobile environment for people with disabilities.},
booktitle = {Proceedings of HCI Korea},
pages = {31–38},
numpages = {8},
keywords = {evaluation, heuristic walkthrough, design guideline, standardization, VoiceOver},
location = {Seoul, Republic of Korea},
series = {HCIK '15}
}

@inproceedings{Yeh11,
author = {Yeh, Tom and Chang, Tsung-Hsiang and Xie, Bo and Walsh, Greg and Watkins, Ivan and Wongsuphasawat, Krist and Huang, Man and Davis, Larry S. and Bederson, Benjamin B.},
title = {Creating Contextual Help for GUIs Using Screenshots},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047214},
doi = {10.1145/2047196.2047214},
abstract = {Contextual help is effective for learning how to use GUIs by showing instructions and highlights on the actual interface rather than in a separate viewer. However, end-users and third-party tech support typically cannot create contextual help to assist other users because it requires programming skill and source code access. We present a creation tool for contextual help that allows users to apply common computer skills-taking screenshots and writing simple scripts. We perform pixel analysis on screenshots to make this tool applicable to a wide range of applications and platforms without source code access. We evaluated the tool's usability with three groups of participants: developers, in-structors, and tech support. We further validated the applicability of our tool with 60 real tasks supported by the tech support of a university campus.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {145–154},
numpages = {10},
keywords = {pixel analysis, help, contextual help},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}



@inproceedings{Stuerzlinger06,
author = {Stuerzlinger, Wolfgang and Chapuis, Olivier and Phillips, Dusty and Roussel, Nicolas},
title = {User Interface Fa\c{c}ades: Towards Fully Adaptable User Interfaces},
year = {2006},
isbn = {1595933131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166253.1166301},
doi = {10.1145/1166253.1166301},
abstract = {User interfaces are becoming more and more complex. Adaptable and adaptive interfaces have been proposed to address this issue and previous studies have shown that users prefer interfaces that they can adapt to self-adjusting ones. However, most existing systems provide users with little support for adapting their interfaces. Interface customization techniques are still very primitive and usually constricted to particular applications. In this paper, we present User Interface Fa\c{c}ades, a system that provides users with simple ways to adapt, reconfigure, and re-combine existing graphical interfaces, through the use of direct manipulation techniques. The paper describes the user's view of the system, provides some technical details, and presents several examples to illustrate its potential.},
booktitle = {Proceedings of the 19th Annual ACM Symposium on User Interface Software and Technology},
pages = {309–318},
numpages = {10},
keywords = {adaptable user interfaces},
location = {Montreux, Switzerland},
series = {UIST '06}
}
STARTING POINT============================================
@inproceedings{Mchugh20,
author = {McHugh, Thomas B. and Barth, Cooper},
title = {Assistive Technology Design as a Computer Science Learning Experience},
year = {2020},
isbn = {9781450371032},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373625.3417081},
doi = {10.1145/3373625.3417081},
abstract = {As awareness surrounding the importance of developing accessible applications has grown, work to integrate inclusive design into computer science (CS) curriculum has gained traction. However, there remain obstacles to integrating accessibility into introductory CS coursework. In this paper, we discuss current challenges to building assistive technology and the findings of a formative study exploring the role of accessibility in an undergraduate CS curriculum. We respond to the observed obstacles by presenting V11, a cross-platform programming interface to empower novice CS students to build assistive technology. To evaluate the effectiveness of V11 as a CS and accessibility learning tool, we conducted design workshops with ten undergraduate CS students, who brainstormed solutions to a real accessibility problem and then used V11 to prototype their solution. Post-workshop evaluations showed a 28\% average increase in student interest in building accessible technology, and V11 was rated easier to use than other accessibility programming tools. Student reflections indicate that V11 can be an accessibility learning tool, while also teaching fundamental Computer Science concepts.},
booktitle = {Proceedings of the 22nd International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {100},
numpages = {4},
keywords = {accessibility, assistive technology, computer science education, inclusive design, allyship},
location = {Virtual Event, Greece},
series = {ASSETS '20}
}

@inproceedings{Almeida10,
author = {Almeida, Leonelo Dell Anhol and Baranauskas, Maria Cec\'{\i}lia Calani},
title = {Universal Design Principles Combined with Web Accessibility Guidelines: A Case Study},
year = {2010},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {Accessibility evaluation processes currently face important issues. One is the difficulty to understand the issues pointed out in accessibility guidelines and the procedure to correct them. Other regards the coverage of the accessibility guidelines in relation to the whole technical and social aspects, and contexts of use. Those deficiencies contribute to the low adherence to recommendations and naive solutions since designers usually are not experts in accessibility. Aiming at contributing to this scenario in clarifying understanding and action towards the development of accessible code we proposed a merging of accessibility guidelines for web content from WCAG 2.0 and accessibility physical recommendations from ISO 9241 to Universal Design principles and guidelines. To evaluate our approach we conducted a case study involving students in computer science and specialists in web accessibility. As results we verified relevant contributions regarding the number and quality of the identified accessibility issues even for the specialists in accessibility.},
booktitle = {Proceedings of the IX Symposium on Human Factors in Computing Systems},
pages = {169–178},
numpages = {10},
keywords = {universal design, web accessibility guidelines, evaluation},
location = {Belo Horizonte, Minas Gerais, Brazil},
series = {IHC '10}
}

@article{Yan19,
author = {Yan, Shunguo and Ramachandran, P. G.},
title = {The Current Status of Accessibility in Mobile Apps},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-7228},
url = {https://doi.org/10.1145/3300176},
doi = {10.1145/3300176},
abstract = {This study evaluated the status of accessibility in mobile apps by investigating the graphical user interface (GUI) structures and conformance to accessibility guidelines of 479 Android apps in 23 business categories from Google Play. An automated tool, IBM Mobile Accessibility Checker (MAC), was used to identify the accessibility issues, which were categorized as a violation (V), potential violation (PV), or warning (W). The results showed 94.8\%, 97.5\%, and 66.4\% of apps studied contained issues related to V, PV, or W, respectively. Five widget categories (TextView, ImageView, View, Button, and ImageButton) were used to create 92\% of the total number of the GUI elements and caused 89\%, 78\%, and 86\% of V, PV, and W, respectively. These accessibility issues were mainly caused by lack of element focus, missing element description, low text color contrast, lack of sufficient spacing between elements, and less than minimum sizes of text fonts and elements. Together, these accessibility issues accounted for 97.0\%, 77.8\%, and 94.5\% of V, PV, and W, respectively.This study proposed coverage measures to estimate the percentage of accessibility issues identified by an automated tool. The result showed that MAC, on average, identified about 67\% of accessibility issues in mobile apps.Two new accessibility conformance measures were proposed in this study: inaccessible element rate (IAER) and accessibility issue rate (AIR). IAER estimates the percentage of GUI elements that are inaccessible. AIR calculates the percentage of the actual number of accessibility issues relative to the maximum number of accessibility issues. Average IAER and AIR scores were 27.3\%, 19.9\%, 6.3\% and 20.7\%, 15.0\%, 5.4\% for V, PV, and W, respectively, for the studied apps. The IAER score showed approximately 30\% of the GUI elements had accessibility issues, and the AIR score indicated that 15\% of the accessibility issues remained and need to be fixed to make the apps accessible.},
journal = {ACM Trans. Access. Comput.},
month = {feb},
articleno = {3},
numpages = {31},
keywords = {accessibility evaluation, disability, accessibility, Mobile, mobile apps, graphical user interface (GUI), usability}
}

@inproceedings{Silva20,
author = {da Silva, Henrique Neves and Endo, Andre Takeshi and Eler, Marcelo Medeiros and Vergilio, Silvia Regina and Durelli, Vinicius H. S.},
title = {On the Relation between Code Elements and Accessibility Issues in Android Apps},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425209},
doi = {10.1145/3425174.3425209},
abstract = {Mobile apps have gone mainstream and become part of our daily lives. Currently, many efforts have been made to make apps more accessible to people with disabilities. However, little is still known on how to implement more accessible apps. In the Android API, there are (code) elements that may be employed to (in)directly improve the app's accessibility. This paper aims to investigate the prevalence of accessibility code elements and their relation to potential accessibility issues. First, we identified code elements of the native Android API that may be related to accessibility features, and mapped them to principles and success criteria of the Web Content Accessibility Guidelines (WCAG) 2.1. Using a sample of 111 open source mobile apps available in Google Play, we conducted a characterization study to examine the prevalence of accessibility code elements. We also analyzed how these code elements are related to issues detected by the static analyzer Android Lint and the accessibility testing tool MATE. Our results indicate that code elements are not widely used; the ones directly related to accessibility are present in only a few apps. Additionally, our results would seem to suggest that apps that adopt accessibility code elements, tend to have less accessibility issues. By analyzing our results from the standpoint of the WCAG principles, we conclude that there is room for improvement in terms of how both the Android API and automated testing tools deal with accessibility-related issues.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {40–49},
numpages = {10},
keywords = {Mobile apps. Accessibility},
location = {Natal, Brazil},
series = {SAST 20}
}

@inproceedings{Kim16,
author = {Kim, Hyun K. and Kim, Changwon and Lim, Eunyoung and Kim, Hyunjin},
title = {How to Develop Accessibility UX Design Guideline in Samsung},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2957271},
doi = {10.1145/2957265.2957271},
abstract = {Accessibility is the major social responsibility of the Information Technology (IT) companies. New accessibility technology can make IT devices more accessible to diverse users, thus it can reduce barriers to the use of IT devices. The objective of this study is to inform the procedures to develop accessibility UX guidelines in Samsung. In 2014, the comprehensive literature survey was conducted including academic research papers, accessibility laws, and international standard documents. In 2015, a user test was conducted to clarify and specify the guidelines. In 2016, lawyers reviewed the guidelines to increase the reliability of them. The proposed procedure is helpful to develop new accessibility UX guidelines.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {551–556},
numpages = {6},
keywords = {user interface, accessibility guideline, IT devices},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{Santiago22,
author = {Santiago, Mara Taynar and Marques, Anna Beatriz},
title = {Are User Reviews Useful for Identifying Accessibility Issues That Autistic Users Face? An Exploratory Study},
year = {2022},
isbn = {9781450395069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3554364.3559114},
doi = {10.1145/3554364.3559114},
abstract = {Google Play Store offers several user reviews that can provide data on aspects of user experience, usability, and accessibility. Several studies address the importance of user reviews and their contributions to the evolution of interactive systems. However, accessibility has been little discussed in this type of study, especially accessibility for users with Autism Spectrum Disorder (ASD). Considering the potential of user reviews, this paper presents a textual analysis of reviews extracted from eight educational applications for autistic children: ABC autismo, Aprendendo com biel e seus amigos, AutApp autismo, Autismo projeto integrar, Jade autism, Matraquinha, OTO (Olhar Tocar Ouvir) and Teacch.me. The analysis adopted the Guidelines for Accessible Interfaces for people with Autism (GAIA) to classify user reviews. An accessibility diagnostic was obtained for apps evaluated and demonstrated that they have recurring accessibility problems regarding responses to actions, visibility of the state of the system, and customization. When compared to studies that have adopted other methods to evaluate the accessibility of the same applications, we observed that the user reviews provided more evidence of accessibility problems.},
booktitle = {Proceedings of the 21st Brazilian Symposium on Human Factors in Computing Systems},
articleno = {6},
numpages = {11},
keywords = {user review, accessibility, educational mobile apps, autism spectrum disorder},
location = {Diamantina, Brazil},
series = {IHC '22}
}

@inproceedings{Mateus20,
author = {Mateus, Delvani Ant\^{o}nio and Silva, Carlos Alberto and Eler, Marcelo Medeiros and Freire, Andr\'{e} Pimenta},
title = {Accessibility of Mobile Applications: Evaluation by Users with Visual Impairment and by Automated Tools},
year = {2020},
isbn = {9781450381727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424953.3426633},
doi = {10.1145/3424953.3426633},
abstract = {Providing accessible mobile applications to people with visual disabilities demands appropriate evaluation techniques and tools to identify problems during the design of such systems. Automated accessibility evaluation tools are important to support evaluation tasks and to make evaluators more productive to perform repetitive analyses. However, automated tools cannot find alone all problems that users encounter in accessibility evaluations of mobile applications. Despite previous investigations on the coverage of accessibility problems encountered by automated tools on websites, there is little knowledge about the relationship between the problems encountered by those tools and problems faced by users with visual impairments in mobile applications. This paper presents a study comparing issues encountered by the automated tools MATE (Mobile Accessibility Testing) and Accessibility Scanner with a set of 415 instances of accessibility problems encountered in a previous user study involving six blind and five partially-sighted users on four mobile applications. The results showed that 36 types of problems were encountered only by users, tree types of problems were encountered both by users and by the tools, and 11 types of problems were encountered only by the automated tools. The results show the kinds of relevant problems that automated tools can identify, aiding in the early identification of such problems. The study also contributes to determining the types of problems that are only encountered by evaluations with users, reinforcing the importance of involving users in accessibility evaluation and characterizing the problems in mobile applications that can go unnoticed if automated tools are used alone.},
booktitle = {Proceedings of the 19th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {4},
numpages = {10},
keywords = {mobile accessibility, user evaluation, automated tests},
location = {Diamantina, Brazil},
series = {IHC '20}
}

@inproceedings{Shiver15,
author = {Shiver, Brent N. and Wolfe, Rosalee J.},
title = {Evaluating Alternatives for Better Deaf Accessibility to Selected Web-Based Multimedia},
year = {2015},
isbn = {9781450334006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2700648.2809857},
doi = {10.1145/2700648.2809857},
abstract = {The proliferation of video and audio media on the Internet has created a distinct disadvantage for deaf Internet users. Despite technological and legislative milestones in recent decades in making television and movies more accessible, there has been less progress with online access. A major obstacle to providing captions for Internet media is the high cost of captioning and transcribing services. This paper reports on two studies that focused on multimedia accessibility for Internet users who were born deaf or became deaf at an early age. An initial study attempted to identify priorities for deaf accessibility improvement. A total of 20 deaf and hard-of-hearing participants were interviewed via videophone about their Internet usage and the issues that were the most frustrating. The most common theme was concern over a lack of accessibility for online news. In the second study, a total of 95 deaf and hard-of-hearing participants evaluated different caption styles, some of which were generated through automatic speech recognition.Results from the second study confirm that captioning online videos makes the Internet more accessible to the deaf users, even when the captions are automatically generated. However color-coded captions used to highlight confidence levels were found neither to be beneficial nor detrimental; yet when asked directly about the benefit of color-coding, participants strongly favored the concept.},
booktitle = {Proceedings of the 17th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {231–238},
numpages = {8},
keywords = {captioning, speech-to-text, multimedia accessibility, automatic speech recognition, deaf, web accessibility},
location = {Lisbon, Portugal},
series = {ASSETS '15}
}

@inproceedings{Aizpurua14,
author = {Aizpurua, Amaia and Arrue, Myriam and Harper, Simon and Vigo, Markel},
title = {Are Users the Gold Standard for Accessibility Evaluation?},
year = {2014},
isbn = {9781450326513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2596695.2596705},
doi = {10.1145/2596695.2596705},
abstract = {User testing is considered a key part of web accessibility evaluation. However, little is known about how effective is for identifying accessibility problems. Our experience, informed by a series of studies with blind users, corroborates that a website with a significant number of guideline violations can be perceived as accessible, and on the contrary, some participants may not perceive a highly accessible website as accessible. Accessibility guidelines are often criticised by their partial coverage and questionable validity. However, we should be very careful about making categorical statements in this regard as there are a number of variables that may introduce biases in user tests. We identify sources of bias related to user expertise, the experimental setting, employed language and reporting that, if not adequately controlled, may influence on the validity and reliability of the evaluation results. We discuss the limitations and practical implications of user testing with blind users for web accessibility evaluation.},
booktitle = {Proceedings of the 11th Web for All Conference},
articleno = {13},
numpages = {4},
keywords = {web accessibility, screen readers, user testing, blind users},
location = {Seoul, Korea},
series = {W4A '14}
}

@inproceedings{Silva19,
author = {Silva, Giovanna Magda S. and de C. Andrade, Rossana M. and de Gois R. Darin, Ticianne},
title = {Design and Evaluation of Mobile Applications for People with Visual Impairments: A Compilation of Usable Accessibility Guidelines},
year = {2019},
isbn = {9781450369718},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357155.3358450},
doi = {10.1145/3357155.3358450},
abstract = {In a context where the individual's daily needs and activities are increasingly focused on mobile devices, the use of sound design practices focused on technically accessible and cognitively usable interfaces becomes critical. This issue is even more sensitive when considered under the perspective of people with visual impairments, who depend on operating systems (OS) accessibility features. However, native OS accessibility support is not mature enough to ensure usability and accessibility simultaneously in different kinds of applications. To overcome these limitations, Human-Computer Interaction literature has been proposing guidelines, best practices, and heuristics for designing and evaluating usable and accessible designs. Still, this information is diffuse and can be challenging to implement because they were proposed considering different definitions and specifications. To support practitioners, students and researchers in finding and implementing usability and accessibility guidelines in mobile applications, the present work presents a compilation of 369 recommendations identified in the literature on the design and evaluation of usable and accessible interfaces for Sil people on mobile devices.},
booktitle = {Proceedings of the 18th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {21},
numpages = {10},
keywords = {mobile devices, usable accessibility, recommendations compilation},
location = {Vit\'{o}ria, Esp\'{\i}rito Santo, Brazil},
series = {IHC '19}
}

@inproceedings{Li22,
author = {Li, Jiasheng and Yan, Zeyu and Jarjue, Ebrima Haddy and Shetty, Ashrith and Peng, Huaishu},
title = {TangibleGrid: Tangible Web Layout Design for Blind Users},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545627},
doi = {10.1145/3526113.3545627},
abstract = {We present TangibleGrid, a novel device that allows blind users to understand and design the layout of a web page with real-time tangible feedback. We conducted semi-structured interviews and a series of co-design sessions with blind users to elicit insights that guided the design of TangibleGrid. Our final prototype contains shape-changing brackets representing the web elements and a baseboard representing the web page canvas. Blind users can design a web page layout through creating and editing web elements by snapping or adjusting tangible brackets on top of the baseboard. The baseboard senses the brackets’ type, size, and location, verbalizes the information, and renders the web page on the client browser. Through a formative user study, we found that blind users could understand a web page layout through TangibleGrid. They were also able to design a new web layout from scratch without the help of sighted people.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {47},
numpages = {12},
keywords = {Accessible web design, tangible user interface, accessibility, tactile feedback, visual impairment},
location = {Bend, OR, USA},
series = {UIST '22}
}

 @inproceedings{Brajnik15,
author = {Brajnik, Giorgio and Pighin, Chiara and Fabbro, Sara},
title = {Model-Based Automated Accessibility Testing},
year = {2015},
isbn = {9781450334006},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/2700648.2811357},
doi = {10.1145/2700648.2811357},
booktitle = {Proceedings of the 17th International ACM SIGACCESS Conference on Computers and Accessibility},
keywords = {human factors},
location = {Lisbon, Portugal},
series = {ASSETS '15}
}


@inproceedings{Bajammal21,
author = {Bajammal, Mohammad and Mesbah, Ali},
title = {Semantic Web Accessibility Testing via Hierarchical Visual Analysis},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00143},
doi = {10.1109/ICSE43902.2021.00143},
abstract = {Web accessibility, the design of web apps to be usable by users with disabilities, impacts millions of people around the globe. Although accessibility has traditionally been a marginal afterthought that is often ignored in many software products, it is increasingly becoming a legal requirement that must be satisfied. While some web accessibility testing tools exist, most only perform rudimentary syntactical checks that do not assess the more important high-level semantic aspects that users with disabilities rely on. Accordingly, assessing web accessibility has largely remained a laborious manual process requiring human input. In this paper, we propose an approach, called AxeRay, that infers semantic groupings of various regions of a web page and their semantic roles. We evaluate our approach on 30 real-world websites and assess the accuracy of semantic inference as well as the ability to detect accessibility failures. The results show that AxeRay achieves, on average, an F-measure of 87\% for inferring semantic groupings, and is able to detect accessibility failures with 85\% accuracy.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1610–1621},
numpages = {12},
keywords = {visual analysis, web testing, web accessibility, accessibility testing},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{Ramachandra18,
author = {Ramachandra, Nagendra Prasad Kasaghatta and Csallner, Christoph},
title = {Testing Web-Based Applications with the <u>v</u>oice <u>c</u>ontrolled <u>a</u>ccessibility and <u>t</u>esting Tool (VCAT)},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3195099},
doi = {10.1145/3183440.3195099},
abstract = {Many web applications and software engineering tools such as test generators are not accessible for users who do not use traditional input devices such as mouse and keyboard. To address this shortcoming of current applications, this work leverages recent speech recognition advances to create a browser plugin that interprets voice inputs as web browser commands and as steps in a corresponding test case. In an initial experiment, the resulting Voice Controlled Accessibility and Testing tool (VCAT) prototype for Chrome and Selenium yielded a lower overall runtime than a traditional test creation approach.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {208–209},
numpages = {2},
keywords = {voice control, accessible software engineering tool, selenium},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

NEW=============================================

@inproceedings{Liu21,
author = {Liu, Zhe},
title = {Discovering UI Display Issues with Visual Understanding},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3418917},
doi = {10.1145/3324884.3418917},
abstract = {GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, blurred screen, missing image always occur during GUI rendering on difference devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a novel approach, OwlEye, based on deep learning for modelling visual information of the GUI screenshot. Therefore, OwlEye can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. We manually construct a large-scale labelled dataset with 4,470 GUI screenshots with UI display issues. We develop a heuristics-based data augmentation method and a GAN-based data augmentation method for boosting the performance of our OwlEye. At present, the evaluation demonstrates that our OwlEye can achieve 85\% precision and 84\% recall in detecting UI display issues, and 90\% accuracy in localizing these issues.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1373–1375},
numpages = {3},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{Liu2021,
author = {Liu, Zhe and Chen, Chunyang and Wang, Junjie and Huang, Yuekai and Hu, Jun and Wang, Qing},
title = {Owl Eyes: Spotting UI Display Issues via Visual Understanding},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416547},
doi = {10.1145/3324884.3416547},
abstract = {Graphical User Interface (GUI) provides a visual bridge between a software application and end users, through which they can interact with each other. With the development of technology and aesthetics, the visual effects of the GUI are more and more attracting. However, such GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, blurred screen, missing image always occur during GUI rendering on different devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a novel approach, OwlEye, based on deep learning for modelling visual information of the GUI screenshot. Therefore, OwlEye can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. We manually construct a large-scale labelled dataset with 4,470 GUI screenshots with UI display issues and develop a heuristics-based data augmentation method for boosting the performance of our OwlEye. The evaluation demonstrates that our OwlEye can achieve 85\% precision and 84\% recall in detecting UI display issues, and 90\% accuracy in localizing these issues. We also evaluate OwlEye with popular Android apps on Google Play and F-droid, and successfully uncover 57 previously-undetected UI display issues with 26 of them being confirmed or fixed so far.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {398–409},
numpages = {12},
keywords = {deep learning, mobile app, UI display, UI testing},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@INPROCEEDINGS{Nguyen:ASE'15,
  author={Nguyen, Tuan Anh and Csallner, Christoph},
  booktitle={2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Reverse Engineering Mobile Application User Interfaces with REMAUI (T)}, 
  year={2015},
  volume={},
  number={},
  pages={248-259},
  doi={10.1109/ASE.2015.32}}

@ARTICLE{Moran:TSE'18,
  author={Moran, Kevin and Bernal-Cárdenas, Carlos and Curcio, Michael and Bonett, Richard and Poshyvanyk, Denys},
  journal={IEEE Transactions on Software Engineering}, 
  title={Machine Learning-Based Prototyping of Graphical User Interfaces for Mobile Apps}, 
  year={2020},
  volume={46},
  number={2},
  pages={196-221},
  doi={10.1109/TSE.2018.2844788}}


@inproceedings{Chen18,
author = {Chen, Chunyang and Su, Ting and Meng, Guozhu and Xing, Zhenchang and Liu, Yang},
title = {From UI Design Image to GUI Skeleton: A Neural Machine Translator to Bootstrap Mobile GUI Implementation},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180240},
doi = {10.1145/3180155.3180240},
abstract = {A GUI skeleton is the starting point for implementing a UI design image. To obtain a GUI skeleton from a UI design image, developers have to visually understand UI elements and their spatial layout in the image, and then translate this understanding into proper GUI components and their compositions. Automating this visual understanding and translation would be beneficial for bootstraping mobile GUI implementation, but it is a challenging task due to the diversity of UI designs and the complexity of GUI skeletons to generate. Existing tools are rigid as they depend on heuristically-designed visual understanding and GUI generation rules. In this paper, we present a neural machine translator that combines recent advances in computer vision and machine translation for translating a UI design image into a GUI skeleton. Our translator learns to extract visual features in UI images, encode these features' spatial layouts, and generate GUI skeletons in a unified neural network framework, without requiring manual rule development. For training our translator, we develop an automated GUI exploration method to automatically collect large-scale UI data from real-world applications. We carry out extensive experiments to evaluate the accuracy, generality and usefulness of our approach.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {665–676},
numpages = {12},
keywords = {user interface, reverse engineering, deep learning},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@INPROCEEDINGS{Vendome19,  author={Vendome, Christopher and Solano, Diana and Liñán, Santiago and Linares-Vásquez, Mario},  booktitle={2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)},   title={Can Everyone use my app? An Empirical Study on Accessibility in Android Apps},   year={2019},  volume={},  number={},  pages={41-52},  doi={10.1109/ICSME.2019.00014}}

@article{Shaoqing15,
  author    = {Shaoqing Ren and
               Kaiming He and
               Ross B. Girshick and
               Jian Sun},
  title     = {Faster {R-CNN:} Towards Real-Time Object Detection with Region Proposal
               Networks},
  journal   = {CoRR},
  volume    = {abs/1506.01497},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.01497},
  eprinttype = {arXiv},
  eprint    = {1506.01497},
  timestamp = {Mon, 13 Aug 2018 16:46:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RenHG015.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
a service of  Schloss Dagstuhl - Leibniz Center for Informatics homebrowsesearchabout

@inproceedings{Kong21,
author = {Kong, Junhan and Zhong, Mingyuan and Fogarty, James and Wobbrock, Jacob O.},
title = {New Metrics for Understanding Touch by People with and without Limited Fine Motor Function},
year = {2021},
isbn = {9781450383066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3441852.3476559},
doi = {10.1145/3441852.3476559},
abstract = {Current performance measures with touch-based systems usually focus on overall performance, such as touch accuracy and target acquisition speed. But a touch is not an atomic event; it is a process that unfolds over time, and this process can be characterized to gain insight into users’ touch behaviors. To this end, our work proposes 13 target-agnostic touch performance metrics to characterize what happens during a touch. These metrics are: touch direction, variability, drift, duration, extent, absolute/signed area change, area variability, area deviation, absolute/signed angle change, angle variability, and angle deviation. Unlike traditional touch performance measures that treat a touch as a single (x, y) coordinate, we regard a touch as a time series of ovals that occur from finger-down to finger-up. We provide a mathematical formula and intuitive description for each metric we propose. To evaluate our metrics, we run an analysis on a publicly available dataset containing touch inputs by people with and without limited fine motor function, finding our metrics helpful in characterizing different fine motor control challenges. Our metrics can be useful to designers and evaluators of touch-based systems, particularly when making touch screens accessible to all forms of touch.},
booktitle = {Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {80},
numpages = {4},
keywords = {limited fine motor function, Understanding touch},
location = {Virtual Event, USA},
series = {ASSETS '21}
}


